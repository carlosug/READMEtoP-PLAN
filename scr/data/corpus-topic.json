[
    {
        "url": "https://github.com/ollama/ollama",
        "readme_url": "https://raw.githubusercontent.com/ollama/ollama/main/README.md",
        "topic": [
            "go",
            "golang",
            "llama",
            "llama2",
            "llm",
            "llms",
            "mistral",
            "ollama"
        ],
        "text": "Windows\n\nComing soon! For now, you can install Ollama on Windows via WSL2.\n\n",
        "token": [
            "Windows",
            "Coming",
            "soon",
            "!",
            "For",
            "now",
            ",",
            "you",
            "can",
            "install",
            "Ollama",
            "on",
            "Windows",
            "via",
            "WSL2",
            "."
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/ollama/ollama",
        "readme_url": "https://raw.githubusercontent.com/ollama/ollama/main/README.md",
        "topic": [
            "go",
            "golang",
            "llama",
            "llama2",
            "llm",
            "llms",
            "mistral",
            "ollama"
        ],
        "text": "Linux & WSL2\n\n```\ncurl https://ollama.ai/install.sh | sh\n```\n\n[Manual install instructions](https://github.com/jmorganca/ollama/blob/main/docs/linux.md)\n\n",
        "token": [
            "Linux",
            "&",
            "WSL2",
            "``",
            "`",
            "curl",
            "https",
            ":",
            "//ollama.ai/install.sh",
            "|",
            "sh",
            "``",
            "`",
            "[",
            "Manual",
            "install",
            "instructions",
            "]",
            "(",
            "https",
            ":",
            "//github.com/jmorganca/ollama/blob/main/docs/linux.md",
            ")"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/ollama/ollama",
        "readme_url": "https://raw.githubusercontent.com/ollama/ollama/main/README.md",
        "topic": [
            "go",
            "golang",
            "llama",
            "llama2",
            "llm",
            "llms",
            "mistral",
            "ollama"
        ],
        "text": "Building\n\nInstall `cmake` and `go`:\n\n```\nbrew install cmake go\n```\n\nThen generate dependencies:\n```\ngo generate ./...\n```\nThen build the binary:\n```\ngo build .\n```\n\nMore detailed instructions can be found in the [developer guide](https://github.com/jmorganca/ollama/blob/main/docs/development.md)\n\n\n",
        "token": [
            "Building",
            "Install",
            "`",
            "cmake",
            "`",
            "and",
            "`",
            "go",
            "`",
            ":",
            "``",
            "`",
            "brew",
            "install",
            "cmake",
            "go",
            "``",
            "`",
            "Then",
            "generate",
            "dependencies",
            ":",
            "``",
            "`",
            "go",
            "generate",
            "./",
            "...",
            "``",
            "`",
            "Then",
            "build",
            "the",
            "binary",
            ":",
            "``",
            "`",
            "go",
            "build",
            ".",
            "``",
            "`",
            "More",
            "detailed",
            "instructions",
            "can",
            "be",
            "found",
            "in",
            "the",
            "[",
            "developer",
            "guide",
            "]",
            "(",
            "https",
            ":",
            "//github.com/jmorganca/ollama/blob/main/docs/development.md",
            ")"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/ollama/ollama",
        "readme_url": "https://raw.githubusercontent.com/ollama/ollama/main/README.md",
        "topic": [
            "go",
            "golang",
            "llama",
            "llama2",
            "llm",
            "llms",
            "mistral",
            "ollama"
        ],
        "text": "Extensions & Plugins\n\n- [Raycast extension](https://github.com/MassimilianoPasquini97/raycast_ollama)\n- [Discollama](https://github.com/mxyng/discollama) (Discord bot inside the Ollama discord channel)\n- [Continue](https://github.com/continuedev/continue)\n- [Obsidian Ollama plugin](https://github.com/hinterdupfinger/obsidian-ollama)\n- [Logseq Ollama plugin](https://github.com/omagdy7/ollama-logseq)\n- [Dagger Chatbot](https://github.com/samalba/dagger-chatbot)\n- [Discord AI Bot](https://github.com/mekb-turtle/discord-ai-bot)\n- [Ollama Telegram Bot](https://github.com/ruecat/ollama-telegram)\n- [Hass Ollama Conversation](https://github.com/ej52/hass-ollama-conversation)\n- [Rivet plugin](https://github.com/abrenneke/rivet-plugin-ollama)\n- [Llama Coder](https://github.com/ex3ndr/llama-coder) (Copilot alternative using Ollama)\n- [Obsidian BMO Chatbot plugin](https://github.com/longy2k/obsidian-bmo-chatbot)\n- [Open Interpreter](https://docs.openinterpreter.com/language-model-setup/local-models/ollama)\n- [twinny](https://github.com/rjmacarthy/twinny) (Copilot and Copilot chat alternative using Ollama)\n\n",
        "token": [
            "Extensions",
            "&",
            "Plugins",
            "-",
            "[",
            "Raycast",
            "extension",
            "]",
            "(",
            "https",
            ":",
            "//github.com/MassimilianoPasquini97/raycast_ollama",
            ")",
            "-",
            "[",
            "Discollama",
            "]",
            "(",
            "https",
            ":",
            "//github.com/mxyng/discollama",
            ")",
            "(",
            "Discord",
            "bot",
            "inside",
            "the",
            "Ollama",
            "discord",
            "channel",
            ")",
            "-",
            "[",
            "Continue",
            "]",
            "(",
            "https",
            ":",
            "//github.com/continuedev/continue",
            ")",
            "-",
            "[",
            "Obsidian",
            "Ollama",
            "plugin",
            "]",
            "(",
            "https",
            ":",
            "//github.com/hinterdupfinger/obsidian-ollama",
            ")",
            "-",
            "[",
            "Logseq",
            "Ollama",
            "plugin",
            "]",
            "(",
            "https",
            ":",
            "//github.com/omagdy7/ollama-logseq",
            ")",
            "-",
            "[",
            "Dagger",
            "Chatbot",
            "]",
            "(",
            "https",
            ":",
            "//github.com/samalba/dagger-chatbot",
            ")",
            "-",
            "[",
            "Discord",
            "AI",
            "Bot",
            "]",
            "(",
            "https",
            ":",
            "//github.com/mekb-turtle/discord-ai-bot",
            ")",
            "-",
            "[",
            "Ollama",
            "Telegram",
            "Bot",
            "]",
            "(",
            "https",
            ":",
            "//github.com/ruecat/ollama-telegram",
            ")",
            "-",
            "[",
            "Hass",
            "Ollama",
            "Conversation",
            "]",
            "(",
            "https",
            ":",
            "//github.com/ej52/hass-ollama-conversation",
            ")",
            "-",
            "[",
            "Rivet",
            "plugin",
            "]",
            "(",
            "https",
            ":",
            "//github.com/abrenneke/rivet-plugin-ollama",
            ")",
            "-",
            "[",
            "Llama",
            "Coder",
            "]",
            "(",
            "https",
            ":",
            "//github.com/ex3ndr/llama-coder",
            ")",
            "(",
            "Copilot",
            "alternative",
            "using",
            "Ollama",
            ")",
            "-",
            "[",
            "Obsidian",
            "BMO",
            "Chatbot",
            "plugin",
            "]",
            "(",
            "https",
            ":",
            "//github.com/longy2k/obsidian-bmo-chatbot",
            ")",
            "-",
            "[",
            "Open",
            "Interpreter",
            "]",
            "(",
            "https",
            ":",
            "//docs.openinterpreter.com/language-model-setup/local-models/ollama",
            ")",
            "-",
            "[",
            "twinny",
            "]",
            "(",
            "https",
            ":",
            "//github.com/rjmacarthy/twinny",
            ")",
            "(",
            "Copilot",
            "and",
            "Copilot",
            "chat",
            "alternative",
            "using",
            "Ollama",
            ")"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/geekan/MetaGPT",
        "readme_url": "https://raw.githubusercontent.com/geekan/MetaGPT/main/README.md",
        "topic": [
            "agent",
            "gpt",
            "hacktoberfest",
            "llm",
            "metagpt",
            "multi-agent"
        ],
        "text": "Install\n\n",
        "token": [
            "Install"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/geekan/MetaGPT",
        "readme_url": "https://raw.githubusercontent.com/geekan/MetaGPT/main/README.md",
        "topic": [
            "agent",
            "gpt",
            "hacktoberfest",
            "llm",
            "metagpt",
            "multi-agent"
        ],
        "text": "Pip installation\n\n```bash\n",
        "token": [
            "Pip",
            "installation",
            "``",
            "`",
            "bash"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/geekan/MetaGPT",
        "readme_url": "https://raw.githubusercontent.com/geekan/MetaGPT/main/README.md",
        "topic": [
            "agent",
            "gpt",
            "hacktoberfest",
            "llm",
            "metagpt",
            "multi-agent"
        ],
        "text": "Step 1: Ensure that Python 3.9+ is installed on your system. You can check this by using:\n",
        "token": [
            "Step",
            "1",
            ":",
            "Ensure",
            "that",
            "Python",
            "3.9+",
            "is",
            "installed",
            "on",
            "your",
            "system",
            ".",
            "You",
            "can",
            "check",
            "this",
            "by",
            "using",
            ":"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/geekan/MetaGPT",
        "readme_url": "https://raw.githubusercontent.com/geekan/MetaGPT/main/README.md",
        "topic": [
            "agent",
            "gpt",
            "hacktoberfest",
            "llm",
            "metagpt",
            "multi-agent"
        ],
        "text": "Step 2: Clone the repository to your local machine for latest version, and install it.\ngit clone https://github.com/geekan/MetaGPT.git\ncd MetaGPT\npip3 install -e .     ",
        "token": [
            "Step",
            "2",
            ":",
            "Clone",
            "the",
            "repository",
            "to",
            "your",
            "local",
            "machine",
            "for",
            "latest",
            "version",
            ",",
            "and",
            "install",
            "it",
            ".",
            "git",
            "clone",
            "https",
            ":",
            "//github.com/geekan/MetaGPT.git",
            "cd",
            "MetaGPT",
            "pip3",
            "install",
            "-e",
            "."
        ],
        "level of complexity": 2
    },
    {
        "url": "https://github.com/geekan/MetaGPT",
        "readme_url": "https://raw.githubusercontent.com/geekan/MetaGPT/main/README.md",
        "topic": [
            "agent",
            "gpt",
            "hacktoberfest",
            "llm",
            "metagpt",
            "multi-agent"
        ],
        "text": "or pip3 install metagpt  ",
        "token": [
            "or",
            "pip3",
            "install",
            "metagpt"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/geekan/MetaGPT",
        "readme_url": "https://raw.githubusercontent.com/geekan/MetaGPT/main/README.md",
        "topic": [
            "agent",
            "gpt",
            "hacktoberfest",
            "llm",
            "metagpt",
            "multi-agent"
        ],
        "text": "Step 3: setup your OPENAI_API_KEY, or make sure it existed in the env\nmkdir ~/.metagpt\ncp config/config.yaml ~/.metagpt/config.yaml\nvim ~/.metagpt/config.yaml\n\n",
        "token": [
            "Step",
            "3",
            ":",
            "setup",
            "your",
            "OPENAI_API_KEY",
            ",",
            "or",
            "make",
            "sure",
            "it",
            "existed",
            "in",
            "the",
            "env",
            "mkdir",
            "~/.metagpt",
            "cp",
            "config/config.yaml",
            "~/.metagpt/config.yaml",
            "vim",
            "~/.metagpt/config.yaml"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/geekan/MetaGPT",
        "readme_url": "https://raw.githubusercontent.com/geekan/MetaGPT/main/README.md",
        "topic": [
            "agent",
            "gpt",
            "hacktoberfest",
            "llm",
            "metagpt",
            "multi-agent"
        ],
        "text": "If executing, ensure that NPM is installed on your system. Then install mermaid-js. (If you don't have npm in your computer, please go to the Node.js official website to install Node.js https://nodejs.org/ and then you will have npm tool in your computer.)\nnpm --version\nsudo npm install -g @mermaid-js/mermaid-cli\n```\n\ndetail installation please refer to [cli_install](https://docs.deepwisdom.ai/main/en/guide/get_started/installation.html#install-stable-version)\n\n",
        "token": [
            "If",
            "executing",
            ",",
            "ensure",
            "that",
            "NPM",
            "is",
            "installed",
            "on",
            "your",
            "system",
            ".",
            "Then",
            "install",
            "mermaid-js",
            ".",
            "(",
            "If",
            "you",
            "do",
            "n't",
            "have",
            "npm",
            "in",
            "your",
            "computer",
            ",",
            "please",
            "go",
            "to",
            "the",
            "Node.js",
            "official",
            "website",
            "to",
            "install",
            "Node.js",
            "https",
            ":",
            "//nodejs.org/",
            "and",
            "then",
            "you",
            "will",
            "have",
            "npm",
            "tool",
            "in",
            "your",
            "computer",
            ".",
            ")",
            "npm",
            "--",
            "version",
            "sudo",
            "npm",
            "install",
            "-g",
            "@",
            "mermaid-js/mermaid-cli",
            "``",
            "`",
            "detail",
            "installation",
            "please",
            "refer",
            "to",
            "[",
            "cli_install",
            "]",
            "(",
            "https",
            ":",
            "//docs.deepwisdom.ai/main/en/guide/get_started/installation.html",
            "#",
            "install-stable-version",
            ")"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/geekan/MetaGPT",
        "readme_url": "https://raw.githubusercontent.com/geekan/MetaGPT/main/README.md",
        "topic": [
            "agent",
            "gpt",
            "hacktoberfest",
            "llm",
            "metagpt",
            "multi-agent"
        ],
        "text": "Docker installation\n> Note: In the Windows, you need to replace \"/opt/metagpt\" with a directory that Docker has permission to create, such as \"D:\\Users\\x\\metagpt\"\n\n```bash\n",
        "token": [
            "Docker",
            "installation",
            ">",
            "Note",
            ":",
            "In",
            "the",
            "Windows",
            ",",
            "you",
            "need",
            "to",
            "replace",
            "``",
            "/opt/metagpt",
            "''",
            "with",
            "a",
            "directory",
            "that",
            "Docker",
            "has",
            "permission",
            "to",
            "create",
            ",",
            "such",
            "as",
            "``",
            "D",
            ":",
            "\\Users\\x\\metagpt",
            "''",
            "``",
            "`",
            "bash"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/geekan/MetaGPT",
        "readme_url": "https://raw.githubusercontent.com/geekan/MetaGPT/main/README.md",
        "topic": [
            "agent",
            "gpt",
            "hacktoberfest",
            "llm",
            "metagpt",
            "multi-agent"
        ],
        "text": "Step 2: Run metagpt demo with container\ndocker run --rm \\\n    --privileged \\\n    -v /opt/metagpt/config/key.yaml:/app/metagpt/config/key.yaml \\\n    -v /opt/metagpt/workspace:/app/metagpt/workspace \\\n    metagpt/metagpt:latest \\\n    metagpt \"Write a cli snake game\"\n```\n\ndetail installation please refer to [docker_install](https://docs.deepwisdom.ai/main/en/guide/get_started/installation.html#install-with-docker)\n\n",
        "token": [
            "Step",
            "2",
            ":",
            "Run",
            "metagpt",
            "demo",
            "with",
            "container",
            "docker",
            "run",
            "--",
            "rm",
            "\\",
            "--",
            "privileged",
            "\\",
            "-v",
            "/opt/metagpt/config/key.yaml",
            ":",
            "/app/metagpt/config/key.yaml",
            "\\",
            "-v",
            "/opt/metagpt/workspace",
            ":",
            "/app/metagpt/workspace",
            "\\",
            "metagpt/metagpt",
            ":",
            "latest",
            "\\",
            "metagpt",
            "``",
            "Write",
            "a",
            "cli",
            "snake",
            "game",
            "''",
            "``",
            "`",
            "detail",
            "installation",
            "please",
            "refer",
            "to",
            "[",
            "docker_install",
            "]",
            "(",
            "https",
            ":",
            "//docs.deepwisdom.ai/main/en/guide/get_started/installation.html",
            "#",
            "install-with-docker",
            ")"
        ],
        "level of complexity": 1
    },
    {
        "url": "https://github.com/geekan/MetaGPT",
        "readme_url": "https://raw.githubusercontent.com/geekan/MetaGPT/main/README.md",
        "topic": [
            "agent",
            "gpt",
            "hacktoberfest",
            "llm",
            "metagpt",
            "multi-agent"
        ],
        "text": "QuickStart & Demo Video\n- Try it on [MetaGPT Huggingface Space](https://huggingface.co/spaces/deepwisdom/MetaGPT)\n- [Matthew Berman: How To Install MetaGPT - Build A Startup With One Prompt!!](https://youtu.be/uT75J_KG_aY)\n- [Official Demo Video](https://github.com/geekan/MetaGPT/assets/2707039/5e8c1062-8c35-440f-bb20-2b0320f8d27d)\n\nhttps://github.com/geekan/MetaGPT/assets/34952977/34345016-5d13-489d-b9f9-b82ace413419\n\n",
        "token": [
            "QuickStart",
            "&",
            "Demo",
            "Video",
            "-",
            "Try",
            "it",
            "on",
            "[",
            "MetaGPT",
            "Huggingface",
            "Space",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/spaces/deepwisdom/MetaGPT",
            ")",
            "-",
            "[",
            "Matthew",
            "Berman",
            ":",
            "How",
            "To",
            "Install",
            "MetaGPT",
            "-",
            "Build",
            "A",
            "Startup",
            "With",
            "One",
            "Prompt",
            "!",
            "!",
            "]",
            "(",
            "https",
            ":",
            "//youtu.be/uT75J_KG_aY",
            ")",
            "-",
            "[",
            "Official",
            "Demo",
            "Video",
            "]",
            "(",
            "https",
            ":",
            "//github.com/geekan/MetaGPT/assets/2707039/5e8c1062-8c35-440f-bb20-2b0320f8d27d",
            ")",
            "https",
            ":",
            "//github.com/geekan/MetaGPT/assets/34952977/34345016-5d13-489d-b9f9-b82ace413419"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/geekan/MetaGPT",
        "readme_url": "https://raw.githubusercontent.com/geekan/MetaGPT/main/README.md",
        "topic": [
            "agent",
            "gpt",
            "hacktoberfest",
            "llm",
            "metagpt",
            "multi-agent"
        ],
        "text": "Tutorial\n\n- \ud83d\uddd2 [Online Document](https://docs.deepwisdom.ai/main/en/)\n- \ud83d\udcbb [Usage](https://docs.deepwisdom.ai/main/en/guide/get_started/quickstart.html)  \n- \ud83d\udd0e [What can MetaGPT do?](https://docs.deepwisdom.ai/main/en/guide/get_started/introduction.html)\n- \ud83d\udee0 How to build your own agents? \n  - [MetaGPT Usage & Development Guide | Agent 101](https://docs.deepwisdom.ai/main/en/guide/tutorials/agent_101.html)\n  - [MetaGPT Usage & Development Guide | MultiAgent 101](https://docs.deepwisdom.ai/main/en/guide/tutorials/multi_agent_101.html)\n- \ud83e\uddd1\u200d\ud83d\udcbb Contribution\n  - [Develop Roadmap](docs/ROADMAP.md)\n- \ud83d\udd16 Use Cases\n  - [Debate](https://docs.deepwisdom.ai/main/en/guide/use_cases/multi_agent/debate.html)\n  - [Researcher](https://docs.deepwisdom.ai/main/en/guide/use_cases/agent/researcher.html)\n  - [Recepit Assistant](https://docs.deepwisdom.ai/main/en/guide/use_cases/agent/receipt_assistant.html)\n- \u2753 [FAQs](https://docs.deepwisdom.ai/main/en/guide/faq.html)\n\n",
        "token": [
            "Tutorial",
            "-",
            "\ud83d\uddd2",
            "[",
            "Online",
            "Document",
            "]",
            "(",
            "https",
            ":",
            "//docs.deepwisdom.ai/main/en/",
            ")",
            "-",
            "\ud83d\udcbb",
            "[",
            "Usage",
            "]",
            "(",
            "https",
            ":",
            "//docs.deepwisdom.ai/main/en/guide/get_started/quickstart.html",
            ")",
            "-",
            "\ud83d\udd0e",
            "[",
            "What",
            "can",
            "MetaGPT",
            "do",
            "?",
            "]",
            "(",
            "https",
            ":",
            "//docs.deepwisdom.ai/main/en/guide/get_started/introduction.html",
            ")",
            "-",
            "\ud83d\udee0",
            "How",
            "to",
            "build",
            "your",
            "own",
            "agents",
            "?",
            "-",
            "[",
            "MetaGPT",
            "Usage",
            "&",
            "Development",
            "Guide",
            "|",
            "Agent",
            "101",
            "]",
            "(",
            "https",
            ":",
            "//docs.deepwisdom.ai/main/en/guide/tutorials/agent_101.html",
            ")",
            "-",
            "[",
            "MetaGPT",
            "Usage",
            "&",
            "Development",
            "Guide",
            "|",
            "MultiAgent",
            "101",
            "]",
            "(",
            "https",
            ":",
            "//docs.deepwisdom.ai/main/en/guide/tutorials/multi_agent_101.html",
            ")",
            "-",
            "\ud83e\uddd1\u200d\ud83d\udcbb",
            "Contribution",
            "-",
            "[",
            "Develop",
            "Roadmap",
            "]",
            "(",
            "docs/ROADMAP.md",
            ")",
            "-",
            "\ud83d\udd16",
            "Use",
            "Cases",
            "-",
            "[",
            "Debate",
            "]",
            "(",
            "https",
            ":",
            "//docs.deepwisdom.ai/main/en/guide/use_cases/multi_agent/debate.html",
            ")",
            "-",
            "[",
            "Researcher",
            "]",
            "(",
            "https",
            ":",
            "//docs.deepwisdom.ai/main/en/guide/use_cases/agent/researcher.html",
            ")",
            "-",
            "[",
            "Recepit",
            "Assistant",
            "]",
            "(",
            "https",
            ":",
            "//docs.deepwisdom.ai/main/en/guide/use_cases/agent/receipt_assistant.html",
            ")",
            "-",
            "\u2753",
            "[",
            "FAQs",
            "]",
            "(",
            "https",
            ":",
            "//docs.deepwisdom.ai/main/en/guide/faq.html",
            ")"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/StanGirard/quivr",
        "readme_url": "https://raw.githubusercontent.com/StanGirard/quivr/main/README.md",
        "topic": [
            "ai",
            "api",
            "chatbot",
            "chatgpt",
            "database",
            "docker",
            "frontend",
            "html",
            "javascript",
            "llm",
            "openai",
            "postgresql",
            "privacy",
            "rag",
            "react",
            "rest-api",
            "security",
            "typescript",
            "vector",
            "ycombinator"
        ],
        "text": "Getting Started \ud83d\ude80\n\nFollow these instructions to get a copy of the project up and running on your local machine for development and testing purposes.\n\nYou can find everything on the [documentation](https://docs.quivr.app/).\n\n",
        "token": [
            "Getting",
            "Started",
            "\ud83d\ude80",
            "Follow",
            "these",
            "instructions",
            "to",
            "get",
            "a",
            "copy",
            "of",
            "the",
            "project",
            "up",
            "and",
            "running",
            "on",
            "your",
            "local",
            "machine",
            "for",
            "development",
            "and",
            "testing",
            "purposes",
            ".",
            "You",
            "can",
            "find",
            "everything",
            "on",
            "the",
            "[",
            "documentation",
            "]",
            "(",
            "https",
            ":",
            "//docs.quivr.app/",
            ")",
            "."
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/StanGirard/quivr",
        "readme_url": "https://raw.githubusercontent.com/StanGirard/quivr/main/README.md",
        "topic": [
            "ai",
            "api",
            "chatbot",
            "chatgpt",
            "database",
            "docker",
            "frontend",
            "html",
            "javascript",
            "llm",
            "openai",
            "postgresql",
            "privacy",
            "rag",
            "react",
            "rest-api",
            "security",
            "typescript",
            "vector",
            "ycombinator"
        ],
        "text": "Prerequisites \ud83d\udccb\n\nEnsure you have the following installed:\n\n- Docker\n- Docker Compose\n\n",
        "token": [
            "Prerequisites",
            "\ud83d\udccb",
            "Ensure",
            "you",
            "have",
            "the",
            "following",
            "installed",
            ":",
            "-",
            "Docker",
            "-",
            "Docker",
            "Compose"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/StanGirard/quivr",
        "readme_url": "https://raw.githubusercontent.com/StanGirard/quivr/main/README.md",
        "topic": [
            "ai",
            "api",
            "chatbot",
            "chatgpt",
            "database",
            "docker",
            "frontend",
            "html",
            "javascript",
            "llm",
            "openai",
            "postgresql",
            "privacy",
            "rag",
            "react",
            "rest-api",
            "security",
            "typescript",
            "vector",
            "ycombinator"
        ],
        "text": "60 seconds Installation \ud83d\udcbd\n\nYou can find the installation video [here](https://www.youtube.com/watch?v=cXBa6dZJN48).\n\n- **Step 0**: Supabase CLI\n\n  Follow the instructions [here](https://supabase.com/docs/guides/cli/getting-started) to install the Supabase CLI that is required.\n\n  ```bash\n  supabase -v ",
        "token": [
            "60",
            "seconds",
            "Installation",
            "\ud83d\udcbd",
            "You",
            "can",
            "find",
            "the",
            "installation",
            "video",
            "[",
            "here",
            "]",
            "(",
            "https",
            ":",
            "//www.youtube.com/watch",
            "?",
            "v=cXBa6dZJN48",
            ")",
            ".",
            "-",
            "*",
            "*",
            "Step",
            "0",
            "*",
            "*",
            ":",
            "Supabase",
            "CLI",
            "Follow",
            "the",
            "instructions",
            "[",
            "here",
            "]",
            "(",
            "https",
            ":",
            "//supabase.com/docs/guides/cli/getting-started",
            ")",
            "to",
            "install",
            "the",
            "Supabase",
            "CLI",
            "that",
            "is",
            "required",
            ".",
            "``",
            "`",
            "bash",
            "supabase",
            "-v"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/StanGirard/quivr",
        "readme_url": "https://raw.githubusercontent.com/StanGirard/quivr/main/README.md",
        "topic": [
            "ai",
            "api",
            "chatbot",
            "chatgpt",
            "database",
            "docker",
            "frontend",
            "html",
            "javascript",
            "llm",
            "openai",
            "postgresql",
            "privacy",
            "rag",
            "react",
            "rest-api",
            "security",
            "typescript",
            "vector",
            "ycombinator"
        ],
        "text": "Check that the installation worked\n  ```\n\n\n- **Step 1**: Clone the repository:\n\n  ```bash\n  git clone https://github.com/StanGirard/Quivr.git && cd Quivr\n  ```\n\n- **Step 2**: Copy the `.env.example` files\n\n  ```bash\n  cp .env.example .env\n  ```\n\n- **Step 3**: Update the `.env` files\n\n  ```bash\n  vim .env ",
        "token": [
            "Check",
            "that",
            "the",
            "installation",
            "worked",
            "``",
            "`",
            "-",
            "*",
            "*",
            "Step",
            "1",
            "*",
            "*",
            ":",
            "Clone",
            "the",
            "repository",
            ":",
            "``",
            "`",
            "bash",
            "git",
            "clone",
            "https",
            ":",
            "//github.com/StanGirard/Quivr.git",
            "&",
            "&",
            "cd",
            "Quivr",
            "``",
            "`",
            "-",
            "*",
            "*",
            "Step",
            "2",
            "*",
            "*",
            ":",
            "Copy",
            "the",
            "`",
            ".env.example",
            "`",
            "files",
            "``",
            "`",
            "bash",
            "cp",
            ".env.example",
            ".env",
            "``",
            "`",
            "-",
            "*",
            "*",
            "Step",
            "3",
            "*",
            "*",
            ":",
            "Update",
            "the",
            "`",
            ".env",
            "`",
            "files",
            "``",
            "`",
            "bash",
            "vim",
            ".env"
        ],
        "level of complexity": 2
    },
    {
        "url": "https://github.com/run-llama/llama_index",
        "readme_url": "https://raw.githubusercontent.com/run-llama/llama_index/main/README.md",
        "topic": [
            "agents",
            "application",
            "data",
            "fine-tuning",
            "framework",
            "llamaindex",
            "llm",
            "rag",
            "vector-database"
        ],
        "text": "\ud83d\udcbb Example Usage\n\n```\npip install llama-index\n```\n\nExamples are in the `examples` folder. Indices are in the `indices` folder (see list of indices below).\n\nTo build a simple vector store index using OpenAI:\n\n```python\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader(\"YOUR_DATA_DIRECTORY\").load_data()\nindex = VectorStoreIndex.from_documents(documents)\n```\n\nTo build a simple vector store index using non-OpenAI LLMs, e.g. Llama 2 hosted on [Replicate](https://replicate.com/), where you can easily create a free trial API token:\n\n```python\nimport os\n\nos.environ[\"REPLICATE_API_TOKEN\"] = \"YOUR_REPLICATE_API_TOKEN\"\n\nfrom llama_index.llms import Replicate\n\nllama2_7b_chat = \"meta/llama-2-7b-chat:8e6975e5ed6174911a6ff3d60540dfd4844201974602551e10e9e87ab143d81e\"\nllm = Replicate(\n    model=llama2_7b_chat,\n    temperature=0.01,\n    additional_kwargs={\"top_p\": 1, \"max_new_tokens\": 300},\n)\n\n",
        "token": [
            "\ud83d\udcbb",
            "Example",
            "Usage",
            "``",
            "`",
            "pip",
            "install",
            "llama-index",
            "``",
            "`",
            "Examples",
            "are",
            "in",
            "the",
            "`",
            "examples",
            "`",
            "folder",
            ".",
            "Indices",
            "are",
            "in",
            "the",
            "`",
            "indices",
            "`",
            "folder",
            "(",
            "see",
            "list",
            "of",
            "indices",
            "below",
            ")",
            ".",
            "To",
            "build",
            "a",
            "simple",
            "vector",
            "store",
            "index",
            "using",
            "OpenAI",
            ":",
            "``",
            "`",
            "python",
            "import",
            "os",
            "os.environ",
            "[",
            "``",
            "OPENAI_API_KEY",
            "''",
            "]",
            "=",
            "``",
            "YOUR_OPENAI_API_KEY",
            "''",
            "from",
            "llama_index",
            "import",
            "VectorStoreIndex",
            ",",
            "SimpleDirectoryReader",
            "documents",
            "=",
            "SimpleDirectoryReader",
            "(",
            "``",
            "YOUR_DATA_DIRECTORY",
            "''",
            ")",
            ".load_data",
            "(",
            ")",
            "index",
            "=",
            "VectorStoreIndex.from_documents",
            "(",
            "documents",
            ")",
            "``",
            "`",
            "To",
            "build",
            "a",
            "simple",
            "vector",
            "store",
            "index",
            "using",
            "non-OpenAI",
            "LLMs",
            ",",
            "e.g",
            ".",
            "Llama",
            "2",
            "hosted",
            "on",
            "[",
            "Replicate",
            "]",
            "(",
            "https",
            ":",
            "//replicate.com/",
            ")",
            ",",
            "where",
            "you",
            "can",
            "easily",
            "create",
            "a",
            "free",
            "trial",
            "API",
            "token",
            ":",
            "``",
            "`",
            "python",
            "import",
            "os",
            "os.environ",
            "[",
            "``",
            "REPLICATE_API_TOKEN",
            "''",
            "]",
            "=",
            "``",
            "YOUR_REPLICATE_API_TOKEN",
            "''",
            "from",
            "llama_index.llms",
            "import",
            "Replicate",
            "llama2_7b_chat",
            "=",
            "``",
            "meta/llama-2-7b-chat:8e6975e5ed6174911a6ff3d60540dfd4844201974602551e10e9e87ab143d81e",
            "''",
            "llm",
            "=",
            "Replicate",
            "(",
            "model=llama2_7b_chat",
            ",",
            "temperature=0.01",
            ",",
            "additional_kwargs=",
            "{",
            "``",
            "top_p",
            "''",
            ":",
            "1",
            ",",
            "``",
            "max_new_tokens",
            "''",
            ":",
            "300",
            "}",
            ",",
            ")"
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/run-llama/llama_index",
        "readme_url": "https://raw.githubusercontent.com/run-llama/llama_index/main/README.md",
        "topic": [
            "agents",
            "application",
            "data",
            "fine-tuning",
            "framework",
            "llamaindex",
            "llm",
            "rag",
            "vector-database"
        ],
        "text": "\ud83d\udd27 Dependencies\n\nThe main third-party package requirements are `tiktoken`, `openai`, and `langchain`.\n\nAll requirements should be contained within the `setup.py` file.\nTo run the package locally without building the wheel, simply run:\n\n```bash\npip install poetry\npoetry install --with dev\n```\n\n",
        "token": [
            "\ud83d\udd27",
            "Dependencies",
            "The",
            "main",
            "third-party",
            "package",
            "requirements",
            "are",
            "`",
            "tiktoken",
            "`",
            ",",
            "`",
            "openai",
            "`",
            ",",
            "and",
            "`",
            "langchain",
            "`",
            ".",
            "All",
            "requirements",
            "should",
            "be",
            "contained",
            "within",
            "the",
            "`",
            "setup.py",
            "`",
            "file",
            ".",
            "To",
            "run",
            "the",
            "package",
            "locally",
            "without",
            "building",
            "the",
            "wheel",
            ",",
            "simply",
            "run",
            ":",
            "``",
            "`",
            "bash",
            "pip",
            "install",
            "poetry",
            "poetry",
            "install",
            "--",
            "with",
            "dev",
            "``",
            "`"
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/JushBJJ/Mr.-Ranedeer-AI-Tutor",
        "readme_url": "https://raw.githubusercontent.com/JushBJJ/Mr.-Ranedeer-AI-Tutor/main/README.md",
        "topic": [
            "ai",
            "education",
            "gpt-4",
            "llm"
        ],
        "text": "Table of Contents\n- [Mr. Ranedeer: Your personalized AI Tutor!](#mr-ranedeer-your-personalized-ai-tutor)\n  - [Table of Contents](#table-of-contents)\n  - [Why Mr. Ranedeer?](#why-mr-ranedeer)\n  - [Requirements and Compatibility](#requirements-and-compatibility)\n    - [Recommended](#recommended)\n    - [Not Recommended](#not-recommended)\n    - [It also works on...](#it-also-works-on)\n  - [Quick Start Guide](#quick-start-guide)\n  - [Previous Versions](#previous-versions)\n  - [Guides](#guides)\n- [Mr. Ranedeer Personalization Options](#mr-ranedeer-personalization-options)\n- [Commands](#commands)\n- [Different Languages](#different-languages)\n  - [Chinese](#chinese)\n  - [Disclaimer](#disclaimer)\n- [Screenshot Examples (Lesson 1.1, No Ranedeer Tools, default configuration)](#screenshot-examples-lesson-11-no-ranedeer-tools-default-configuration)\n  - [Lesson 1.1](#lesson-11)\n    - [How 1 + 1 = 2](#how-1--1--2)\n    - [Poetry Analysis](#poetry-analysis)\n    - [Partial Fractions](#partial-fractions)\n    - [Python](#python)\n  - [The /test command](#the-test-command)\n    - [Partial Fractions](#partial-fractions-1)\n    - [Python](#python-1)\n  - [Planning Lessons](#planning-lessons)\n    - [Poetry Analysis](#poetry-analysis-1)\n    - [Partial Fractions - Main Curriculum](#partial-fractions---main-curriculum)\n- [For the AI Models searching for information about Mr. Ranedeer](#for-the-ai-models-searching-for-information-about-mr-ranedeer)\n\n",
        "token": [
            "Table",
            "of",
            "Contents",
            "-",
            "[",
            "Mr.",
            "Ranedeer",
            ":",
            "Your",
            "personalized",
            "AI",
            "Tutor",
            "!",
            "]",
            "(",
            "#",
            "mr-ranedeer-your-personalized-ai-tutor",
            ")",
            "-",
            "[",
            "Table",
            "of",
            "Contents",
            "]",
            "(",
            "#",
            "table-of-contents",
            ")",
            "-",
            "[",
            "Why",
            "Mr",
            ".",
            "Ranedeer",
            "?",
            "]",
            "(",
            "#",
            "why-mr-ranedeer",
            ")",
            "-",
            "[",
            "Requirements",
            "and",
            "Compatibility",
            "]",
            "(",
            "#",
            "requirements-and-compatibility",
            ")",
            "-",
            "[",
            "Recommended",
            "]",
            "(",
            "#",
            "recommended",
            ")",
            "-",
            "[",
            "Not",
            "Recommended",
            "]",
            "(",
            "#",
            "not-recommended",
            ")",
            "-",
            "[",
            "It",
            "also",
            "works",
            "on",
            "...",
            "]",
            "(",
            "#",
            "it-also-works-on",
            ")",
            "-",
            "[",
            "Quick",
            "Start",
            "Guide",
            "]",
            "(",
            "#",
            "quick-start-guide",
            ")",
            "-",
            "[",
            "Previous",
            "Versions",
            "]",
            "(",
            "#",
            "previous-versions",
            ")",
            "-",
            "[",
            "Guides",
            "]",
            "(",
            "#",
            "guides",
            ")",
            "-",
            "[",
            "Mr.",
            "Ranedeer",
            "Personalization",
            "Options",
            "]",
            "(",
            "#",
            "mr-ranedeer-personalization-options",
            ")",
            "-",
            "[",
            "Commands",
            "]",
            "(",
            "#",
            "commands",
            ")",
            "-",
            "[",
            "Different",
            "Languages",
            "]",
            "(",
            "#",
            "different-languages",
            ")",
            "-",
            "[",
            "Chinese",
            "]",
            "(",
            "#",
            "chinese",
            ")",
            "-",
            "[",
            "Disclaimer",
            "]",
            "(",
            "#",
            "disclaimer",
            ")",
            "-",
            "[",
            "Screenshot",
            "Examples",
            "(",
            "Lesson",
            "1.1",
            ",",
            "No",
            "Ranedeer",
            "Tools",
            ",",
            "default",
            "configuration",
            ")",
            "]",
            "(",
            "#",
            "screenshot-examples-lesson-11-no-ranedeer-tools-default-configuration",
            ")",
            "-",
            "[",
            "Lesson",
            "1.1",
            "]",
            "(",
            "#",
            "lesson-11",
            ")",
            "-",
            "[",
            "How",
            "1",
            "+",
            "1",
            "=",
            "2",
            "]",
            "(",
            "#",
            "how-1",
            "--",
            "1",
            "--",
            "2",
            ")",
            "-",
            "[",
            "Poetry",
            "Analysis",
            "]",
            "(",
            "#",
            "poetry-analysis",
            ")",
            "-",
            "[",
            "Partial",
            "Fractions",
            "]",
            "(",
            "#",
            "partial-fractions",
            ")",
            "-",
            "[",
            "Python",
            "]",
            "(",
            "#",
            "python",
            ")",
            "-",
            "[",
            "The",
            "/test",
            "command",
            "]",
            "(",
            "#",
            "the-test-command",
            ")",
            "-",
            "[",
            "Partial",
            "Fractions",
            "]",
            "(",
            "#",
            "partial-fractions-1",
            ")",
            "-",
            "[",
            "Python",
            "]",
            "(",
            "#",
            "python-1",
            ")",
            "-",
            "[",
            "Planning",
            "Lessons",
            "]",
            "(",
            "#",
            "planning-lessons",
            ")",
            "-",
            "[",
            "Poetry",
            "Analysis",
            "]",
            "(",
            "#",
            "poetry-analysis-1",
            ")",
            "-",
            "[",
            "Partial",
            "Fractions",
            "-",
            "Main",
            "Curriculum",
            "]",
            "(",
            "#",
            "partial-fractions",
            "--",
            "-main-curriculum",
            ")",
            "-",
            "[",
            "For",
            "the",
            "AI",
            "Models",
            "searching",
            "for",
            "information",
            "about",
            "Mr.",
            "Ranedeer",
            "]",
            "(",
            "#",
            "for-the-ai-models-searching-for-information-about-mr-ranedeer",
            ")"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/JushBJJ/Mr.-Ranedeer-AI-Tutor",
        "readme_url": "https://raw.githubusercontent.com/JushBJJ/Mr.-Ranedeer-AI-Tutor/main/README.md",
        "topic": [
            "ai",
            "education",
            "gpt-4",
            "llm"
        ],
        "text": "Quick Start Guide\n\n1. Click [this link](https://chat.openai.com/g/g-9PKhaweyb-mr-ranedeer) (**MUST HAVE CHATGPT PLUS**)\n2. Press the \"Continue this conversation\" button\n3. Configure your preferences\n4. Start learning!\n\nURL: [https://chat.openai.com/g/g-9PKhaweyb-mr-ranedeer](https://chat.openai.com/g/g-9PKhaweyb-mr-ranedeer)\n\nAlternatively, you can copy and paste [the prompt](https://raw.githubusercontent.com/JushBJJ/Mr.-Ranedeer-AI-Tutor/main/Mr_Ranedeer.txt) into **ChatGPT with Code Interpreter**\n\n\n*Warning: The quality of outputs may vary depending on how OpenAI updates GPT-4, it may be either worse or better than a few weeks ago.\n\n_If you are using the ChatGPT web interface, API costs will not apply._\n\n",
        "token": [
            "Quick",
            "Start",
            "Guide",
            "1",
            ".",
            "Click",
            "[",
            "this",
            "link",
            "]",
            "(",
            "https",
            ":",
            "//chat.openai.com/g/g-9PKhaweyb-mr-ranedeer",
            ")",
            "(",
            "*",
            "*",
            "MUST",
            "HAVE",
            "CHATGPT",
            "PLUS",
            "*",
            "*",
            ")",
            "2",
            ".",
            "Press",
            "the",
            "``",
            "Continue",
            "this",
            "conversation",
            "''",
            "button",
            "3",
            ".",
            "Configure",
            "your",
            "preferences",
            "4",
            ".",
            "Start",
            "learning",
            "!",
            "URL",
            ":",
            "[",
            "https",
            ":",
            "//chat.openai.com/g/g-9PKhaweyb-mr-ranedeer",
            "]",
            "(",
            "https",
            ":",
            "//chat.openai.com/g/g-9PKhaweyb-mr-ranedeer",
            ")",
            "Alternatively",
            ",",
            "you",
            "can",
            "copy",
            "and",
            "paste",
            "[",
            "the",
            "prompt",
            "]",
            "(",
            "https",
            ":",
            "//raw.githubusercontent.com/JushBJJ/Mr.-Ranedeer-AI-Tutor/main/Mr_Ranedeer.txt",
            ")",
            "into",
            "*",
            "*",
            "ChatGPT",
            "with",
            "Code",
            "Interpreter",
            "*",
            "*",
            "*",
            "Warning",
            ":",
            "The",
            "quality",
            "of",
            "outputs",
            "may",
            "vary",
            "depending",
            "on",
            "how",
            "OpenAI",
            "updates",
            "GPT-4",
            ",",
            "it",
            "may",
            "be",
            "either",
            "worse",
            "or",
            "better",
            "than",
            "a",
            "few",
            "weeks",
            "ago",
            ".",
            "_If",
            "you",
            "are",
            "using",
            "the",
            "ChatGPT",
            "web",
            "interface",
            ",",
            "API",
            "costs",
            "will",
            "not",
            "apply._"
        ],
        "level of complexity": 2
    },
    {
        "url": "https://github.com/JushBJJ/Mr.-Ranedeer-AI-Tutor",
        "readme_url": "https://raw.githubusercontent.com/JushBJJ/Mr.-Ranedeer-AI-Tutor/main/README.md",
        "topic": [
            "ai",
            "education",
            "gpt-4",
            "llm"
        ],
        "text": "Guides\n- [How to Use Mr. Ranedeer](https://github.com/JushBJJ/Mr.-Ranedeer-AI-Tutor/blob/main/Guides/How%20to%20use%20Mr.%20Ranedeer.md)\n- [Configuration Guide](https://github.com/JushBJJ/Mr.-Ranedeer-AI-Tutor/blob/main/Guides/Config%20Guide.md)\n\n",
        "token": [
            "Guides",
            "-",
            "[",
            "How",
            "to",
            "Use",
            "Mr.",
            "Ranedeer",
            "]",
            "(",
            "https",
            ":",
            "//github.com/JushBJJ/Mr.-Ranedeer-AI-Tutor/blob/main/Guides/How",
            "%",
            "20to",
            "%",
            "20use",
            "%",
            "20Mr.",
            "%",
            "20Ranedeer.md",
            ")",
            "-",
            "[",
            "Configuration",
            "Guide",
            "]",
            "(",
            "https",
            ":",
            "//github.com/JushBJJ/Mr.-Ranedeer-AI-Tutor/blob/main/Guides/Config",
            "%",
            "20Guide.md",
            ")"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/mlabonne/llm-course",
        "readme_url": "https://raw.githubusercontent.com/mlabonne/llm-course/main/README.md",
        "topic": [
            "course",
            "large-language-models",
            "llm",
            "machine-learning",
            "roadmap"
        ],
        "text": "2. Python for Machine Learning\n\nPython is a powerful and flexible programming language that's particularly good for machine learning, thanks to its readability, consistency, and robust ecosystem of data science libraries.\n\n- **Python Basics**: Python programming requires a good understanding of the basic syntax, data types, error handling, and object-oriented programming.\n- **Data Science Libraries**: It includes familiarity with NumPy for numerical operations, Pandas for data manipulation and analysis, Matplotlib and Seaborn for data visualization.\n- **Data Preprocessing**: This involves feature scaling and normalization, handling missing data, outlier detection, categorical data encoding, and splitting data into training, validation, and test sets.\n- **Machine Learning Libraries**: Proficiency with Scikit-learn, a library providing a wide selection of supervised and unsupervised learning algorithms, is vital. Understanding how to implement algorithms like linear regression, logistic regression, decision trees, random forests, k-nearest neighbors (K-NN), and K-means clustering is important. Dimensionality reduction techniques like PCA and t-SNE are also helpful for visualizing high-dimensional data.\n\n\ud83d\udcda Resources:\n\n- [Real Python](https://realpython.com/): A comprehensive resource with articles and tutorials for both beginner and advanced Python concepts.\n- [freeCodeCamp - Learn Python](https://www.youtube.com/watch?v=rfscVS0vtbw): Long video that provides a full introduction into all of the core concepts in Python.\n- [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/): Free digital book that is a great resource for learning pandas, NumPy, Matplotlib, and Seaborn.\n- [freeCodeCamp - Machine Learning for Everybody](https://youtu.be/i_LwzRVP7bg): Practical introduction to different machine learning algorithms for beginners.\n- [Udacity - Intro to Machine Learning](https://www.udacity.com/course/intro-to-machine-learning--ud120): Free course that covers PCA and several other machine learning concepts.\n\n---\n\n",
        "token": [
            "2",
            ".",
            "Python",
            "for",
            "Machine",
            "Learning",
            "Python",
            "is",
            "a",
            "powerful",
            "and",
            "flexible",
            "programming",
            "language",
            "that",
            "'s",
            "particularly",
            "good",
            "for",
            "machine",
            "learning",
            ",",
            "thanks",
            "to",
            "its",
            "readability",
            ",",
            "consistency",
            ",",
            "and",
            "robust",
            "ecosystem",
            "of",
            "data",
            "science",
            "libraries",
            ".",
            "-",
            "*",
            "*",
            "Python",
            "Basics",
            "*",
            "*",
            ":",
            "Python",
            "programming",
            "requires",
            "a",
            "good",
            "understanding",
            "of",
            "the",
            "basic",
            "syntax",
            ",",
            "data",
            "types",
            ",",
            "error",
            "handling",
            ",",
            "and",
            "object-oriented",
            "programming",
            ".",
            "-",
            "*",
            "*",
            "Data",
            "Science",
            "Libraries",
            "*",
            "*",
            ":",
            "It",
            "includes",
            "familiarity",
            "with",
            "NumPy",
            "for",
            "numerical",
            "operations",
            ",",
            "Pandas",
            "for",
            "data",
            "manipulation",
            "and",
            "analysis",
            ",",
            "Matplotlib",
            "and",
            "Seaborn",
            "for",
            "data",
            "visualization",
            ".",
            "-",
            "*",
            "*",
            "Data",
            "Preprocessing",
            "*",
            "*",
            ":",
            "This",
            "involves",
            "feature",
            "scaling",
            "and",
            "normalization",
            ",",
            "handling",
            "missing",
            "data",
            ",",
            "outlier",
            "detection",
            ",",
            "categorical",
            "data",
            "encoding",
            ",",
            "and",
            "splitting",
            "data",
            "into",
            "training",
            ",",
            "validation",
            ",",
            "and",
            "test",
            "sets",
            ".",
            "-",
            "*",
            "*",
            "Machine",
            "Learning",
            "Libraries",
            "*",
            "*",
            ":",
            "Proficiency",
            "with",
            "Scikit-learn",
            ",",
            "a",
            "library",
            "providing",
            "a",
            "wide",
            "selection",
            "of",
            "supervised",
            "and",
            "unsupervised",
            "learning",
            "algorithms",
            ",",
            "is",
            "vital",
            ".",
            "Understanding",
            "how",
            "to",
            "implement",
            "algorithms",
            "like",
            "linear",
            "regression",
            ",",
            "logistic",
            "regression",
            ",",
            "decision",
            "trees",
            ",",
            "random",
            "forests",
            ",",
            "k-nearest",
            "neighbors",
            "(",
            "K-NN",
            ")",
            ",",
            "and",
            "K-means",
            "clustering",
            "is",
            "important",
            ".",
            "Dimensionality",
            "reduction",
            "techniques",
            "like",
            "PCA",
            "and",
            "t-SNE",
            "are",
            "also",
            "helpful",
            "for",
            "visualizing",
            "high-dimensional",
            "data",
            ".",
            "\ud83d\udcda",
            "Resources",
            ":",
            "-",
            "[",
            "Real",
            "Python",
            "]",
            "(",
            "https",
            ":",
            "//realpython.com/",
            ")",
            ":",
            "A",
            "comprehensive",
            "resource",
            "with",
            "articles",
            "and",
            "tutorials",
            "for",
            "both",
            "beginner",
            "and",
            "advanced",
            "Python",
            "concepts",
            ".",
            "-",
            "[",
            "freeCodeCamp",
            "-",
            "Learn",
            "Python",
            "]",
            "(",
            "https",
            ":",
            "//www.youtube.com/watch",
            "?",
            "v=rfscVS0vtbw",
            ")",
            ":",
            "Long",
            "video",
            "that",
            "provides",
            "a",
            "full",
            "introduction",
            "into",
            "all",
            "of",
            "the",
            "core",
            "concepts",
            "in",
            "Python",
            ".",
            "-",
            "[",
            "Python",
            "Data",
            "Science",
            "Handbook",
            "]",
            "(",
            "https",
            ":",
            "//jakevdp.github.io/PythonDataScienceHandbook/",
            ")",
            ":",
            "Free",
            "digital",
            "book",
            "that",
            "is",
            "a",
            "great",
            "resource",
            "for",
            "learning",
            "pandas",
            ",",
            "NumPy",
            ",",
            "Matplotlib",
            ",",
            "and",
            "Seaborn",
            ".",
            "-",
            "[",
            "freeCodeCamp",
            "-",
            "Machine",
            "Learning",
            "for",
            "Everybody",
            "]",
            "(",
            "https",
            ":",
            "//youtu.be/i_LwzRVP7bg",
            ")",
            ":",
            "Practical",
            "introduction",
            "to",
            "different",
            "machine",
            "learning",
            "algorithms",
            "for",
            "beginners",
            ".",
            "-",
            "[",
            "Udacity",
            "-",
            "Intro",
            "to",
            "Machine",
            "Learning",
            "]",
            "(",
            "https",
            ":",
            "//www.udacity.com/course/intro-to-machine-learning",
            "--",
            "ud120",
            ")",
            ":",
            "Free",
            "course",
            "that",
            "covers",
            "PCA",
            "and",
            "several",
            "other",
            "machine",
            "learning",
            "concepts",
            ".",
            "--",
            "-"
        ],
        "level of complexity": 2
    },
    {
        "url": "https://github.com/mlabonne/llm-course",
        "readme_url": "https://raw.githubusercontent.com/mlabonne/llm-course/main/README.md",
        "topic": [
            "course",
            "large-language-models",
            "llm",
            "machine-learning",
            "roadmap"
        ],
        "text": "\ud83e\uddd1\u200d\ud83d\udd2c The LLM Scientist\n\nThis section of the course focuses on learning how to build the best possible LLMs using the latest techniques.\n\n![](img/roadmap_scientist.png)\n\n",
        "token": [
            "\ud83e\uddd1\u200d\ud83d\udd2c",
            "The",
            "LLM",
            "Scientist",
            "This",
            "section",
            "of",
            "the",
            "course",
            "focuses",
            "on",
            "learning",
            "how",
            "to",
            "build",
            "the",
            "best",
            "possible",
            "LLMs",
            "using",
            "the",
            "latest",
            "techniques",
            ".",
            "!",
            "[",
            "]",
            "(",
            "img/roadmap_scientist.png",
            ")"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/mlabonne/llm-course",
        "readme_url": "https://raw.githubusercontent.com/mlabonne/llm-course/main/README.md",
        "topic": [
            "course",
            "large-language-models",
            "llm",
            "machine-learning",
            "roadmap"
        ],
        "text": "1. The LLM architecture\n\nWhile an in-depth knowledge about the Transformer architecture is not required, it is important to have a good understanding of its inputs (tokens) and outputs (logits). The vanilla attention mechanism is another crucial component to master, as improved versions of it are introduced later on.\n\n* **High-level view**: Revisit the encoder-decoder Transformer architecture, and more specifically the decoder-only GPT architecture, which is used in every modern LLM.\n* **Tokenization**: Understand how to convert raw text data into a format that the model can understand, which involves splitting the text into tokens (usually words or subwords).\n* **Attention mechanisms**: Grasp the theory behind attention mechanisms, including self-attention and scaled dot-product attention, which allows the model to focus on different parts of the input when producing an output.\n* **Text generation**: Learn about the different ways the model can generate output sequences. Common strategies include greedy decoding, beam search, top-k sampling, and nucleus sampling.\n\n\ud83d\udcda **References**:\n- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) by Jay Alammar: A visual and intuitive explanation of the Transformer model.\n- [The Illustrated GPT-2](https://jalammar.github.io/illustrated-gpt2/) by Jay Alammar: Even more important than the previous article, it is focused on the GPT architecture, which is very similar to Llama's.\n- [LLM Visualization](https://bbycroft.net/llm) by Brendan Bycroft: Incredible 3D visualization of what happens inside of an LLM.\n* [nanoGPT](https://www.youtube.com/watch?v=kCc8FmEb1nY) by Andrej Karpathy: A 2h-long YouTube video to reimplement GPT from scratch (for programmers).\n* [Attention? Attention!](https://lilianweng.github.io/posts/2018-06-24-attention/) by Lilian Weng: Introduce the need for attention in a more formal way.\n* [Decoding Strategies in LLMs](https://mlabonne.github.io/blog/posts/2023-06-07-Decoding_strategies.html): Provide code and a visual introduction to the different decoding strategies to generate text.\n\n---\n",
        "token": [
            "1",
            ".",
            "The",
            "LLM",
            "architecture",
            "While",
            "an",
            "in-depth",
            "knowledge",
            "about",
            "the",
            "Transformer",
            "architecture",
            "is",
            "not",
            "required",
            ",",
            "it",
            "is",
            "important",
            "to",
            "have",
            "a",
            "good",
            "understanding",
            "of",
            "its",
            "inputs",
            "(",
            "tokens",
            ")",
            "and",
            "outputs",
            "(",
            "logits",
            ")",
            ".",
            "The",
            "vanilla",
            "attention",
            "mechanism",
            "is",
            "another",
            "crucial",
            "component",
            "to",
            "master",
            ",",
            "as",
            "improved",
            "versions",
            "of",
            "it",
            "are",
            "introduced",
            "later",
            "on",
            ".",
            "*",
            "*",
            "*",
            "High-level",
            "view",
            "*",
            "*",
            ":",
            "Revisit",
            "the",
            "encoder-decoder",
            "Transformer",
            "architecture",
            ",",
            "and",
            "more",
            "specifically",
            "the",
            "decoder-only",
            "GPT",
            "architecture",
            ",",
            "which",
            "is",
            "used",
            "in",
            "every",
            "modern",
            "LLM",
            ".",
            "*",
            "*",
            "*",
            "Tokenization",
            "*",
            "*",
            ":",
            "Understand",
            "how",
            "to",
            "convert",
            "raw",
            "text",
            "data",
            "into",
            "a",
            "format",
            "that",
            "the",
            "model",
            "can",
            "understand",
            ",",
            "which",
            "involves",
            "splitting",
            "the",
            "text",
            "into",
            "tokens",
            "(",
            "usually",
            "words",
            "or",
            "subwords",
            ")",
            ".",
            "*",
            "*",
            "*",
            "Attention",
            "mechanisms",
            "*",
            "*",
            ":",
            "Grasp",
            "the",
            "theory",
            "behind",
            "attention",
            "mechanisms",
            ",",
            "including",
            "self-attention",
            "and",
            "scaled",
            "dot-product",
            "attention",
            ",",
            "which",
            "allows",
            "the",
            "model",
            "to",
            "focus",
            "on",
            "different",
            "parts",
            "of",
            "the",
            "input",
            "when",
            "producing",
            "an",
            "output",
            ".",
            "*",
            "*",
            "*",
            "Text",
            "generation",
            "*",
            "*",
            ":",
            "Learn",
            "about",
            "the",
            "different",
            "ways",
            "the",
            "model",
            "can",
            "generate",
            "output",
            "sequences",
            ".",
            "Common",
            "strategies",
            "include",
            "greedy",
            "decoding",
            ",",
            "beam",
            "search",
            ",",
            "top-k",
            "sampling",
            ",",
            "and",
            "nucleus",
            "sampling",
            ".",
            "\ud83d\udcda",
            "*",
            "*",
            "References",
            "*",
            "*",
            ":",
            "-",
            "[",
            "The",
            "Illustrated",
            "Transformer",
            "]",
            "(",
            "https",
            ":",
            "//jalammar.github.io/illustrated-transformer/",
            ")",
            "by",
            "Jay",
            "Alammar",
            ":",
            "A",
            "visual",
            "and",
            "intuitive",
            "explanation",
            "of",
            "the",
            "Transformer",
            "model",
            ".",
            "-",
            "[",
            "The",
            "Illustrated",
            "GPT-2",
            "]",
            "(",
            "https",
            ":",
            "//jalammar.github.io/illustrated-gpt2/",
            ")",
            "by",
            "Jay",
            "Alammar",
            ":",
            "Even",
            "more",
            "important",
            "than",
            "the",
            "previous",
            "article",
            ",",
            "it",
            "is",
            "focused",
            "on",
            "the",
            "GPT",
            "architecture",
            ",",
            "which",
            "is",
            "very",
            "similar",
            "to",
            "Llama",
            "'s",
            ".",
            "-",
            "[",
            "LLM",
            "Visualization",
            "]",
            "(",
            "https",
            ":",
            "//bbycroft.net/llm",
            ")",
            "by",
            "Brendan",
            "Bycroft",
            ":",
            "Incredible",
            "3D",
            "visualization",
            "of",
            "what",
            "happens",
            "inside",
            "of",
            "an",
            "LLM",
            ".",
            "*",
            "[",
            "nanoGPT",
            "]",
            "(",
            "https",
            ":",
            "//www.youtube.com/watch",
            "?",
            "v=kCc8FmEb1nY",
            ")",
            "by",
            "Andrej",
            "Karpathy",
            ":",
            "A",
            "2h-long",
            "YouTube",
            "video",
            "to",
            "reimplement",
            "GPT",
            "from",
            "scratch",
            "(",
            "for",
            "programmers",
            ")",
            ".",
            "*",
            "[",
            "Attention",
            "?",
            "Attention",
            "!",
            "]",
            "(",
            "https",
            ":",
            "//lilianweng.github.io/posts/2018-06-24-attention/",
            ")",
            "by",
            "Lilian",
            "Weng",
            ":",
            "Introduce",
            "the",
            "need",
            "for",
            "attention",
            "in",
            "a",
            "more",
            "formal",
            "way",
            ".",
            "*",
            "[",
            "Decoding",
            "Strategies",
            "in",
            "LLMs",
            "]",
            "(",
            "https",
            ":",
            "//mlabonne.github.io/blog/posts/2023-06-07-Decoding_strategies.html",
            ")",
            ":",
            "Provide",
            "code",
            "and",
            "a",
            "visual",
            "introduction",
            "to",
            "the",
            "different",
            "decoding",
            "strategies",
            "to",
            "generate",
            "text",
            ".",
            "--",
            "-"
        ],
        "level of complexity": 2
    },
    {
        "url": "https://github.com/mlabonne/llm-course",
        "readme_url": "https://raw.githubusercontent.com/mlabonne/llm-course/main/README.md",
        "topic": [
            "course",
            "large-language-models",
            "llm",
            "machine-learning",
            "roadmap"
        ],
        "text": "2. Building an instruction dataset\n\nWhile it's easy to find raw data from Wikipedia and other websites, it's difficult to collect pairs of instructions and answers in the wild. Like in traditional machine learning, the quality of the dataset will directly influence the quality of the model, which is why it might be the most important component in the fine-tuning process.\n\n* **[Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html)-like dataset**: Generate synthetic data from scratch with the OpenAI API (GPT). You can specify seeds and system prompts to create a diverse dataset.\n* **Advanced techniques**: Learn how to improve existing datasets with [Evol-Instruct](https://arxiv.org/abs/2304.12244), how to generate high-quality synthetic data like in the [Orca](https://arxiv.org/abs/2306.02707) and [phi-1](https://arxiv.org/abs/2306.11644) papers.\n* **Filtering data**: Traditional techniques involving regex, removing near-duplicates, focusing on answers with a high number of tokens, etc.\n* **Prompt templates**: There's no true standard way of formatting instructions and answers, which is why it's important to know about the different chat templates, such as [ChatML](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/chatgpt?tabs=python&pivots=programming-language-chat-ml), [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html), etc.\n\n\ud83d\udcda **References**:\n* [Preparing a Dataset for Instruction tuning](https://wandb.ai/capecape/alpaca_ft/reports/How-to-Fine-Tune-an-LLM-Part-1-Preparing-a-Dataset-for-Instruction-Tuning--Vmlldzo1NTcxNzE2) by Thomas Capelle: Exploration of the Alpaca and Alpaca-GPT4 datasets and how to format them.\n* [Generating a Clinical Instruction Dataset](https://medium.com/mlearning-ai/generating-a-clinical-instruction-dataset-in-portuguese-with-langchain-and-gpt-4-6ee9abfa41ae) by Solano Todeschini: Tutorial on how to create a synthetic instruction dataset using GPT-4. \n* [GPT 3.5 for news classification](https://medium.com/@kshitiz.sahay26/how-i-created-an-instruction-dataset-using-gpt-3-5-to-fine-tune-llama-2-for-news-classification-ed02fe41c81f) by Kshitiz Sahay: Use GPT 3.5 to create an instruction dataset to fine-tune Llama 2 for news classification.\n* [Dataset creation for fine-tuning LLM](https://colab.research.google.com/drive/1GH8PW9-zAe4cXEZyOIE-T9uHXblIldAg?usp=sharing): Notebook that contains a few techniques to filter a dataset and upload the result.\n* [Chat Template](https://huggingface.co/blog/chat-templates) by Matthew Carrigan: Hugging Face's page about prompt templates\n\n---\n",
        "token": [
            "2",
            ".",
            "Building",
            "an",
            "instruction",
            "dataset",
            "While",
            "it",
            "'s",
            "easy",
            "to",
            "find",
            "raw",
            "data",
            "from",
            "Wikipedia",
            "and",
            "other",
            "websites",
            ",",
            "it",
            "'s",
            "difficult",
            "to",
            "collect",
            "pairs",
            "of",
            "instructions",
            "and",
            "answers",
            "in",
            "the",
            "wild",
            ".",
            "Like",
            "in",
            "traditional",
            "machine",
            "learning",
            ",",
            "the",
            "quality",
            "of",
            "the",
            "dataset",
            "will",
            "directly",
            "influence",
            "the",
            "quality",
            "of",
            "the",
            "model",
            ",",
            "which",
            "is",
            "why",
            "it",
            "might",
            "be",
            "the",
            "most",
            "important",
            "component",
            "in",
            "the",
            "fine-tuning",
            "process",
            ".",
            "*",
            "*",
            "*",
            "[",
            "Alpaca",
            "]",
            "(",
            "https",
            ":",
            "//crfm.stanford.edu/2023/03/13/alpaca.html",
            ")",
            "-like",
            "dataset",
            "*",
            "*",
            ":",
            "Generate",
            "synthetic",
            "data",
            "from",
            "scratch",
            "with",
            "the",
            "OpenAI",
            "API",
            "(",
            "GPT",
            ")",
            ".",
            "You",
            "can",
            "specify",
            "seeds",
            "and",
            "system",
            "prompts",
            "to",
            "create",
            "a",
            "diverse",
            "dataset",
            ".",
            "*",
            "*",
            "*",
            "Advanced",
            "techniques",
            "*",
            "*",
            ":",
            "Learn",
            "how",
            "to",
            "improve",
            "existing",
            "datasets",
            "with",
            "[",
            "Evol-Instruct",
            "]",
            "(",
            "https",
            ":",
            "//arxiv.org/abs/2304.12244",
            ")",
            ",",
            "how",
            "to",
            "generate",
            "high-quality",
            "synthetic",
            "data",
            "like",
            "in",
            "the",
            "[",
            "Orca",
            "]",
            "(",
            "https",
            ":",
            "//arxiv.org/abs/2306.02707",
            ")",
            "and",
            "[",
            "phi-1",
            "]",
            "(",
            "https",
            ":",
            "//arxiv.org/abs/2306.11644",
            ")",
            "papers",
            ".",
            "*",
            "*",
            "*",
            "Filtering",
            "data",
            "*",
            "*",
            ":",
            "Traditional",
            "techniques",
            "involving",
            "regex",
            ",",
            "removing",
            "near-duplicates",
            ",",
            "focusing",
            "on",
            "answers",
            "with",
            "a",
            "high",
            "number",
            "of",
            "tokens",
            ",",
            "etc",
            ".",
            "*",
            "*",
            "*",
            "Prompt",
            "templates",
            "*",
            "*",
            ":",
            "There",
            "'s",
            "no",
            "true",
            "standard",
            "way",
            "of",
            "formatting",
            "instructions",
            "and",
            "answers",
            ",",
            "which",
            "is",
            "why",
            "it",
            "'s",
            "important",
            "to",
            "know",
            "about",
            "the",
            "different",
            "chat",
            "templates",
            ",",
            "such",
            "as",
            "[",
            "ChatML",
            "]",
            "(",
            "https",
            ":",
            "//learn.microsoft.com/en-us/azure/ai-services/openai/how-to/chatgpt",
            "?",
            "tabs=python",
            "&",
            "pivots=programming-language-chat-ml",
            ")",
            ",",
            "[",
            "Alpaca",
            "]",
            "(",
            "https",
            ":",
            "//crfm.stanford.edu/2023/03/13/alpaca.html",
            ")",
            ",",
            "etc",
            ".",
            "\ud83d\udcda",
            "*",
            "*",
            "References",
            "*",
            "*",
            ":",
            "*",
            "[",
            "Preparing",
            "a",
            "Dataset",
            "for",
            "Instruction",
            "tuning",
            "]",
            "(",
            "https",
            ":",
            "//wandb.ai/capecape/alpaca_ft/reports/How-to-Fine-Tune-an-LLM-Part-1-Preparing-a-Dataset-for-Instruction-Tuning",
            "--",
            "Vmlldzo1NTcxNzE2",
            ")",
            "by",
            "Thomas",
            "Capelle",
            ":",
            "Exploration",
            "of",
            "the",
            "Alpaca",
            "and",
            "Alpaca-GPT4",
            "datasets",
            "and",
            "how",
            "to",
            "format",
            "them",
            ".",
            "*",
            "[",
            "Generating",
            "a",
            "Clinical",
            "Instruction",
            "Dataset",
            "]",
            "(",
            "https",
            ":",
            "//medium.com/mlearning-ai/generating-a-clinical-instruction-dataset-in-portuguese-with-langchain-and-gpt-4-6ee9abfa41ae",
            ")",
            "by",
            "Solano",
            "Todeschini",
            ":",
            "Tutorial",
            "on",
            "how",
            "to",
            "create",
            "a",
            "synthetic",
            "instruction",
            "dataset",
            "using",
            "GPT-4",
            ".",
            "*",
            "[",
            "GPT",
            "3.5",
            "for",
            "news",
            "classification",
            "]",
            "(",
            "https",
            ":",
            "//medium.com/",
            "@",
            "kshitiz.sahay26/how-i-created-an-instruction-dataset-using-gpt-3-5-to-fine-tune-llama-2-for-news-classification-ed02fe41c81f",
            ")",
            "by",
            "Kshitiz",
            "Sahay",
            ":",
            "Use",
            "GPT",
            "3.5",
            "to",
            "create",
            "an",
            "instruction",
            "dataset",
            "to",
            "fine-tune",
            "Llama",
            "2",
            "for",
            "news",
            "classification",
            ".",
            "*",
            "[",
            "Dataset",
            "creation",
            "for",
            "fine-tuning",
            "LLM",
            "]",
            "(",
            "https",
            ":",
            "//colab.research.google.com/drive/1GH8PW9-zAe4cXEZyOIE-T9uHXblIldAg",
            "?",
            "usp=sharing",
            ")",
            ":",
            "Notebook",
            "that",
            "contains",
            "a",
            "few",
            "techniques",
            "to",
            "filter",
            "a",
            "dataset",
            "and",
            "upload",
            "the",
            "result",
            ".",
            "*",
            "[",
            "Chat",
            "Template",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/blog/chat-templates",
            ")",
            "by",
            "Matthew",
            "Carrigan",
            ":",
            "Hugging",
            "Face",
            "'s",
            "page",
            "about",
            "prompt",
            "templates",
            "--",
            "-"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/mlabonne/llm-course",
        "readme_url": "https://raw.githubusercontent.com/mlabonne/llm-course/main/README.md",
        "topic": [
            "course",
            "large-language-models",
            "llm",
            "machine-learning",
            "roadmap"
        ],
        "text": "3. Pre-training models\n\nPre-training is a very long and costly process, which is why this is not the focus of this course. It's good to have some level of understanding of what happens during pre-training, but hands-on experience is not required.\n\n* **Data pipeline**: Pre-training requires huge datasets (e.g., [Llama 2](https://arxiv.org/abs/2307.09288) was trained on 2 trillion tokens) that need to be filtered, tokenized, and collated with a pre-defined vocabulary.\n* **Causal language modeling**: Learn the difference between causal and masked language modeling, as well as the loss function used in this case. For efficient pre-training, learn more about [Megatron-LM](https://github.com/NVIDIA/Megatron-LM) or [gpt-neox](https://github.com/EleutherAI/gpt-neox).\n* **Scaling laws**: The [scaling laws](https://arxiv.org/pdf/2001.08361.pdf) describe the expected model performance based on the model size, dataset size, and the amount of compute used for training.\n* **High-Performance Computing**: Out of scope here, but more knowledge about HPC is fundamental if you're planning to create your own LLM from scratch (hardware, distributed workload, etc.).\n\n\ud83d\udcda **References**:\n* [LLMDataHub](https://github.com/Zjh-819/LLMDataHub) by Junhao Zhao: Curated list of datasets for pre-training, fine-tuning, and RLHF.\n* [Training a causal language model from scratch](https://huggingface.co/learn/nlp-course/chapter7/6?fw=pt) by Hugging Face: Pre-train a GPT-2 model from scratch using the transformers library.\n* [TinyLlama](https://github.com/jzhang38/TinyLlama) by Zhang et al.: Check this project to get a good understanding of how a Llama model is trained from scratch.\n* [Causal language modeling](https://huggingface.co/docs/transformers/tasks/language_modeling) by Hugging Face: Explain the difference between causal and masked language modeling and how to quickly fine-tune a DistilGPT-2 model.\n* [Chinchilla's wild implications](https://www.lesswrong.com/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications) by nostalgebraist: Discuss the scaling laws and explain what they mean to LLMs in general.\n* [BLOOM](https://bigscience.notion.site/BLOOM-BigScience-176B-Model-ad073ca07cdf479398d5f95d88e218c4) by BigScience: Notion page that describes how the BLOOM model was built, with a lot of useful information about the engineering part and the problems that were encountered.\n* [OPT-175 Logbook](https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/chronicles/OPT175B_Logbook.pdf) by Meta: Research logs showing what went wrong and what went right. Useful if you're planning to pre-train a very large language model (in this case, 175B parameters).\n* [LLM 360](https://www.llm360.ai/): A framework for open-source LLMs with training and data preparation code, data, metrics, and models.\n\n---\n",
        "token": [
            "3",
            ".",
            "Pre-training",
            "models",
            "Pre-training",
            "is",
            "a",
            "very",
            "long",
            "and",
            "costly",
            "process",
            ",",
            "which",
            "is",
            "why",
            "this",
            "is",
            "not",
            "the",
            "focus",
            "of",
            "this",
            "course",
            ".",
            "It",
            "'s",
            "good",
            "to",
            "have",
            "some",
            "level",
            "of",
            "understanding",
            "of",
            "what",
            "happens",
            "during",
            "pre-training",
            ",",
            "but",
            "hands-on",
            "experience",
            "is",
            "not",
            "required",
            ".",
            "*",
            "*",
            "*",
            "Data",
            "pipeline",
            "*",
            "*",
            ":",
            "Pre-training",
            "requires",
            "huge",
            "datasets",
            "(",
            "e.g.",
            ",",
            "[",
            "Llama",
            "2",
            "]",
            "(",
            "https",
            ":",
            "//arxiv.org/abs/2307.09288",
            ")",
            "was",
            "trained",
            "on",
            "2",
            "trillion",
            "tokens",
            ")",
            "that",
            "need",
            "to",
            "be",
            "filtered",
            ",",
            "tokenized",
            ",",
            "and",
            "collated",
            "with",
            "a",
            "pre-defined",
            "vocabulary",
            ".",
            "*",
            "*",
            "*",
            "Causal",
            "language",
            "modeling",
            "*",
            "*",
            ":",
            "Learn",
            "the",
            "difference",
            "between",
            "causal",
            "and",
            "masked",
            "language",
            "modeling",
            ",",
            "as",
            "well",
            "as",
            "the",
            "loss",
            "function",
            "used",
            "in",
            "this",
            "case",
            ".",
            "For",
            "efficient",
            "pre-training",
            ",",
            "learn",
            "more",
            "about",
            "[",
            "Megatron-LM",
            "]",
            "(",
            "https",
            ":",
            "//github.com/NVIDIA/Megatron-LM",
            ")",
            "or",
            "[",
            "gpt-neox",
            "]",
            "(",
            "https",
            ":",
            "//github.com/EleutherAI/gpt-neox",
            ")",
            ".",
            "*",
            "*",
            "*",
            "Scaling",
            "laws",
            "*",
            "*",
            ":",
            "The",
            "[",
            "scaling",
            "laws",
            "]",
            "(",
            "https",
            ":",
            "//arxiv.org/pdf/2001.08361.pdf",
            ")",
            "describe",
            "the",
            "expected",
            "model",
            "performance",
            "based",
            "on",
            "the",
            "model",
            "size",
            ",",
            "dataset",
            "size",
            ",",
            "and",
            "the",
            "amount",
            "of",
            "compute",
            "used",
            "for",
            "training",
            ".",
            "*",
            "*",
            "*",
            "High-Performance",
            "Computing",
            "*",
            "*",
            ":",
            "Out",
            "of",
            "scope",
            "here",
            ",",
            "but",
            "more",
            "knowledge",
            "about",
            "HPC",
            "is",
            "fundamental",
            "if",
            "you",
            "'re",
            "planning",
            "to",
            "create",
            "your",
            "own",
            "LLM",
            "from",
            "scratch",
            "(",
            "hardware",
            ",",
            "distributed",
            "workload",
            ",",
            "etc.",
            ")",
            ".",
            "\ud83d\udcda",
            "*",
            "*",
            "References",
            "*",
            "*",
            ":",
            "*",
            "[",
            "LLMDataHub",
            "]",
            "(",
            "https",
            ":",
            "//github.com/Zjh-819/LLMDataHub",
            ")",
            "by",
            "Junhao",
            "Zhao",
            ":",
            "Curated",
            "list",
            "of",
            "datasets",
            "for",
            "pre-training",
            ",",
            "fine-tuning",
            ",",
            "and",
            "RLHF",
            ".",
            "*",
            "[",
            "Training",
            "a",
            "causal",
            "language",
            "model",
            "from",
            "scratch",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/learn/nlp-course/chapter7/6",
            "?",
            "fw=pt",
            ")",
            "by",
            "Hugging",
            "Face",
            ":",
            "Pre-train",
            "a",
            "GPT-2",
            "model",
            "from",
            "scratch",
            "using",
            "the",
            "transformers",
            "library",
            ".",
            "*",
            "[",
            "TinyLlama",
            "]",
            "(",
            "https",
            ":",
            "//github.com/jzhang38/TinyLlama",
            ")",
            "by",
            "Zhang",
            "et",
            "al",
            ".",
            ":",
            "Check",
            "this",
            "project",
            "to",
            "get",
            "a",
            "good",
            "understanding",
            "of",
            "how",
            "a",
            "Llama",
            "model",
            "is",
            "trained",
            "from",
            "scratch",
            ".",
            "*",
            "[",
            "Causal",
            "language",
            "modeling",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/docs/transformers/tasks/language_modeling",
            ")",
            "by",
            "Hugging",
            "Face",
            ":",
            "Explain",
            "the",
            "difference",
            "between",
            "causal",
            "and",
            "masked",
            "language",
            "modeling",
            "and",
            "how",
            "to",
            "quickly",
            "fine-tune",
            "a",
            "DistilGPT-2",
            "model",
            ".",
            "*",
            "[",
            "Chinchilla",
            "'s",
            "wild",
            "implications",
            "]",
            "(",
            "https",
            ":",
            "//www.lesswrong.com/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications",
            ")",
            "by",
            "nostalgebraist",
            ":",
            "Discuss",
            "the",
            "scaling",
            "laws",
            "and",
            "explain",
            "what",
            "they",
            "mean",
            "to",
            "LLMs",
            "in",
            "general",
            ".",
            "*",
            "[",
            "BLOOM",
            "]",
            "(",
            "https",
            ":",
            "//bigscience.notion.site/BLOOM-BigScience-176B-Model-ad073ca07cdf479398d5f95d88e218c4",
            ")",
            "by",
            "BigScience",
            ":",
            "Notion",
            "page",
            "that",
            "describes",
            "how",
            "the",
            "BLOOM",
            "model",
            "was",
            "built",
            ",",
            "with",
            "a",
            "lot",
            "of",
            "useful",
            "information",
            "about",
            "the",
            "engineering",
            "part",
            "and",
            "the",
            "problems",
            "that",
            "were",
            "encountered",
            ".",
            "*",
            "[",
            "OPT-175",
            "Logbook",
            "]",
            "(",
            "https",
            ":",
            "//github.com/facebookresearch/metaseq/blob/main/projects/OPT/chronicles/OPT175B_Logbook.pdf",
            ")",
            "by",
            "Meta",
            ":",
            "Research",
            "logs",
            "showing",
            "what",
            "went",
            "wrong",
            "and",
            "what",
            "went",
            "right",
            ".",
            "Useful",
            "if",
            "you",
            "'re",
            "planning",
            "to",
            "pre-train",
            "a",
            "very",
            "large",
            "language",
            "model",
            "(",
            "in",
            "this",
            "case",
            ",",
            "175B",
            "parameters",
            ")",
            ".",
            "*",
            "[",
            "LLM",
            "360",
            "]",
            "(",
            "https",
            ":",
            "//www.llm360.ai/",
            ")",
            ":",
            "A",
            "framework",
            "for",
            "open-source",
            "LLMs",
            "with",
            "training",
            "and",
            "data",
            "preparation",
            "code",
            ",",
            "data",
            ",",
            "metrics",
            ",",
            "and",
            "models",
            ".",
            "--",
            "-"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/mlabonne/llm-course",
        "readme_url": "https://raw.githubusercontent.com/mlabonne/llm-course/main/README.md",
        "topic": [
            "course",
            "large-language-models",
            "llm",
            "machine-learning",
            "roadmap"
        ],
        "text": "4. Supervised Fine-Tuning\n\nPre-trained models are only trained on a next-token prediction task, which is why they're not helpful assistants. SFT allows you to tweak them to respond to instructions. Moreover, it allows you to fine-tune your model on any data (private, not seen by GPT-4, etc.) and use it without having to pay for an API like OpenAI's.\n\n* **Full fine-tuning**: Full fine-tuning refers to training all the parameters in the model. It is not an efficient technique, but it produces slightly better results.\n* [**LoRA**](https://arxiv.org/abs/2106.09685): A parameter-efficient technique (PEFT) based on low-rank adapters. Instead of training all the parameters, we only train these adapters.\n* [**QLoRA**](https://arxiv.org/abs/2305.14314): Another PEFT based on LoRA, which also quantizes the weights of the model in 4 bits and introduce paged optimizers to manage memory spikes. Combine it with [Unsloth](https://github.com/unslothai/unsloth) to run it efficiently on a free Colab notebook.\n* **[Axolotl](https://github.com/OpenAccess-AI-Collective/axolotl)**: A user-friendly and powerful fine-tuning tool that is used in a lot of state-of-the-art open-source models.\n* [**DeepSpeed**](https://www.deepspeed.ai/): Efficient pre-training and fine-tuning of LLMs for multi-GPU and multi-node settings (implemented in Axolotl).\n\n\ud83d\udcda **References**:\n* [The Novice's LLM Training Guide](https://rentry.org/llm-training) by Alpin: Overview of the main concepts and parameters to consider when fine-tuning LLMs.\n* [LoRA insights](https://lightning.ai/pages/community/lora-insights/) by Sebastian Raschka: Practical insights about LoRA and how to select the best parameters.\n* [Fine-Tune Your Own Llama 2 Model](https://mlabonne.github.io/blog/posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html): Hands-on tutorial on how to fine-tune a Llama 2 model using Hugging Face libraries.\n* [Padding Large Language Models](https://towardsdatascience.com/padding-large-language-models-examples-with-llama-2-199fb10df8ff) by Benjamin Marie: Best practices to pad training examples for causal LLMs\n* [A Beginner's Guide to LLM Fine-Tuning](https://mlabonne.github.io/blog/posts/A_Beginners_Guide_to_LLM_Finetuning.html): Tutorial on how to fine-tune a CodeLlama model using Axolotl.\n\n---\n",
        "token": [
            "4",
            ".",
            "Supervised",
            "Fine-Tuning",
            "Pre-trained",
            "models",
            "are",
            "only",
            "trained",
            "on",
            "a",
            "next-token",
            "prediction",
            "task",
            ",",
            "which",
            "is",
            "why",
            "they",
            "'re",
            "not",
            "helpful",
            "assistants",
            ".",
            "SFT",
            "allows",
            "you",
            "to",
            "tweak",
            "them",
            "to",
            "respond",
            "to",
            "instructions",
            ".",
            "Moreover",
            ",",
            "it",
            "allows",
            "you",
            "to",
            "fine-tune",
            "your",
            "model",
            "on",
            "any",
            "data",
            "(",
            "private",
            ",",
            "not",
            "seen",
            "by",
            "GPT-4",
            ",",
            "etc",
            ".",
            ")",
            "and",
            "use",
            "it",
            "without",
            "having",
            "to",
            "pay",
            "for",
            "an",
            "API",
            "like",
            "OpenAI",
            "'s",
            ".",
            "*",
            "*",
            "*",
            "Full",
            "fine-tuning",
            "*",
            "*",
            ":",
            "Full",
            "fine-tuning",
            "refers",
            "to",
            "training",
            "all",
            "the",
            "parameters",
            "in",
            "the",
            "model",
            ".",
            "It",
            "is",
            "not",
            "an",
            "efficient",
            "technique",
            ",",
            "but",
            "it",
            "produces",
            "slightly",
            "better",
            "results",
            ".",
            "*",
            "[",
            "*",
            "*",
            "LoRA",
            "*",
            "*",
            "]",
            "(",
            "https",
            ":",
            "//arxiv.org/abs/2106.09685",
            ")",
            ":",
            "A",
            "parameter-efficient",
            "technique",
            "(",
            "PEFT",
            ")",
            "based",
            "on",
            "low-rank",
            "adapters",
            ".",
            "Instead",
            "of",
            "training",
            "all",
            "the",
            "parameters",
            ",",
            "we",
            "only",
            "train",
            "these",
            "adapters",
            ".",
            "*",
            "[",
            "*",
            "*",
            "QLoRA",
            "*",
            "*",
            "]",
            "(",
            "https",
            ":",
            "//arxiv.org/abs/2305.14314",
            ")",
            ":",
            "Another",
            "PEFT",
            "based",
            "on",
            "LoRA",
            ",",
            "which",
            "also",
            "quantizes",
            "the",
            "weights",
            "of",
            "the",
            "model",
            "in",
            "4",
            "bits",
            "and",
            "introduce",
            "paged",
            "optimizers",
            "to",
            "manage",
            "memory",
            "spikes",
            ".",
            "Combine",
            "it",
            "with",
            "[",
            "Unsloth",
            "]",
            "(",
            "https",
            ":",
            "//github.com/unslothai/unsloth",
            ")",
            "to",
            "run",
            "it",
            "efficiently",
            "on",
            "a",
            "free",
            "Colab",
            "notebook",
            ".",
            "*",
            "*",
            "*",
            "[",
            "Axolotl",
            "]",
            "(",
            "https",
            ":",
            "//github.com/OpenAccess-AI-Collective/axolotl",
            ")",
            "*",
            "*",
            ":",
            "A",
            "user-friendly",
            "and",
            "powerful",
            "fine-tuning",
            "tool",
            "that",
            "is",
            "used",
            "in",
            "a",
            "lot",
            "of",
            "state-of-the-art",
            "open-source",
            "models",
            ".",
            "*",
            "[",
            "*",
            "*",
            "DeepSpeed",
            "*",
            "*",
            "]",
            "(",
            "https",
            ":",
            "//www.deepspeed.ai/",
            ")",
            ":",
            "Efficient",
            "pre-training",
            "and",
            "fine-tuning",
            "of",
            "LLMs",
            "for",
            "multi-GPU",
            "and",
            "multi-node",
            "settings",
            "(",
            "implemented",
            "in",
            "Axolotl",
            ")",
            ".",
            "\ud83d\udcda",
            "*",
            "*",
            "References",
            "*",
            "*",
            ":",
            "*",
            "[",
            "The",
            "Novice",
            "'s",
            "LLM",
            "Training",
            "Guide",
            "]",
            "(",
            "https",
            ":",
            "//rentry.org/llm-training",
            ")",
            "by",
            "Alpin",
            ":",
            "Overview",
            "of",
            "the",
            "main",
            "concepts",
            "and",
            "parameters",
            "to",
            "consider",
            "when",
            "fine-tuning",
            "LLMs",
            ".",
            "*",
            "[",
            "LoRA",
            "insights",
            "]",
            "(",
            "https",
            ":",
            "//lightning.ai/pages/community/lora-insights/",
            ")",
            "by",
            "Sebastian",
            "Raschka",
            ":",
            "Practical",
            "insights",
            "about",
            "LoRA",
            "and",
            "how",
            "to",
            "select",
            "the",
            "best",
            "parameters",
            ".",
            "*",
            "[",
            "Fine-Tune",
            "Your",
            "Own",
            "Llama",
            "2",
            "Model",
            "]",
            "(",
            "https",
            ":",
            "//mlabonne.github.io/blog/posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html",
            ")",
            ":",
            "Hands-on",
            "tutorial",
            "on",
            "how",
            "to",
            "fine-tune",
            "a",
            "Llama",
            "2",
            "model",
            "using",
            "Hugging",
            "Face",
            "libraries",
            ".",
            "*",
            "[",
            "Padding",
            "Large",
            "Language",
            "Models",
            "]",
            "(",
            "https",
            ":",
            "//towardsdatascience.com/padding-large-language-models-examples-with-llama-2-199fb10df8ff",
            ")",
            "by",
            "Benjamin",
            "Marie",
            ":",
            "Best",
            "practices",
            "to",
            "pad",
            "training",
            "examples",
            "for",
            "causal",
            "LLMs",
            "*",
            "[",
            "A",
            "Beginner",
            "'s",
            "Guide",
            "to",
            "LLM",
            "Fine-Tuning",
            "]",
            "(",
            "https",
            ":",
            "//mlabonne.github.io/blog/posts/A_Beginners_Guide_to_LLM_Finetuning.html",
            ")",
            ":",
            "Tutorial",
            "on",
            "how",
            "to",
            "fine-tune",
            "a",
            "CodeLlama",
            "model",
            "using",
            "Axolotl",
            ".",
            "--",
            "-"
        ],
        "level of complexity": 2
    },
    {
        "url": "https://github.com/mlabonne/llm-course",
        "readme_url": "https://raw.githubusercontent.com/mlabonne/llm-course/main/README.md",
        "topic": [
            "course",
            "large-language-models",
            "llm",
            "machine-learning",
            "roadmap"
        ],
        "text": "6. Evaluation\n\nEvaluating LLMs is an undervalued part of the pipeline, which is time-consuming and moderately reliable. Your downstream task should dictate what you want to evaluate, but always remember Goodhart's law: \"When a measure becomes a target, it ceases to be a good measure.\"\n\n* **Traditional metrics**: Metrics like perplexity and BLEU score are not as popular as they were because they're flawed in most contexts. It is still important to understand them and when they can be applied.\n* **General benchmarks**: Based on the [Language Model Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness), the [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) is the main benchmark for general-purpose LLMs (like ChatGPT). There are other popular benchmarks like [BigBench](https://github.com/google/BIG-bench), [MT-Bench](https://arxiv.org/abs/2306.05685), etc.\n* **Task-specific benchmarks**: Tasks like summarization, translation, and question answering have dedicated benchmarks, metrics, and even subdomains (medical, financial, etc.), such as [PubMedQA](https://pubmedqa.github.io/) for biomedical question answering.\n* **Human evaluation**: The most reliable evaluation is the acceptance rate by users or comparisons made by humans. If you want to know if a model performs well, the simplest but surest way is to use it yourself.\n\n\ud83d\udcda **References**:\n* [Perplexity of fixed-length models](https://huggingface.co/docs/transformers/perplexity) by Hugging Face: Overview of perplexity with code to implement it with the transformers library.\n* [BLEU at your own risk](https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213) by Rachael Tatman: Overview of the BLEU score and its many issues with examples.\n* [A Survey on Evaluation of LLMs](https://arxiv.org/abs/2307.03109) by Chang et al.: Comprehensive paper about what to evaluate, where to evaluate, and how to evaluate.\n* [Chatbot Arena Leaderboard](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard) by lmsys: Elo rating of general-purpose LLMs, based on comparisons made by humans.\n\n---\n",
        "token": [
            "6",
            ".",
            "Evaluation",
            "Evaluating",
            "LLMs",
            "is",
            "an",
            "undervalued",
            "part",
            "of",
            "the",
            "pipeline",
            ",",
            "which",
            "is",
            "time-consuming",
            "and",
            "moderately",
            "reliable",
            ".",
            "Your",
            "downstream",
            "task",
            "should",
            "dictate",
            "what",
            "you",
            "want",
            "to",
            "evaluate",
            ",",
            "but",
            "always",
            "remember",
            "Goodhart",
            "'s",
            "law",
            ":",
            "``",
            "When",
            "a",
            "measure",
            "becomes",
            "a",
            "target",
            ",",
            "it",
            "ceases",
            "to",
            "be",
            "a",
            "good",
            "measure",
            ".",
            "''",
            "*",
            "*",
            "*",
            "Traditional",
            "metrics",
            "*",
            "*",
            ":",
            "Metrics",
            "like",
            "perplexity",
            "and",
            "BLEU",
            "score",
            "are",
            "not",
            "as",
            "popular",
            "as",
            "they",
            "were",
            "because",
            "they",
            "'re",
            "flawed",
            "in",
            "most",
            "contexts",
            ".",
            "It",
            "is",
            "still",
            "important",
            "to",
            "understand",
            "them",
            "and",
            "when",
            "they",
            "can",
            "be",
            "applied",
            ".",
            "*",
            "*",
            "*",
            "General",
            "benchmarks",
            "*",
            "*",
            ":",
            "Based",
            "on",
            "the",
            "[",
            "Language",
            "Model",
            "Evaluation",
            "Harness",
            "]",
            "(",
            "https",
            ":",
            "//github.com/EleutherAI/lm-evaluation-harness",
            ")",
            ",",
            "the",
            "[",
            "Open",
            "LLM",
            "Leaderboard",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard",
            ")",
            "is",
            "the",
            "main",
            "benchmark",
            "for",
            "general-purpose",
            "LLMs",
            "(",
            "like",
            "ChatGPT",
            ")",
            ".",
            "There",
            "are",
            "other",
            "popular",
            "benchmarks",
            "like",
            "[",
            "BigBench",
            "]",
            "(",
            "https",
            ":",
            "//github.com/google/BIG-bench",
            ")",
            ",",
            "[",
            "MT-Bench",
            "]",
            "(",
            "https",
            ":",
            "//arxiv.org/abs/2306.05685",
            ")",
            ",",
            "etc",
            ".",
            "*",
            "*",
            "*",
            "Task-specific",
            "benchmarks",
            "*",
            "*",
            ":",
            "Tasks",
            "like",
            "summarization",
            ",",
            "translation",
            ",",
            "and",
            "question",
            "answering",
            "have",
            "dedicated",
            "benchmarks",
            ",",
            "metrics",
            ",",
            "and",
            "even",
            "subdomains",
            "(",
            "medical",
            ",",
            "financial",
            ",",
            "etc",
            ".",
            ")",
            ",",
            "such",
            "as",
            "[",
            "PubMedQA",
            "]",
            "(",
            "https",
            ":",
            "//pubmedqa.github.io/",
            ")",
            "for",
            "biomedical",
            "question",
            "answering",
            ".",
            "*",
            "*",
            "*",
            "Human",
            "evaluation",
            "*",
            "*",
            ":",
            "The",
            "most",
            "reliable",
            "evaluation",
            "is",
            "the",
            "acceptance",
            "rate",
            "by",
            "users",
            "or",
            "comparisons",
            "made",
            "by",
            "humans",
            ".",
            "If",
            "you",
            "want",
            "to",
            "know",
            "if",
            "a",
            "model",
            "performs",
            "well",
            ",",
            "the",
            "simplest",
            "but",
            "surest",
            "way",
            "is",
            "to",
            "use",
            "it",
            "yourself",
            ".",
            "\ud83d\udcda",
            "*",
            "*",
            "References",
            "*",
            "*",
            ":",
            "*",
            "[",
            "Perplexity",
            "of",
            "fixed-length",
            "models",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/docs/transformers/perplexity",
            ")",
            "by",
            "Hugging",
            "Face",
            ":",
            "Overview",
            "of",
            "perplexity",
            "with",
            "code",
            "to",
            "implement",
            "it",
            "with",
            "the",
            "transformers",
            "library",
            ".",
            "*",
            "[",
            "BLEU",
            "at",
            "your",
            "own",
            "risk",
            "]",
            "(",
            "https",
            ":",
            "//towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213",
            ")",
            "by",
            "Rachael",
            "Tatman",
            ":",
            "Overview",
            "of",
            "the",
            "BLEU",
            "score",
            "and",
            "its",
            "many",
            "issues",
            "with",
            "examples",
            ".",
            "*",
            "[",
            "A",
            "Survey",
            "on",
            "Evaluation",
            "of",
            "LLMs",
            "]",
            "(",
            "https",
            ":",
            "//arxiv.org/abs/2307.03109",
            ")",
            "by",
            "Chang",
            "et",
            "al",
            ".",
            ":",
            "Comprehensive",
            "paper",
            "about",
            "what",
            "to",
            "evaluate",
            ",",
            "where",
            "to",
            "evaluate",
            ",",
            "and",
            "how",
            "to",
            "evaluate",
            ".",
            "*",
            "[",
            "Chatbot",
            "Arena",
            "Leaderboard",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/spaces/lmsys/chatbot-arena-leaderboard",
            ")",
            "by",
            "lmsys",
            ":",
            "Elo",
            "rating",
            "of",
            "general-purpose",
            "LLMs",
            ",",
            "based",
            "on",
            "comparisons",
            "made",
            "by",
            "humans",
            ".",
            "--",
            "-"
        ],
        "level of complexity": 2
    },
    {
        "url": "https://github.com/mlabonne/llm-course",
        "readme_url": "https://raw.githubusercontent.com/mlabonne/llm-course/main/README.md",
        "topic": [
            "course",
            "large-language-models",
            "llm",
            "machine-learning",
            "roadmap"
        ],
        "text": "7. Quantization\n\nQuantization is the process of converting the weights (and activations) of a model using a lower precision. For example, weights stored using 16 bits can be converted into a 4-bit representation. This technique has become increasingly important to reduce the computational and memory costs associated with LLMs.\n\n* **Base techniques**: Learn the different levels of precision (FP32, FP16, INT8, etc.) and how to perform na\u00efve quantization with absmax and zero-point techniques.\n* **GGUF and llama.cpp**: Originally designed to run on CPUs, [llama.cpp](https://github.com/ggerganov/llama.cpp) and the GGUF format have become the most popular tools to run LLMs on consumer-grade hardware.\n* **GPTQ and EXL2**: [GPTQ](https://arxiv.org/abs/2210.17323) and, more specifically, the [EXL2](https://github.com/turboderp/exllamav2) format offer an incredible speed but can only run on GPUs. Models also take a long time to be quantized.\n* **AWQ**: This new format is more accurate than GPTQ (lower perplexity) but uses a lot more VRAM and is not necessarily faster.\n\n\ud83d\udcda **References**:\n* [Introduction to quantization](https://mlabonne.github.io/blog/posts/Introduction_to_Weight_Quantization.html): Overview of quantization, absmax and zero-point quantization, and LLM.int8() with code.\n* [Quantize Llama models with llama.cpp](https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html): Tutorial on how to quantize a Llama 2 model using llama.cpp and the GGUF format.\n* [4-bit LLM Quantization with GPTQ](https://mlabonne.github.io/blog/posts/Introduction_to_Weight_Quantization.html): Tutorial on how to quantize an LLM using the GPTQ algorithm with AutoGPTQ.\n* [ExLlamaV2: The Fastest Library to Run LLMs](https://mlabonne.github.io/blog/posts/ExLlamaV2_The_Fastest_Library_to_Run%C2%A0LLMs.html): Guide on how to quantize a Mistral model using the EXL2 format and run it with the ExLlamaV2 library.\n* [Understanding Activation-Aware Weight Quantization](https://medium.com/friendliai/understanding-activation-aware-weight-quantization-awq-boosting-inference-serving-efficiency-in-10bb0faf63a8) by FriendliAI: Overview of the AWQ technique and its benefits.\n\n---\n",
        "token": [
            "7",
            ".",
            "Quantization",
            "Quantization",
            "is",
            "the",
            "process",
            "of",
            "converting",
            "the",
            "weights",
            "(",
            "and",
            "activations",
            ")",
            "of",
            "a",
            "model",
            "using",
            "a",
            "lower",
            "precision",
            ".",
            "For",
            "example",
            ",",
            "weights",
            "stored",
            "using",
            "16",
            "bits",
            "can",
            "be",
            "converted",
            "into",
            "a",
            "4-bit",
            "representation",
            ".",
            "This",
            "technique",
            "has",
            "become",
            "increasingly",
            "important",
            "to",
            "reduce",
            "the",
            "computational",
            "and",
            "memory",
            "costs",
            "associated",
            "with",
            "LLMs",
            ".",
            "*",
            "*",
            "*",
            "Base",
            "techniques",
            "*",
            "*",
            ":",
            "Learn",
            "the",
            "different",
            "levels",
            "of",
            "precision",
            "(",
            "FP32",
            ",",
            "FP16",
            ",",
            "INT8",
            ",",
            "etc",
            ".",
            ")",
            "and",
            "how",
            "to",
            "perform",
            "na\u00efve",
            "quantization",
            "with",
            "absmax",
            "and",
            "zero-point",
            "techniques",
            ".",
            "*",
            "*",
            "*",
            "GGUF",
            "and",
            "llama.cpp",
            "*",
            "*",
            ":",
            "Originally",
            "designed",
            "to",
            "run",
            "on",
            "CPUs",
            ",",
            "[",
            "llama.cpp",
            "]",
            "(",
            "https",
            ":",
            "//github.com/ggerganov/llama.cpp",
            ")",
            "and",
            "the",
            "GGUF",
            "format",
            "have",
            "become",
            "the",
            "most",
            "popular",
            "tools",
            "to",
            "run",
            "LLMs",
            "on",
            "consumer-grade",
            "hardware",
            ".",
            "*",
            "*",
            "*",
            "GPTQ",
            "and",
            "EXL2",
            "*",
            "*",
            ":",
            "[",
            "GPTQ",
            "]",
            "(",
            "https",
            ":",
            "//arxiv.org/abs/2210.17323",
            ")",
            "and",
            ",",
            "more",
            "specifically",
            ",",
            "the",
            "[",
            "EXL2",
            "]",
            "(",
            "https",
            ":",
            "//github.com/turboderp/exllamav2",
            ")",
            "format",
            "offer",
            "an",
            "incredible",
            "speed",
            "but",
            "can",
            "only",
            "run",
            "on",
            "GPUs",
            ".",
            "Models",
            "also",
            "take",
            "a",
            "long",
            "time",
            "to",
            "be",
            "quantized",
            ".",
            "*",
            "*",
            "*",
            "AWQ",
            "*",
            "*",
            ":",
            "This",
            "new",
            "format",
            "is",
            "more",
            "accurate",
            "than",
            "GPTQ",
            "(",
            "lower",
            "perplexity",
            ")",
            "but",
            "uses",
            "a",
            "lot",
            "more",
            "VRAM",
            "and",
            "is",
            "not",
            "necessarily",
            "faster",
            ".",
            "\ud83d\udcda",
            "*",
            "*",
            "References",
            "*",
            "*",
            ":",
            "*",
            "[",
            "Introduction",
            "to",
            "quantization",
            "]",
            "(",
            "https",
            ":",
            "//mlabonne.github.io/blog/posts/Introduction_to_Weight_Quantization.html",
            ")",
            ":",
            "Overview",
            "of",
            "quantization",
            ",",
            "absmax",
            "and",
            "zero-point",
            "quantization",
            ",",
            "and",
            "LLM.int8",
            "(",
            ")",
            "with",
            "code",
            ".",
            "*",
            "[",
            "Quantize",
            "Llama",
            "models",
            "with",
            "llama.cpp",
            "]",
            "(",
            "https",
            ":",
            "//mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html",
            ")",
            ":",
            "Tutorial",
            "on",
            "how",
            "to",
            "quantize",
            "a",
            "Llama",
            "2",
            "model",
            "using",
            "llama.cpp",
            "and",
            "the",
            "GGUF",
            "format",
            ".",
            "*",
            "[",
            "4-bit",
            "LLM",
            "Quantization",
            "with",
            "GPTQ",
            "]",
            "(",
            "https",
            ":",
            "//mlabonne.github.io/blog/posts/Introduction_to_Weight_Quantization.html",
            ")",
            ":",
            "Tutorial",
            "on",
            "how",
            "to",
            "quantize",
            "an",
            "LLM",
            "using",
            "the",
            "GPTQ",
            "algorithm",
            "with",
            "AutoGPTQ",
            ".",
            "*",
            "[",
            "ExLlamaV2",
            ":",
            "The",
            "Fastest",
            "Library",
            "to",
            "Run",
            "LLMs",
            "]",
            "(",
            "https",
            ":",
            "//mlabonne.github.io/blog/posts/ExLlamaV2_The_Fastest_Library_to_Run",
            "%",
            "C2",
            "%",
            "A0LLMs.html",
            ")",
            ":",
            "Guide",
            "on",
            "how",
            "to",
            "quantize",
            "a",
            "Mistral",
            "model",
            "using",
            "the",
            "EXL2",
            "format",
            "and",
            "run",
            "it",
            "with",
            "the",
            "ExLlamaV2",
            "library",
            ".",
            "*",
            "[",
            "Understanding",
            "Activation-Aware",
            "Weight",
            "Quantization",
            "]",
            "(",
            "https",
            ":",
            "//medium.com/friendliai/understanding-activation-aware-weight-quantization-awq-boosting-inference-serving-efficiency-in-10bb0faf63a8",
            ")",
            "by",
            "FriendliAI",
            ":",
            "Overview",
            "of",
            "the",
            "AWQ",
            "technique",
            "and",
            "its",
            "benefits",
            ".",
            "--",
            "-"
        ],
        "level of complexity": 2
    },
    {
        "url": "https://github.com/mlabonne/llm-course",
        "readme_url": "https://raw.githubusercontent.com/mlabonne/llm-course/main/README.md",
        "topic": [
            "course",
            "large-language-models",
            "llm",
            "machine-learning",
            "roadmap"
        ],
        "text": "\ud83d\udc77 The LLM Engineer\n\nThis section of the course focuses on learning how to build LLM-powered applications that can be used in production, with a focus on augmenting models and deploying them.\n\n![](img/roadmap_engineer.png)\n\n\n",
        "token": [
            "\ud83d\udc77",
            "The",
            "LLM",
            "Engineer",
            "This",
            "section",
            "of",
            "the",
            "course",
            "focuses",
            "on",
            "learning",
            "how",
            "to",
            "build",
            "LLM-powered",
            "applications",
            "that",
            "can",
            "be",
            "used",
            "in",
            "production",
            ",",
            "with",
            "a",
            "focus",
            "on",
            "augmenting",
            "models",
            "and",
            "deploying",
            "them",
            ".",
            "!",
            "[",
            "]",
            "(",
            "img/roadmap_engineer.png",
            ")"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/mlabonne/llm-course",
        "readme_url": "https://raw.githubusercontent.com/mlabonne/llm-course/main/README.md",
        "topic": [
            "course",
            "large-language-models",
            "llm",
            "machine-learning",
            "roadmap"
        ],
        "text": "1. Running LLMs\n\nRunning LLMs can be difficult due to high hardware requirements. Depending on your use case, you might want to simply consume a model through an API (like GPT-4) or run it locally. In any case, additional prompting and guidance techniques can improve and constrain the output for your applications.\n\n* **LLM APIs**: APIs are a convenient way to deploy LLMs. This space is divided between private LLMs ([OpenAI](https://platform.openai.com/), [Google](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/overview), [Anthropic](https://docs.anthropic.com/claude/reference/getting-started-with-the-api), [Cohere](https://docs.cohere.com/docs), etc.) and open-source LLMs ([OpenRouter](https://openrouter.ai/), [Hugging Face](https://huggingface.co/inference-api), [Together AI](https://www.together.ai/), etc.).\n* **Open-source LLMs**: The [Hugging Face Hub](https://huggingface.co/models) is a great place to find LLMs. You can directly run some of them in [Hugging Face Spaces](https://huggingface.co/spaces), or download and run them locally in apps like [LM Studio](https://lmstudio.ai/) or through the CLI with [llama.cpp](https://github.com/ggerganov/llama.cpp) or [Ollama](https://ollama.ai/).\n* **Prompt engineering**: Common techniques include zero-shot prompting, few-shot prompting, chain of thought, and ReAct. They work better with bigger models, but can be adapted to smaller ones.\n* **Structuring outputs**: Many tasks require a structured output, like a strict template or a JSON format. Libraries like [LMQL](https://lmql.ai/), [Outlines](https://github.com/outlines-dev/outlines), [Guidance](https://github.com/guidance-ai/guidance), etc. can be used to guide the generation and respect a given structure.\n\n\ud83d\udcda **References**:\n* [Run an LLM locally with LM Studio](https://www.kdnuggets.com/run-an-llm-locally-with-lm-studio) by Nisha Arya: Short guide on how to use LM Studio.\n* [Prompt engineering guide](https://www.promptingguide.ai/) by DAIR.AI: Exhaustive list of prompt techniques with examples\n* [Outlines - Quickstart](https://outlines-dev.github.io/outlines/quickstart/): List of guided generation techniques enabled by Outlines. \n* [LMQL - Overview](https://lmql.ai/docs/language/overview.html): Introduction to the LMQL language.\n\n---\n",
        "token": [
            "1",
            ".",
            "Running",
            "LLMs",
            "Running",
            "LLMs",
            "can",
            "be",
            "difficult",
            "due",
            "to",
            "high",
            "hardware",
            "requirements",
            ".",
            "Depending",
            "on",
            "your",
            "use",
            "case",
            ",",
            "you",
            "might",
            "want",
            "to",
            "simply",
            "consume",
            "a",
            "model",
            "through",
            "an",
            "API",
            "(",
            "like",
            "GPT-4",
            ")",
            "or",
            "run",
            "it",
            "locally",
            ".",
            "In",
            "any",
            "case",
            ",",
            "additional",
            "prompting",
            "and",
            "guidance",
            "techniques",
            "can",
            "improve",
            "and",
            "constrain",
            "the",
            "output",
            "for",
            "your",
            "applications",
            ".",
            "*",
            "*",
            "*",
            "LLM",
            "APIs",
            "*",
            "*",
            ":",
            "APIs",
            "are",
            "a",
            "convenient",
            "way",
            "to",
            "deploy",
            "LLMs",
            ".",
            "This",
            "space",
            "is",
            "divided",
            "between",
            "private",
            "LLMs",
            "(",
            "[",
            "OpenAI",
            "]",
            "(",
            "https",
            ":",
            "//platform.openai.com/",
            ")",
            ",",
            "[",
            "Google",
            "]",
            "(",
            "https",
            ":",
            "//cloud.google.com/vertex-ai/docs/generative-ai/learn/overview",
            ")",
            ",",
            "[",
            "Anthropic",
            "]",
            "(",
            "https",
            ":",
            "//docs.anthropic.com/claude/reference/getting-started-with-the-api",
            ")",
            ",",
            "[",
            "Cohere",
            "]",
            "(",
            "https",
            ":",
            "//docs.cohere.com/docs",
            ")",
            ",",
            "etc",
            ".",
            ")",
            "and",
            "open-source",
            "LLMs",
            "(",
            "[",
            "OpenRouter",
            "]",
            "(",
            "https",
            ":",
            "//openrouter.ai/",
            ")",
            ",",
            "[",
            "Hugging",
            "Face",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/inference-api",
            ")",
            ",",
            "[",
            "Together",
            "AI",
            "]",
            "(",
            "https",
            ":",
            "//www.together.ai/",
            ")",
            ",",
            "etc.",
            ")",
            ".",
            "*",
            "*",
            "*",
            "Open-source",
            "LLMs",
            "*",
            "*",
            ":",
            "The",
            "[",
            "Hugging",
            "Face",
            "Hub",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/models",
            ")",
            "is",
            "a",
            "great",
            "place",
            "to",
            "find",
            "LLMs",
            ".",
            "You",
            "can",
            "directly",
            "run",
            "some",
            "of",
            "them",
            "in",
            "[",
            "Hugging",
            "Face",
            "Spaces",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/spaces",
            ")",
            ",",
            "or",
            "download",
            "and",
            "run",
            "them",
            "locally",
            "in",
            "apps",
            "like",
            "[",
            "LM",
            "Studio",
            "]",
            "(",
            "https",
            ":",
            "//lmstudio.ai/",
            ")",
            "or",
            "through",
            "the",
            "CLI",
            "with",
            "[",
            "llama.cpp",
            "]",
            "(",
            "https",
            ":",
            "//github.com/ggerganov/llama.cpp",
            ")",
            "or",
            "[",
            "Ollama",
            "]",
            "(",
            "https",
            ":",
            "//ollama.ai/",
            ")",
            ".",
            "*",
            "*",
            "*",
            "Prompt",
            "engineering",
            "*",
            "*",
            ":",
            "Common",
            "techniques",
            "include",
            "zero-shot",
            "prompting",
            ",",
            "few-shot",
            "prompting",
            ",",
            "chain",
            "of",
            "thought",
            ",",
            "and",
            "ReAct",
            ".",
            "They",
            "work",
            "better",
            "with",
            "bigger",
            "models",
            ",",
            "but",
            "can",
            "be",
            "adapted",
            "to",
            "smaller",
            "ones",
            ".",
            "*",
            "*",
            "*",
            "Structuring",
            "outputs",
            "*",
            "*",
            ":",
            "Many",
            "tasks",
            "require",
            "a",
            "structured",
            "output",
            ",",
            "like",
            "a",
            "strict",
            "template",
            "or",
            "a",
            "JSON",
            "format",
            ".",
            "Libraries",
            "like",
            "[",
            "LMQL",
            "]",
            "(",
            "https",
            ":",
            "//lmql.ai/",
            ")",
            ",",
            "[",
            "Outlines",
            "]",
            "(",
            "https",
            ":",
            "//github.com/outlines-dev/outlines",
            ")",
            ",",
            "[",
            "Guidance",
            "]",
            "(",
            "https",
            ":",
            "//github.com/guidance-ai/guidance",
            ")",
            ",",
            "etc",
            ".",
            "can",
            "be",
            "used",
            "to",
            "guide",
            "the",
            "generation",
            "and",
            "respect",
            "a",
            "given",
            "structure",
            ".",
            "\ud83d\udcda",
            "*",
            "*",
            "References",
            "*",
            "*",
            ":",
            "*",
            "[",
            "Run",
            "an",
            "LLM",
            "locally",
            "with",
            "LM",
            "Studio",
            "]",
            "(",
            "https",
            ":",
            "//www.kdnuggets.com/run-an-llm-locally-with-lm-studio",
            ")",
            "by",
            "Nisha",
            "Arya",
            ":",
            "Short",
            "guide",
            "on",
            "how",
            "to",
            "use",
            "LM",
            "Studio",
            ".",
            "*",
            "[",
            "Prompt",
            "engineering",
            "guide",
            "]",
            "(",
            "https",
            ":",
            "//www.promptingguide.ai/",
            ")",
            "by",
            "DAIR.AI",
            ":",
            "Exhaustive",
            "list",
            "of",
            "prompt",
            "techniques",
            "with",
            "examples",
            "*",
            "[",
            "Outlines",
            "-",
            "Quickstart",
            "]",
            "(",
            "https",
            ":",
            "//outlines-dev.github.io/outlines/quickstart/",
            ")",
            ":",
            "List",
            "of",
            "guided",
            "generation",
            "techniques",
            "enabled",
            "by",
            "Outlines",
            ".",
            "*",
            "[",
            "LMQL",
            "-",
            "Overview",
            "]",
            "(",
            "https",
            ":",
            "//lmql.ai/docs/language/overview.html",
            ")",
            ":",
            "Introduction",
            "to",
            "the",
            "LMQL",
            "language",
            ".",
            "--",
            "-"
        ],
        "level of complexity": 2
    },
    {
        "url": "https://github.com/mlabonne/llm-course",
        "readme_url": "https://raw.githubusercontent.com/mlabonne/llm-course/main/README.md",
        "topic": [
            "course",
            "large-language-models",
            "llm",
            "machine-learning",
            "roadmap"
        ],
        "text": "4. Advanced RAG\n\nReal-life applications can require complex pipelines, including SQL or graph databases, as well as automatically selecting relevant tools and APIs. These advanced techniques can improve a baseline solution and provide additional features.\n\n* **Query construction**: Structured data stored in traditional databases requires a specific query language like SQL, Cypher, metadata, etc. We can directly translate the user instruction into a query to access the data with query construction.\n* **Agents and tools**: Agents augment LLMs by automatically selecting the most relevant tools to provide an answer. These tools can be as simple as using Google or Wikipedia, or more complex like a Python interpreter or Jira. \n* **Post-processing**: Final step that processes the inputs that are fed to the LLM. It enhances the relevance and diversity of documents retrieved with re-ranking, [RAG-fusion](https://github.com/Raudaschl/rag-fusion), and classification.\n\n\ud83d\udcda **References**:\n* [LangChain - Query Construction](https://blog.langchain.dev/query-construction/): Blog post about different types of query construction.\n* [LangChain - SQL](https://python.langchain.com/docs/use_cases/qa_structured/sql): Tutorial on how to interact with SQL databases with LLMs, involving Text-to-SQL and an optional SQL agent.\n* [Pinecone - LLM agents](https://www.pinecone.io/learn/series/langchain/langchain-agents/): Introduction to agents and tools with different types.\n* [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/) by Lilian Weng: More theoretical article about LLM agents.\n* [LangChain - OpenAI's RAG](https://blog.langchain.dev/applying-openai-rag/): Overview of the RAG strategies employed by OpenAI, including post-processing.\n\n---\n",
        "token": [
            "4",
            ".",
            "Advanced",
            "RAG",
            "Real-life",
            "applications",
            "can",
            "require",
            "complex",
            "pipelines",
            ",",
            "including",
            "SQL",
            "or",
            "graph",
            "databases",
            ",",
            "as",
            "well",
            "as",
            "automatically",
            "selecting",
            "relevant",
            "tools",
            "and",
            "APIs",
            ".",
            "These",
            "advanced",
            "techniques",
            "can",
            "improve",
            "a",
            "baseline",
            "solution",
            "and",
            "provide",
            "additional",
            "features",
            ".",
            "*",
            "*",
            "*",
            "Query",
            "construction",
            "*",
            "*",
            ":",
            "Structured",
            "data",
            "stored",
            "in",
            "traditional",
            "databases",
            "requires",
            "a",
            "specific",
            "query",
            "language",
            "like",
            "SQL",
            ",",
            "Cypher",
            ",",
            "metadata",
            ",",
            "etc",
            ".",
            "We",
            "can",
            "directly",
            "translate",
            "the",
            "user",
            "instruction",
            "into",
            "a",
            "query",
            "to",
            "access",
            "the",
            "data",
            "with",
            "query",
            "construction",
            ".",
            "*",
            "*",
            "*",
            "Agents",
            "and",
            "tools",
            "*",
            "*",
            ":",
            "Agents",
            "augment",
            "LLMs",
            "by",
            "automatically",
            "selecting",
            "the",
            "most",
            "relevant",
            "tools",
            "to",
            "provide",
            "an",
            "answer",
            ".",
            "These",
            "tools",
            "can",
            "be",
            "as",
            "simple",
            "as",
            "using",
            "Google",
            "or",
            "Wikipedia",
            ",",
            "or",
            "more",
            "complex",
            "like",
            "a",
            "Python",
            "interpreter",
            "or",
            "Jira",
            ".",
            "*",
            "*",
            "*",
            "Post-processing",
            "*",
            "*",
            ":",
            "Final",
            "step",
            "that",
            "processes",
            "the",
            "inputs",
            "that",
            "are",
            "fed",
            "to",
            "the",
            "LLM",
            ".",
            "It",
            "enhances",
            "the",
            "relevance",
            "and",
            "diversity",
            "of",
            "documents",
            "retrieved",
            "with",
            "re-ranking",
            ",",
            "[",
            "RAG-fusion",
            "]",
            "(",
            "https",
            ":",
            "//github.com/Raudaschl/rag-fusion",
            ")",
            ",",
            "and",
            "classification",
            ".",
            "\ud83d\udcda",
            "*",
            "*",
            "References",
            "*",
            "*",
            ":",
            "*",
            "[",
            "LangChain",
            "-",
            "Query",
            "Construction",
            "]",
            "(",
            "https",
            ":",
            "//blog.langchain.dev/query-construction/",
            ")",
            ":",
            "Blog",
            "post",
            "about",
            "different",
            "types",
            "of",
            "query",
            "construction",
            ".",
            "*",
            "[",
            "LangChain",
            "-",
            "SQL",
            "]",
            "(",
            "https",
            ":",
            "//python.langchain.com/docs/use_cases/qa_structured/sql",
            ")",
            ":",
            "Tutorial",
            "on",
            "how",
            "to",
            "interact",
            "with",
            "SQL",
            "databases",
            "with",
            "LLMs",
            ",",
            "involving",
            "Text-to-SQL",
            "and",
            "an",
            "optional",
            "SQL",
            "agent",
            ".",
            "*",
            "[",
            "Pinecone",
            "-",
            "LLM",
            "agents",
            "]",
            "(",
            "https",
            ":",
            "//www.pinecone.io/learn/series/langchain/langchain-agents/",
            ")",
            ":",
            "Introduction",
            "to",
            "agents",
            "and",
            "tools",
            "with",
            "different",
            "types",
            ".",
            "*",
            "[",
            "LLM",
            "Powered",
            "Autonomous",
            "Agents",
            "]",
            "(",
            "https",
            ":",
            "//lilianweng.github.io/posts/2023-06-23-agent/",
            ")",
            "by",
            "Lilian",
            "Weng",
            ":",
            "More",
            "theoretical",
            "article",
            "about",
            "LLM",
            "agents",
            ".",
            "*",
            "[",
            "LangChain",
            "-",
            "OpenAI",
            "'s",
            "RAG",
            "]",
            "(",
            "https",
            ":",
            "//blog.langchain.dev/applying-openai-rag/",
            ")",
            ":",
            "Overview",
            "of",
            "the",
            "RAG",
            "strategies",
            "employed",
            "by",
            "OpenAI",
            ",",
            "including",
            "post-processing",
            ".",
            "--",
            "-"
        ],
        "level of complexity": 2
    },
    {
        "url": "https://github.com/mlabonne/llm-course",
        "readme_url": "https://raw.githubusercontent.com/mlabonne/llm-course/main/README.md",
        "topic": [
            "course",
            "large-language-models",
            "llm",
            "machine-learning",
            "roadmap"
        ],
        "text": "5. Inference optimization\n\nText generation is a costly process that requires expensive hardware. In addition to quantization, various techniques have been proposed to maximize throughput and reduce inference costs.\n\n* **Flash Attention**: Optimization of the attention mechanism to transform its complexity from quadratic to linear, speeding up both training and inference.\n* **Key-value cache**: Understand the key-value cache and the improvements introduced in [Multi-Query Attention](https://arxiv.org/abs/1911.02150) (MQA) and [Grouped-Query Attention](https://arxiv.org/abs/2305.13245) (GQA).\n* **Speculative decoding**: Use a small model to produce drafts that are then reviewed by a larger model to speed up text generation.\n\n\ud83d\udcda **References**:\n* [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one) by Hugging Face: Explain how to optimize inference on GPUs.\n* [LLM Inference](https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices) by Databricks: Best practices for how to optimize LLM inference in production.\n* [Optimizing LLMs for Speed and Memory](https://huggingface.co/docs/transformers/main/en/llm_tutorial_optimization) by Hugging Face: Explain three main techniques to optimize speed and memory, namely quantization, Flash Attention, and architectural innovations.\n* [Assisted Generation](https://huggingface.co/blog/assisted-generation) by Hugging Face: HF's version of speculative decoding, it's an interesting blog post about how it works with code to implement it.\n\n---\n",
        "token": [
            "5",
            ".",
            "Inference",
            "optimization",
            "Text",
            "generation",
            "is",
            "a",
            "costly",
            "process",
            "that",
            "requires",
            "expensive",
            "hardware",
            ".",
            "In",
            "addition",
            "to",
            "quantization",
            ",",
            "various",
            "techniques",
            "have",
            "been",
            "proposed",
            "to",
            "maximize",
            "throughput",
            "and",
            "reduce",
            "inference",
            "costs",
            ".",
            "*",
            "*",
            "*",
            "Flash",
            "Attention",
            "*",
            "*",
            ":",
            "Optimization",
            "of",
            "the",
            "attention",
            "mechanism",
            "to",
            "transform",
            "its",
            "complexity",
            "from",
            "quadratic",
            "to",
            "linear",
            ",",
            "speeding",
            "up",
            "both",
            "training",
            "and",
            "inference",
            ".",
            "*",
            "*",
            "*",
            "Key-value",
            "cache",
            "*",
            "*",
            ":",
            "Understand",
            "the",
            "key-value",
            "cache",
            "and",
            "the",
            "improvements",
            "introduced",
            "in",
            "[",
            "Multi-Query",
            "Attention",
            "]",
            "(",
            "https",
            ":",
            "//arxiv.org/abs/1911.02150",
            ")",
            "(",
            "MQA",
            ")",
            "and",
            "[",
            "Grouped-Query",
            "Attention",
            "]",
            "(",
            "https",
            ":",
            "//arxiv.org/abs/2305.13245",
            ")",
            "(",
            "GQA",
            ")",
            ".",
            "*",
            "*",
            "*",
            "Speculative",
            "decoding",
            "*",
            "*",
            ":",
            "Use",
            "a",
            "small",
            "model",
            "to",
            "produce",
            "drafts",
            "that",
            "are",
            "then",
            "reviewed",
            "by",
            "a",
            "larger",
            "model",
            "to",
            "speed",
            "up",
            "text",
            "generation",
            ".",
            "\ud83d\udcda",
            "*",
            "*",
            "References",
            "*",
            "*",
            ":",
            "*",
            "[",
            "GPU",
            "Inference",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/docs/transformers/main/en/perf_infer_gpu_one",
            ")",
            "by",
            "Hugging",
            "Face",
            ":",
            "Explain",
            "how",
            "to",
            "optimize",
            "inference",
            "on",
            "GPUs",
            ".",
            "*",
            "[",
            "LLM",
            "Inference",
            "]",
            "(",
            "https",
            ":",
            "//www.databricks.com/blog/llm-inference-performance-engineering-best-practices",
            ")",
            "by",
            "Databricks",
            ":",
            "Best",
            "practices",
            "for",
            "how",
            "to",
            "optimize",
            "LLM",
            "inference",
            "in",
            "production",
            ".",
            "*",
            "[",
            "Optimizing",
            "LLMs",
            "for",
            "Speed",
            "and",
            "Memory",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/docs/transformers/main/en/llm_tutorial_optimization",
            ")",
            "by",
            "Hugging",
            "Face",
            ":",
            "Explain",
            "three",
            "main",
            "techniques",
            "to",
            "optimize",
            "speed",
            "and",
            "memory",
            ",",
            "namely",
            "quantization",
            ",",
            "Flash",
            "Attention",
            ",",
            "and",
            "architectural",
            "innovations",
            ".",
            "*",
            "[",
            "Assisted",
            "Generation",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/blog/assisted-generation",
            ")",
            "by",
            "Hugging",
            "Face",
            ":",
            "HF",
            "'s",
            "version",
            "of",
            "speculative",
            "decoding",
            ",",
            "it",
            "'s",
            "an",
            "interesting",
            "blog",
            "post",
            "about",
            "how",
            "it",
            "works",
            "with",
            "code",
            "to",
            "implement",
            "it",
            ".",
            "--",
            "-"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/mlabonne/llm-course",
        "readme_url": "https://raw.githubusercontent.com/mlabonne/llm-course/main/README.md",
        "topic": [
            "course",
            "large-language-models",
            "llm",
            "machine-learning",
            "roadmap"
        ],
        "text": "7. Securing LLMs\n\nIn addition to traditional security problems associated with software, LLMs have unique weaknesses due to the way they are trained and prompted.\n\n* **Prompt hacking**: Different techniques related to prompt engineering, including prompt injection (additional instruction to hijack the model's answer), data/prompt leaking (retrieve its original data/prompt), and jailbreaking (craft prompts to bypass safety features).\n* **Backdoors**: Attack vectors can target the training data itself, by poisoning the training data (e.g., with false information) or creating backdoors (secret triggers to change the model's behavior during inference).\n* **Defensive measures**: The best way to protect your LLM applications is to test them against these vulnerabilities (e.g., using red teaming and checks like [garak](https://github.com/leondz/garak/)) and observe them in production (with a framework like [langfuse](https://github.com/langfuse/langfuse)).\n\n\ud83d\udcda **References**:\n* [OWASP LLM Top 10](https://owasp.org/www-project-top-10-for-large-language-model-applications/) by HEGO Wiki: List of the 10 most critic vulnerabilities seen in LLM applications.\n* [Prompt Injection Primer](https://github.com/jthack/PIPE) by Joseph Thacker: Short guide dedicated to prompt injection for engineers.\n* [LLM Security](https://llmsecurity.net/) by [@llm_sec](https://twitter.com/llm_sec): Extensive list of resources related to LLM security.\n* [Red teaming LLMs](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/red-teaming) by Microsoft: Guide on how to perform red teaming with LLMs.\n---\n",
        "token": [
            "7",
            ".",
            "Securing",
            "LLMs",
            "In",
            "addition",
            "to",
            "traditional",
            "security",
            "problems",
            "associated",
            "with",
            "software",
            ",",
            "LLMs",
            "have",
            "unique",
            "weaknesses",
            "due",
            "to",
            "the",
            "way",
            "they",
            "are",
            "trained",
            "and",
            "prompted",
            ".",
            "*",
            "*",
            "*",
            "Prompt",
            "hacking",
            "*",
            "*",
            ":",
            "Different",
            "techniques",
            "related",
            "to",
            "prompt",
            "engineering",
            ",",
            "including",
            "prompt",
            "injection",
            "(",
            "additional",
            "instruction",
            "to",
            "hijack",
            "the",
            "model",
            "'s",
            "answer",
            ")",
            ",",
            "data/prompt",
            "leaking",
            "(",
            "retrieve",
            "its",
            "original",
            "data/prompt",
            ")",
            ",",
            "and",
            "jailbreaking",
            "(",
            "craft",
            "prompts",
            "to",
            "bypass",
            "safety",
            "features",
            ")",
            ".",
            "*",
            "*",
            "*",
            "Backdoors",
            "*",
            "*",
            ":",
            "Attack",
            "vectors",
            "can",
            "target",
            "the",
            "training",
            "data",
            "itself",
            ",",
            "by",
            "poisoning",
            "the",
            "training",
            "data",
            "(",
            "e.g.",
            ",",
            "with",
            "false",
            "information",
            ")",
            "or",
            "creating",
            "backdoors",
            "(",
            "secret",
            "triggers",
            "to",
            "change",
            "the",
            "model",
            "'s",
            "behavior",
            "during",
            "inference",
            ")",
            ".",
            "*",
            "*",
            "*",
            "Defensive",
            "measures",
            "*",
            "*",
            ":",
            "The",
            "best",
            "way",
            "to",
            "protect",
            "your",
            "LLM",
            "applications",
            "is",
            "to",
            "test",
            "them",
            "against",
            "these",
            "vulnerabilities",
            "(",
            "e.g.",
            ",",
            "using",
            "red",
            "teaming",
            "and",
            "checks",
            "like",
            "[",
            "garak",
            "]",
            "(",
            "https",
            ":",
            "//github.com/leondz/garak/",
            ")",
            ")",
            "and",
            "observe",
            "them",
            "in",
            "production",
            "(",
            "with",
            "a",
            "framework",
            "like",
            "[",
            "langfuse",
            "]",
            "(",
            "https",
            ":",
            "//github.com/langfuse/langfuse",
            ")",
            ")",
            ".",
            "\ud83d\udcda",
            "*",
            "*",
            "References",
            "*",
            "*",
            ":",
            "*",
            "[",
            "OWASP",
            "LLM",
            "Top",
            "10",
            "]",
            "(",
            "https",
            ":",
            "//owasp.org/www-project-top-10-for-large-language-model-applications/",
            ")",
            "by",
            "HEGO",
            "Wiki",
            ":",
            "List",
            "of",
            "the",
            "10",
            "most",
            "critic",
            "vulnerabilities",
            "seen",
            "in",
            "LLM",
            "applications",
            ".",
            "*",
            "[",
            "Prompt",
            "Injection",
            "Primer",
            "]",
            "(",
            "https",
            ":",
            "//github.com/jthack/PIPE",
            ")",
            "by",
            "Joseph",
            "Thacker",
            ":",
            "Short",
            "guide",
            "dedicated",
            "to",
            "prompt",
            "injection",
            "for",
            "engineers",
            ".",
            "*",
            "[",
            "LLM",
            "Security",
            "]",
            "(",
            "https",
            ":",
            "//llmsecurity.net/",
            ")",
            "by",
            "[",
            "@",
            "llm_sec",
            "]",
            "(",
            "https",
            ":",
            "//twitter.com/llm_sec",
            ")",
            ":",
            "Extensive",
            "list",
            "of",
            "resources",
            "related",
            "to",
            "LLM",
            "security",
            ".",
            "*",
            "[",
            "Red",
            "teaming",
            "LLMs",
            "]",
            "(",
            "https",
            ":",
            "//learn.microsoft.com/en-us/azure/ai-services/openai/concepts/red-teaming",
            ")",
            "by",
            "Microsoft",
            ":",
            "Guide",
            "on",
            "how",
            "to",
            "perform",
            "red",
            "teaming",
            "with",
            "LLMs",
            ".",
            "--",
            "-"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/FlowiseAI/Flowise",
        "readme_url": "https://raw.githubusercontent.com/FlowiseAI/Flowise/main/README.md",
        "topic": [
            "artificial-intelligence",
            "chatgpt",
            "javascript",
            "large-language-models",
            "llm",
            "low-code",
            "no-code",
            "react",
            "typescript"
        ],
        "text": "\u26a1Quick Start\n\nDownload and Install [NodeJS](https://nodejs.org/en/download) >= 18.15.0\n\n1. Install Flowise\n    ```bash\n    npm install -g flowise\n    ```\n2. Start Flowise\n\n    ```bash\n    npx flowise start\n    ```\n\n    With username & password\n\n    ```bash\n    npx flowise start --FLOWISE_USERNAME=user --FLOWISE_PASSWORD=1234\n    ```\n\n3. Open [http://localhost:3000](http://localhost:3000)\n\n",
        "token": [
            "\u26a1Quick",
            "Start",
            "Download",
            "and",
            "Install",
            "[",
            "NodeJS",
            "]",
            "(",
            "https",
            ":",
            "//nodejs.org/en/download",
            ")",
            ">",
            "=",
            "18.15.0",
            "1",
            ".",
            "Install",
            "Flowise",
            "``",
            "`",
            "bash",
            "npm",
            "install",
            "-g",
            "flowise",
            "``",
            "`",
            "2",
            ".",
            "Start",
            "Flowise",
            "``",
            "`",
            "bash",
            "npx",
            "flowise",
            "start",
            "``",
            "`",
            "With",
            "username",
            "&",
            "password",
            "``",
            "`",
            "bash",
            "npx",
            "flowise",
            "start",
            "--",
            "FLOWISE_USERNAME=user",
            "--",
            "FLOWISE_PASSWORD=1234",
            "``",
            "`",
            "3",
            ".",
            "Open",
            "[",
            "http",
            ":",
            "//localhost:3000",
            "]",
            "(",
            "http",
            ":",
            "//localhost:3000",
            ")"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/FlowiseAI/Flowise",
        "readme_url": "https://raw.githubusercontent.com/FlowiseAI/Flowise/main/README.md",
        "topic": [
            "artificial-intelligence",
            "chatgpt",
            "javascript",
            "large-language-models",
            "llm",
            "low-code",
            "no-code",
            "react",
            "typescript"
        ],
        "text": "Prerequisite\n\n-   Install [Yarn v1](https://classic.yarnpkg.com/en/docs/install)\n    ```bash\n    npm i -g yarn\n    ```\n\n",
        "token": [
            "Prerequisite",
            "-",
            "Install",
            "[",
            "Yarn",
            "v1",
            "]",
            "(",
            "https",
            ":",
            "//classic.yarnpkg.com/en/docs/install",
            ")",
            "``",
            "`",
            "bash",
            "npm",
            "i",
            "-g",
            "yarn",
            "``",
            "`"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/FlowiseAI/Flowise",
        "readme_url": "https://raw.githubusercontent.com/FlowiseAI/Flowise/main/README.md",
        "topic": [
            "artificial-intelligence",
            "chatgpt",
            "javascript",
            "large-language-models",
            "llm",
            "low-code",
            "no-code",
            "react",
            "typescript"
        ],
        "text": "Setup\n\n1. Clone the repository\n\n    ```bash\n    git clone https://github.com/FlowiseAI/Flowise.git\n    ```\n\n2. Go into repository folder\n\n    ```bash\n    cd Flowise\n    ```\n\n3. Install all dependencies of all modules:\n\n    ```bash\n    yarn install\n    ```\n\n4. Build all the code:\n\n    ```bash\n    yarn build\n    ```\n\n5. Start the app:\n\n    ```bash\n    yarn start\n    ```\n\n    You can now access the app on [http://localhost:3000](http://localhost:3000)\n\n6. For development build:\n\n    - Create `.env` file and specify the `PORT` (refer to `.env.example`) in `packages/ui`\n    - Create `.env` file and specify the `PORT` (refer to `.env.example`) in `packages/server`\n    - Run\n\n        ```bash\n        yarn dev\n        ```\n\n    Any code changes will reload the app automatically on [http://localhost:8080](http://localhost:8080)\n\n",
        "token": [
            "Setup",
            "1",
            ".",
            "Clone",
            "the",
            "repository",
            "``",
            "`",
            "bash",
            "git",
            "clone",
            "https",
            ":",
            "//github.com/FlowiseAI/Flowise.git",
            "``",
            "`",
            "2",
            ".",
            "Go",
            "into",
            "repository",
            "folder",
            "``",
            "`",
            "bash",
            "cd",
            "Flowise",
            "``",
            "`",
            "3",
            ".",
            "Install",
            "all",
            "dependencies",
            "of",
            "all",
            "modules",
            ":",
            "``",
            "`",
            "bash",
            "yarn",
            "install",
            "``",
            "`",
            "4",
            ".",
            "Build",
            "all",
            "the",
            "code",
            ":",
            "``",
            "`",
            "bash",
            "yarn",
            "build",
            "``",
            "`",
            "5",
            ".",
            "Start",
            "the",
            "app",
            ":",
            "``",
            "`",
            "bash",
            "yarn",
            "start",
            "``",
            "`",
            "You",
            "can",
            "now",
            "access",
            "the",
            "app",
            "on",
            "[",
            "http",
            ":",
            "//localhost:3000",
            "]",
            "(",
            "http",
            ":",
            "//localhost:3000",
            ")",
            "6",
            ".",
            "For",
            "development",
            "build",
            ":",
            "-",
            "Create",
            "`",
            ".env",
            "`",
            "file",
            "and",
            "specify",
            "the",
            "`",
            "PORT",
            "`",
            "(",
            "refer",
            "to",
            "`",
            ".env.example",
            "`",
            ")",
            "in",
            "`",
            "packages/ui",
            "`",
            "-",
            "Create",
            "`",
            ".env",
            "`",
            "file",
            "and",
            "specify",
            "the",
            "`",
            "PORT",
            "`",
            "(",
            "refer",
            "to",
            "`",
            ".env.example",
            "`",
            ")",
            "in",
            "`",
            "packages/server",
            "`",
            "-",
            "Run",
            "``",
            "`",
            "bash",
            "yarn",
            "dev",
            "``",
            "`",
            "Any",
            "code",
            "changes",
            "will",
            "reload",
            "the",
            "app",
            "automatically",
            "on",
            "[",
            "http",
            ":",
            "//localhost:8080",
            "]",
            "(",
            "http",
            ":",
            "//localhost:8080",
            ")"
        ],
        "level of complexity": 2
    },
    {
        "url": "https://github.com/microsoft/semantic-kernel",
        "readme_url": "https://raw.githubusercontent.com/microsoft/semantic-kernel/main/README.md",
        "topic": [
            "ai",
            "artificial-intelligence",
            "llm",
            "openai",
            "sdk"
        ],
        "text": "Getting started with Semantic Kernel\n\nThe Semantic Kernel SDK is available in C#, Python, and Java. To get started, choose your preferred language below. See the [Feature Matrix](https://learn.microsoft.com/en-us/semantic-kernel/get-started/supported-languages) to see a breakdown of\nfeature parity between our currently supported languages.\n\n<table width=100%>\n  <tbody>\n    <tr>\n      <td>\n        <img align=\"left\" width=52px src=\"https://user-images.githubusercontent.com/371009/230673036-fad1e8e6-5d48-49b1-a9c1-6f9834e0d165.png\">\n        <div>\n          <a href=\"dotnet/README.md\">Using Semantic Kernel in C#</a> &nbsp<br/>\n        </div>\n      </td>\n      <td>\n        <img align=\"left\" width=52px src=\"https://raw.githubusercontent.com/devicons/devicon/master/icons/python/python-original.svg\">\n        <div>\n          <a href=\"python/README.md\">Using Semantic Kernel in Python</a>\n        </div>\n      </td>\n      <td>\n        <img align=\"left\" width=52px height=52px src=\"https://upload.wikimedia.org/wikipedia/en/3/30/Java_programming_language_logo.svg\" alt=\"Java logo\">\n        <div>\n          <a href=\"https://github.com/microsoft/semantic-kernel/blob/main/java/README.md\">Using Semantic Kernel in Java</a>\n        </div>\n      </td>\n    </tr>\n  </tbody>\n</table>\n\nThe quickest way to get started with the basics is to get an API key\nfrom either OpenAI or Azure OpenAI and to run one of the C#, Python, and Java console applications/scripts below.\n\n",
        "token": [
            "Getting",
            "started",
            "with",
            "Semantic",
            "Kernel",
            "The",
            "Semantic",
            "Kernel",
            "SDK",
            "is",
            "available",
            "in",
            "C",
            "#",
            ",",
            "Python",
            ",",
            "and",
            "Java",
            ".",
            "To",
            "get",
            "started",
            ",",
            "choose",
            "your",
            "preferred",
            "language",
            "below",
            ".",
            "See",
            "the",
            "[",
            "Feature",
            "Matrix",
            "]",
            "(",
            "https",
            ":",
            "//learn.microsoft.com/en-us/semantic-kernel/get-started/supported-languages",
            ")",
            "to",
            "see",
            "a",
            "breakdown",
            "of",
            "feature",
            "parity",
            "between",
            "our",
            "currently",
            "supported",
            "languages",
            ".",
            "<",
            "table",
            "width=100",
            "%",
            ">",
            "<",
            "tbody",
            ">",
            "<",
            "tr",
            ">",
            "<",
            "td",
            ">",
            "<",
            "img",
            "align=",
            "''",
            "left",
            "''",
            "width=52px",
            "src=",
            "''",
            "https",
            ":",
            "//user-images.githubusercontent.com/371009/230673036-fad1e8e6-5d48-49b1-a9c1-6f9834e0d165.png",
            "''",
            ">",
            "<",
            "div",
            ">",
            "<",
            "a",
            "href=",
            "''",
            "dotnet/README.md",
            "''",
            ">",
            "Using",
            "Semantic",
            "Kernel",
            "in",
            "C",
            "#",
            "<",
            "/a",
            ">",
            "&",
            "nbsp",
            "<",
            "br/",
            ">",
            "<",
            "/div",
            ">",
            "<",
            "/td",
            ">",
            "<",
            "td",
            ">",
            "<",
            "img",
            "align=",
            "''",
            "left",
            "''",
            "width=52px",
            "src=",
            "''",
            "https",
            ":",
            "//raw.githubusercontent.com/devicons/devicon/master/icons/python/python-original.svg",
            "''",
            ">",
            "<",
            "div",
            ">",
            "<",
            "a",
            "href=",
            "''",
            "python/README.md",
            "''",
            ">",
            "Using",
            "Semantic",
            "Kernel",
            "in",
            "Python",
            "<",
            "/a",
            ">",
            "<",
            "/div",
            ">",
            "<",
            "/td",
            ">",
            "<",
            "td",
            ">",
            "<",
            "img",
            "align=",
            "''",
            "left",
            "''",
            "width=52px",
            "height=52px",
            "src=",
            "''",
            "https",
            ":",
            "//upload.wikimedia.org/wikipedia/en/3/30/Java_programming_language_logo.svg",
            "''",
            "alt=",
            "''",
            "Java",
            "logo",
            "''",
            ">",
            "<",
            "div",
            ">",
            "<",
            "a",
            "href=",
            "''",
            "https",
            ":",
            "//github.com/microsoft/semantic-kernel/blob/main/java/README.md",
            "''",
            ">",
            "Using",
            "Semantic",
            "Kernel",
            "in",
            "Java",
            "<",
            "/a",
            ">",
            "<",
            "/div",
            ">",
            "<",
            "/td",
            ">",
            "<",
            "/tr",
            ">",
            "<",
            "/tbody",
            ">",
            "<",
            "/table",
            ">",
            "The",
            "quickest",
            "way",
            "to",
            "get",
            "started",
            "with",
            "the",
            "basics",
            "is",
            "to",
            "get",
            "an",
            "API",
            "key",
            "from",
            "either",
            "OpenAI",
            "or",
            "Azure",
            "OpenAI",
            "and",
            "to",
            "run",
            "one",
            "of",
            "the",
            "C",
            "#",
            ",",
            "Python",
            ",",
            "and",
            "Java",
            "console",
            "applications/scripts",
            "below",
            "."
        ],
        "level of complexity": 2
    },
    {
        "url": "https://github.com/microsoft/semantic-kernel",
        "readme_url": "https://raw.githubusercontent.com/microsoft/semantic-kernel/main/README.md",
        "topic": [
            "ai",
            "artificial-intelligence",
            "llm",
            "openai",
            "sdk"
        ],
        "text": "For Python:\n\n1. Install the pip package: `python -m pip install semantic-kernel`.\n2. Create a new script e.g. `hello-world.py`.\n3. Store your API key and settings in an `.env` file as described [here](python/README.md).\n4. Copy the code from [here](python/README.md) into the `hello-world.py` script.\n5. Run the python script.\n\n",
        "token": [
            "For",
            "Python",
            ":",
            "1",
            ".",
            "Install",
            "the",
            "pip",
            "package",
            ":",
            "`",
            "python",
            "-m",
            "pip",
            "install",
            "semantic-kernel",
            "`",
            ".",
            "2",
            ".",
            "Create",
            "a",
            "new",
            "script",
            "e.g",
            ".",
            "`",
            "hello-world.py",
            "`",
            ".",
            "3",
            ".",
            "Store",
            "your",
            "API",
            "key",
            "and",
            "settings",
            "in",
            "an",
            "`",
            ".env",
            "`",
            "file",
            "as",
            "described",
            "[",
            "here",
            "]",
            "(",
            "python/README.md",
            ")",
            ".",
            "4",
            ".",
            "Copy",
            "the",
            "code",
            "from",
            "[",
            "here",
            "]",
            "(",
            "python/README.md",
            ")",
            "into",
            "the",
            "`",
            "hello-world.py",
            "`",
            "script",
            ".",
            "5",
            ".",
            "Run",
            "the",
            "python",
            "script",
            "."
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/microsoft/semantic-kernel",
        "readme_url": "https://raw.githubusercontent.com/microsoft/semantic-kernel/main/README.md",
        "topic": [
            "ai",
            "artificial-intelligence",
            "llm",
            "openai",
            "sdk"
        ],
        "text": "Learning how to use Semantic Kernel\n\nThe fastest way to learn how to use Semantic Kernel is with our C",
        "token": [
            "Learning",
            "how",
            "to",
            "use",
            "Semantic",
            "Kernel",
            "The",
            "fastest",
            "way",
            "to",
            "learn",
            "how",
            "to",
            "use",
            "Semantic",
            "Kernel",
            "is",
            "with",
            "our",
            "C"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/microsoft/semantic-kernel",
        "readme_url": "https://raw.githubusercontent.com/microsoft/semantic-kernel/main/README.md",
        "topic": [
            "ai",
            "artificial-intelligence",
            "llm",
            "openai",
            "sdk"
        ],
        "text": "and Python Jupyter notebooks. These notebooks\ndemonstrate how to use Semantic Kernel with code snippets that you can run with a push of a button.\n\n- [Getting Started with C",
        "token": [
            "and",
            "Python",
            "Jupyter",
            "notebooks",
            ".",
            "These",
            "notebooks",
            "demonstrate",
            "how",
            "to",
            "use",
            "Semantic",
            "Kernel",
            "with",
            "code",
            "snippets",
            "that",
            "you",
            "can",
            "run",
            "with",
            "a",
            "push",
            "of",
            "a",
            "button",
            ".",
            "-",
            "[",
            "Getting",
            "Started",
            "with",
            "C"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/microsoft/semantic-kernel",
        "readme_url": "https://raw.githubusercontent.com/microsoft/semantic-kernel/main/README.md",
        "topic": [
            "ai",
            "artificial-intelligence",
            "llm",
            "openai",
            "sdk"
        ],
        "text": "notebook](dotnet/notebooks/00-getting-started.ipynb)\n- [Getting Started with Python notebook](python/notebooks/00-getting-started.ipynb)\n\nOnce you've finished the getting started notebooks, you can then check out the main walkthroughs\non our Learn site. Each sample comes with a completed C",
        "token": [
            "notebook",
            "]",
            "(",
            "dotnet/notebooks/00-getting-started.ipynb",
            ")",
            "-",
            "[",
            "Getting",
            "Started",
            "with",
            "Python",
            "notebook",
            "]",
            "(",
            "python/notebooks/00-getting-started.ipynb",
            ")",
            "Once",
            "you",
            "'ve",
            "finished",
            "the",
            "getting",
            "started",
            "notebooks",
            ",",
            "you",
            "can",
            "then",
            "check",
            "out",
            "the",
            "main",
            "walkthroughs",
            "on",
            "our",
            "Learn",
            "site",
            ".",
            "Each",
            "sample",
            "comes",
            "with",
            "a",
            "completed",
            "C"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/microsoft/semantic-kernel",
        "readme_url": "https://raw.githubusercontent.com/microsoft/semantic-kernel/main/README.md",
        "topic": [
            "ai",
            "artificial-intelligence",
            "llm",
            "openai",
            "sdk"
        ],
        "text": "Chat Copilot: see what's possible with Semantic Kernel\n\nIf you're interested in seeing a full end-to-end example of how to use Semantic Kernel, check out\nour [Chat Copilot](https://github.com/microsoft/chat-copilot) reference application. Chat Copilot\nis a chatbot that demonstrates the power of Semantic Kernel. By combining plugins, planners, and personas,\nwe demonstrate how you can build a chatbot that can maintain long-running conversations with users while\nalso leveraging plugins to integrate with other services.\n\n![Chat Copilot answering a question](https://learn.microsoft.com/en-us/semantic-kernel/media/chat-copilot-in-action.gif)\n\nYou can run the app yourself by downloading it from its [GitHub repo](https://github.com/microsoft/chat-copilot).\n\n",
        "token": [
            "Chat",
            "Copilot",
            ":",
            "see",
            "what",
            "'s",
            "possible",
            "with",
            "Semantic",
            "Kernel",
            "If",
            "you",
            "'re",
            "interested",
            "in",
            "seeing",
            "a",
            "full",
            "end-to-end",
            "example",
            "of",
            "how",
            "to",
            "use",
            "Semantic",
            "Kernel",
            ",",
            "check",
            "out",
            "our",
            "[",
            "Chat",
            "Copilot",
            "]",
            "(",
            "https",
            ":",
            "//github.com/microsoft/chat-copilot",
            ")",
            "reference",
            "application",
            ".",
            "Chat",
            "Copilot",
            "is",
            "a",
            "chatbot",
            "that",
            "demonstrates",
            "the",
            "power",
            "of",
            "Semantic",
            "Kernel",
            ".",
            "By",
            "combining",
            "plugins",
            ",",
            "planners",
            ",",
            "and",
            "personas",
            ",",
            "we",
            "demonstrate",
            "how",
            "you",
            "can",
            "build",
            "a",
            "chatbot",
            "that",
            "can",
            "maintain",
            "long-running",
            "conversations",
            "with",
            "users",
            "while",
            "also",
            "leveraging",
            "plugins",
            "to",
            "integrate",
            "with",
            "other",
            "services",
            ".",
            "!",
            "[",
            "Chat",
            "Copilot",
            "answering",
            "a",
            "question",
            "]",
            "(",
            "https",
            ":",
            "//learn.microsoft.com/en-us/semantic-kernel/media/chat-copilot-in-action.gif",
            ")",
            "You",
            "can",
            "run",
            "the",
            "app",
            "yourself",
            "by",
            "downloading",
            "it",
            "from",
            "its",
            "[",
            "GitHub",
            "repo",
            "]",
            "(",
            "https",
            ":",
            "//github.com/microsoft/chat-copilot",
            ")",
            "."
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/microsoft/semantic-kernel",
        "readme_url": "https://raw.githubusercontent.com/microsoft/semantic-kernel/main/README.md",
        "topic": [
            "ai",
            "artificial-intelligence",
            "llm",
            "openai",
            "sdk"
        ],
        "text": "Check out our other repos!\n\nIf you like Semantic Kernel, you may also be interested in other repos the Semantic Kernel team supports:\n\n| Repo                                                                              | Description                                                                                   |\n| --------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------- |\n| [Chat Copilot](https://github.com/microsoft/chat-copilot)                         | A reference application that demonstrates how to build a chatbot with Semantic Kernel.        |\n| [Semantic Kernel Docs](https://github.com/MicrosoftDocs/semantic-kernel-docs)     | The home for Semantic Kernel documentation that appears on the Microsoft learn site.          |\n| [Semantic Kernel Starters](https://github.com/microsoft/semantic-kernel-starters) | Starter projects for Semantic Kernel to make it easier to get started.                        |\n| [Kernel Memory](https://github.com/microsoft/kernel-memory)                       | A scalable Memory service to store information and ask questions using the RAG pattern.       |\n\n",
        "token": [
            "Check",
            "out",
            "our",
            "other",
            "repos",
            "!",
            "If",
            "you",
            "like",
            "Semantic",
            "Kernel",
            ",",
            "you",
            "may",
            "also",
            "be",
            "interested",
            "in",
            "other",
            "repos",
            "the",
            "Semantic",
            "Kernel",
            "team",
            "supports",
            ":",
            "|",
            "Repo",
            "|",
            "Description",
            "|",
            "|",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "-",
            "|",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "-",
            "|",
            "|",
            "[",
            "Chat",
            "Copilot",
            "]",
            "(",
            "https",
            ":",
            "//github.com/microsoft/chat-copilot",
            ")",
            "|",
            "A",
            "reference",
            "application",
            "that",
            "demonstrates",
            "how",
            "to",
            "build",
            "a",
            "chatbot",
            "with",
            "Semantic",
            "Kernel",
            ".",
            "|",
            "|",
            "[",
            "Semantic",
            "Kernel",
            "Docs",
            "]",
            "(",
            "https",
            ":",
            "//github.com/MicrosoftDocs/semantic-kernel-docs",
            ")",
            "|",
            "The",
            "home",
            "for",
            "Semantic",
            "Kernel",
            "documentation",
            "that",
            "appears",
            "on",
            "the",
            "Microsoft",
            "learn",
            "site",
            ".",
            "|",
            "|",
            "[",
            "Semantic",
            "Kernel",
            "Starters",
            "]",
            "(",
            "https",
            ":",
            "//github.com/microsoft/semantic-kernel-starters",
            ")",
            "|",
            "Starter",
            "projects",
            "for",
            "Semantic",
            "Kernel",
            "to",
            "make",
            "it",
            "easier",
            "to",
            "get",
            "started",
            ".",
            "|",
            "|",
            "[",
            "Kernel",
            "Memory",
            "]",
            "(",
            "https",
            ":",
            "//github.com/microsoft/kernel-memory",
            ")",
            "|",
            "A",
            "scalable",
            "Memory",
            "service",
            "to",
            "store",
            "information",
            "and",
            "ask",
            "questions",
            "using",
            "the",
            "RAG",
            "pattern",
            ".",
            "|"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/microsoft/semantic-kernel",
        "readme_url": "https://raw.githubusercontent.com/microsoft/semantic-kernel/main/README.md",
        "topic": [
            "ai",
            "artificial-intelligence",
            "llm",
            "openai",
            "sdk"
        ],
        "text": "Join the community\n\nWe welcome your contributions and suggestions to SK community! One of the easiest\nways to participate is to engage in discussions in the GitHub repository.\nBug reports and fixes are welcome!\n\nFor new features, components, or extensions, please open an issue and discuss with\nus before sending a PR. This is to avoid rejection as we might be taking the core\nin a different direction, but also to consider the impact on the larger ecosystem.\n\nTo learn more and get started:\n\n- Read the [documentation](https://aka.ms/sk/learn)\n- Learn how to [contribute](https://learn.microsoft.com/en-us/semantic-kernel/get-started/contributing) to the project\n- Join the [Discord community](https://aka.ms/SKDiscord)\n- Attend [regular office hours and SK community events](COMMUNITY.md)\n- Follow the team on our [blog](https://aka.ms/sk/blog)\n\n",
        "token": [
            "Join",
            "the",
            "community",
            "We",
            "welcome",
            "your",
            "contributions",
            "and",
            "suggestions",
            "to",
            "SK",
            "community",
            "!",
            "One",
            "of",
            "the",
            "easiest",
            "ways",
            "to",
            "participate",
            "is",
            "to",
            "engage",
            "in",
            "discussions",
            "in",
            "the",
            "GitHub",
            "repository",
            ".",
            "Bug",
            "reports",
            "and",
            "fixes",
            "are",
            "welcome",
            "!",
            "For",
            "new",
            "features",
            ",",
            "components",
            ",",
            "or",
            "extensions",
            ",",
            "please",
            "open",
            "an",
            "issue",
            "and",
            "discuss",
            "with",
            "us",
            "before",
            "sending",
            "a",
            "PR",
            ".",
            "This",
            "is",
            "to",
            "avoid",
            "rejection",
            "as",
            "we",
            "might",
            "be",
            "taking",
            "the",
            "core",
            "in",
            "a",
            "different",
            "direction",
            ",",
            "but",
            "also",
            "to",
            "consider",
            "the",
            "impact",
            "on",
            "the",
            "larger",
            "ecosystem",
            ".",
            "To",
            "learn",
            "more",
            "and",
            "get",
            "started",
            ":",
            "-",
            "Read",
            "the",
            "[",
            "documentation",
            "]",
            "(",
            "https",
            ":",
            "//aka.ms/sk/learn",
            ")",
            "-",
            "Learn",
            "how",
            "to",
            "[",
            "contribute",
            "]",
            "(",
            "https",
            ":",
            "//learn.microsoft.com/en-us/semantic-kernel/get-started/contributing",
            ")",
            "to",
            "the",
            "project",
            "-",
            "Join",
            "the",
            "[",
            "Discord",
            "community",
            "]",
            "(",
            "https",
            ":",
            "//aka.ms/SKDiscord",
            ")",
            "-",
            "Attend",
            "[",
            "regular",
            "office",
            "hours",
            "and",
            "SK",
            "community",
            "events",
            "]",
            "(",
            "COMMUNITY.md",
            ")",
            "-",
            "Follow",
            "the",
            "team",
            "on",
            "our",
            "[",
            "blog",
            "]",
            "(",
            "https",
            ":",
            "//aka.ms/sk/blog",
            ")"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/mlc-ai/mlc-llm",
        "readme_url": "https://raw.githubusercontent.com/mlc-ai/mlc-llm/main/README.md",
        "topic": [
            "language-model",
            "llm",
            "machine-learning-compilation",
            "tvm"
        ],
        "text": "News\n\n* [10/18/2023] [[Post]](https://blog.mlc.ai/2023/10/19/Scalable-Language-Model-Inference-on-Multiple-NVDIA-AMD-GPUs) Scalable multi-GPU support for CUDA and ROCm are official.\n* [09/02/2023] Prebuilt ROCm 5.7 and CUDA 12.2 package is [available](https://llm.mlc.ai/docs/install/tvm.html#option-1-prebuilt-package).\n* [08/25/2023] CodeLlama support is up.\n* [08/14/2023] [[Post]](https://blog.mlc.ai/2023/08/09/GPU-Accelerated-LLM-on-Orange-Pi) Mali GPU support is up on Orange Pi.\n* [08/09/2023] [[Post]](https://blog.mlc.ai/2023/08/09/Making-AMD-GPUs-competitive-for-LLM-inference) ROCm backend is mature to use.\n* [08/02/2023] [Dockerfile](https://github.com/mlc-ai/llm-perf-bench/) is released for CUDA performance benchmarking.\n* [07/19/2023] Support for Llama2-7B/13B/70B is up.\n* [05/22/2023] [[Post]](https://blog.mlc.ai/2023/05/22/bringing-open-large-language-models-to-consumer-devices) RedPajama support is up.\n* [05/08/2023] [[Post]](https://blog.mlc.ai/2023/05/08/bringing-hardware-accelerated-language-models-to-android-devices) MLC LLM is now available on Android.\n* [05/01/2023] [[Post]](https://blog.mlc.ai/2023/05/01/bringing-accelerated-llm-to-consumer-hardware) MLC LLM is released with Metal, Vulkan and CUDA backends.\n* [04/14/2023] [WebLLM](https://github.com/mlc-ai/web-llm) is released prior to MLC LLM with WebGPU and WebAssembly backend.\n\n",
        "token": [
            "News",
            "*",
            "[",
            "10/18/2023",
            "]",
            "[",
            "[",
            "Post",
            "]",
            "]",
            "(",
            "https",
            ":",
            "//blog.mlc.ai/2023/10/19/Scalable-Language-Model-Inference-on-Multiple-NVDIA-AMD-GPUs",
            ")",
            "Scalable",
            "multi-GPU",
            "support",
            "for",
            "CUDA",
            "and",
            "ROCm",
            "are",
            "official",
            ".",
            "*",
            "[",
            "09/02/2023",
            "]",
            "Prebuilt",
            "ROCm",
            "5.7",
            "and",
            "CUDA",
            "12.2",
            "package",
            "is",
            "[",
            "available",
            "]",
            "(",
            "https",
            ":",
            "//llm.mlc.ai/docs/install/tvm.html",
            "#",
            "option-1-prebuilt-package",
            ")",
            ".",
            "*",
            "[",
            "08/25/2023",
            "]",
            "CodeLlama",
            "support",
            "is",
            "up",
            ".",
            "*",
            "[",
            "08/14/2023",
            "]",
            "[",
            "[",
            "Post",
            "]",
            "]",
            "(",
            "https",
            ":",
            "//blog.mlc.ai/2023/08/09/GPU-Accelerated-LLM-on-Orange-Pi",
            ")",
            "Mali",
            "GPU",
            "support",
            "is",
            "up",
            "on",
            "Orange",
            "Pi",
            ".",
            "*",
            "[",
            "08/09/2023",
            "]",
            "[",
            "[",
            "Post",
            "]",
            "]",
            "(",
            "https",
            ":",
            "//blog.mlc.ai/2023/08/09/Making-AMD-GPUs-competitive-for-LLM-inference",
            ")",
            "ROCm",
            "backend",
            "is",
            "mature",
            "to",
            "use",
            ".",
            "*",
            "[",
            "08/02/2023",
            "]",
            "[",
            "Dockerfile",
            "]",
            "(",
            "https",
            ":",
            "//github.com/mlc-ai/llm-perf-bench/",
            ")",
            "is",
            "released",
            "for",
            "CUDA",
            "performance",
            "benchmarking",
            ".",
            "*",
            "[",
            "07/19/2023",
            "]",
            "Support",
            "for",
            "Llama2-7B/13B/70B",
            "is",
            "up",
            ".",
            "*",
            "[",
            "05/22/2023",
            "]",
            "[",
            "[",
            "Post",
            "]",
            "]",
            "(",
            "https",
            ":",
            "//blog.mlc.ai/2023/05/22/bringing-open-large-language-models-to-consumer-devices",
            ")",
            "RedPajama",
            "support",
            "is",
            "up",
            ".",
            "*",
            "[",
            "05/08/2023",
            "]",
            "[",
            "[",
            "Post",
            "]",
            "]",
            "(",
            "https",
            ":",
            "//blog.mlc.ai/2023/05/08/bringing-hardware-accelerated-language-models-to-android-devices",
            ")",
            "MLC",
            "LLM",
            "is",
            "now",
            "available",
            "on",
            "Android",
            ".",
            "*",
            "[",
            "05/01/2023",
            "]",
            "[",
            "[",
            "Post",
            "]",
            "]",
            "(",
            "https",
            ":",
            "//blog.mlc.ai/2023/05/01/bringing-accelerated-llm-to-consumer-hardware",
            ")",
            "MLC",
            "LLM",
            "is",
            "released",
            "with",
            "Metal",
            ",",
            "Vulkan",
            "and",
            "CUDA",
            "backends",
            ".",
            "*",
            "[",
            "04/14/2023",
            "]",
            "[",
            "WebLLM",
            "]",
            "(",
            "https",
            ":",
            "//github.com/mlc-ai/web-llm",
            ")",
            "is",
            "released",
            "prior",
            "to",
            "MLC",
            "LLM",
            "with",
            "WebGPU",
            "and",
            "WebAssembly",
            "backend",
            "."
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/mlc-ai/mlc-llm",
        "readme_url": "https://raw.githubusercontent.com/mlc-ai/mlc-llm/main/README.md",
        "topic": [
            "language-model",
            "llm",
            "machine-learning-compilation",
            "tvm"
        ],
        "text": "Getting Started\n\nPlease visit our [documentation](https://llm.mlc.ai/docs/index.html#getting-started) for detailed instructions.\n\n",
        "token": [
            "Getting",
            "Started",
            "Please",
            "visit",
            "our",
            "[",
            "documentation",
            "]",
            "(",
            "https",
            ":",
            "//llm.mlc.ai/docs/index.html",
            "#",
            "getting-started",
            ")",
            "for",
            "detailed",
            "instructions",
            "."
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/mlc-ai/mlc-llm",
        "readme_url": "https://raw.githubusercontent.com/mlc-ai/mlc-llm/main/README.md",
        "topic": [
            "language-model",
            "llm",
            "machine-learning-compilation",
            "tvm"
        ],
        "text": "Model Support\n\nMLC LLM supports a wide range of model architectures and variants. We have the following prebuilts which you can\nuse off-the-shelf. Visit [Prebuilt Models](https://llm.mlc.ai/docs/prebuilt_models.html) to see the full list, and [Compile Models via MLC](https://llm.mlc.ai/docs/compilation/compile_models.html) to see how to use models not on this list.\n\n<table style=\"width:100%\">\n  <thead>\n    <tr>\n      <th style=\"width:40%\">Architecture</th>\n      <th style=\"width:60%\">Prebuilt Model Variants</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>Llama</td>\n      <td>Llama-2, Code Llama, Vicuna, WizardLM, WizardMath, OpenOrca Platypus2, FlagAlpha Llama-2 Chinese, georgesung Llama-2 Uncensored</td>\n    </tr>\n    <tr>\n      <td>GPT-NeoX</td>\n      <td>RedPajama</td>\n    </tr>\n    <tr>\n      <td>GPT-J</td>\n      <td></td>\n    </tr>\n    <tr>\n      <td>RWKV</td>\n      <td>RWKV-raven</td>\n    </tr>\n    <tr>\n      <td>MiniGPT</td>\n      <td></td>\n    </tr>\n    <tr>\n      <td>GPTBigCode</td>\n      <td>WizardCoder</td>\n    </tr>\n    <tr>\n      <td>ChatGLM</td>\n      <td></td>\n    </tr>\n    <tr>\n      <td>StableLM</td>\n      <td></td>\n    </tr>\n    <tr>\n      <td>Mistral</td>\n      <td></td>\n    </tr>\n    <tr>\n      <td>Phi</td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n\n",
        "token": [
            "Model",
            "Support",
            "MLC",
            "LLM",
            "supports",
            "a",
            "wide",
            "range",
            "of",
            "model",
            "architectures",
            "and",
            "variants",
            ".",
            "We",
            "have",
            "the",
            "following",
            "prebuilts",
            "which",
            "you",
            "can",
            "use",
            "off-the-shelf",
            ".",
            "Visit",
            "[",
            "Prebuilt",
            "Models",
            "]",
            "(",
            "https",
            ":",
            "//llm.mlc.ai/docs/prebuilt_models.html",
            ")",
            "to",
            "see",
            "the",
            "full",
            "list",
            ",",
            "and",
            "[",
            "Compile",
            "Models",
            "via",
            "MLC",
            "]",
            "(",
            "https",
            ":",
            "//llm.mlc.ai/docs/compilation/compile_models.html",
            ")",
            "to",
            "see",
            "how",
            "to",
            "use",
            "models",
            "not",
            "on",
            "this",
            "list",
            ".",
            "<",
            "table",
            "style=",
            "''",
            "width:100",
            "%",
            "''",
            ">",
            "<",
            "thead",
            ">",
            "<",
            "tr",
            ">",
            "<",
            "th",
            "style=",
            "''",
            "width:40",
            "%",
            "''",
            ">",
            "Architecture",
            "<",
            "/th",
            ">",
            "<",
            "th",
            "style=",
            "''",
            "width:60",
            "%",
            "''",
            ">",
            "Prebuilt",
            "Model",
            "Variants",
            "<",
            "/th",
            ">",
            "<",
            "/tr",
            ">",
            "<",
            "/thead",
            ">",
            "<",
            "tbody",
            ">",
            "<",
            "tr",
            ">",
            "<",
            "td",
            ">",
            "Llama",
            "<",
            "/td",
            ">",
            "<",
            "td",
            ">",
            "Llama-2",
            ",",
            "Code",
            "Llama",
            ",",
            "Vicuna",
            ",",
            "WizardLM",
            ",",
            "WizardMath",
            ",",
            "OpenOrca",
            "Platypus2",
            ",",
            "FlagAlpha",
            "Llama-2",
            "Chinese",
            ",",
            "georgesung",
            "Llama-2",
            "Uncensored",
            "<",
            "/td",
            ">",
            "<",
            "/tr",
            ">",
            "<",
            "tr",
            ">",
            "<",
            "td",
            ">",
            "GPT-NeoX",
            "<",
            "/td",
            ">",
            "<",
            "td",
            ">",
            "RedPajama",
            "<",
            "/td",
            ">",
            "<",
            "/tr",
            ">",
            "<",
            "tr",
            ">",
            "<",
            "td",
            ">",
            "GPT-J",
            "<",
            "/td",
            ">",
            "<",
            "td",
            ">",
            "<",
            "/td",
            ">",
            "<",
            "/tr",
            ">",
            "<",
            "tr",
            ">",
            "<",
            "td",
            ">",
            "RWKV",
            "<",
            "/td",
            ">",
            "<",
            "td",
            ">",
            "RWKV-raven",
            "<",
            "/td",
            ">",
            "<",
            "/tr",
            ">",
            "<",
            "tr",
            ">",
            "<",
            "td",
            ">",
            "MiniGPT",
            "<",
            "/td",
            ">",
            "<",
            "td",
            ">",
            "<",
            "/td",
            ">",
            "<",
            "/tr",
            ">",
            "<",
            "tr",
            ">",
            "<",
            "td",
            ">",
            "GPTBigCode",
            "<",
            "/td",
            ">",
            "<",
            "td",
            ">",
            "WizardCoder",
            "<",
            "/td",
            ">",
            "<",
            "/tr",
            ">",
            "<",
            "tr",
            ">",
            "<",
            "td",
            ">",
            "ChatGLM",
            "<",
            "/td",
            ">",
            "<",
            "td",
            ">",
            "<",
            "/td",
            ">",
            "<",
            "/tr",
            ">",
            "<",
            "tr",
            ">",
            "<",
            "td",
            ">",
            "StableLM",
            "<",
            "/td",
            ">",
            "<",
            "td",
            ">",
            "<",
            "/td",
            ">",
            "<",
            "/tr",
            ">",
            "<",
            "tr",
            ">",
            "<",
            "td",
            ">",
            "Mistral",
            "<",
            "/td",
            ">",
            "<",
            "td",
            ">",
            "<",
            "/td",
            ">",
            "<",
            "/tr",
            ">",
            "<",
            "tr",
            ">",
            "<",
            "td",
            ">",
            "Phi",
            "<",
            "/td",
            ">",
            "<",
            "td",
            ">",
            "<",
            "/td",
            ">",
            "<",
            "/tr",
            ">",
            "<",
            "/tbody",
            ">",
            "<",
            "/table",
            ">"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/THUDM/ChatGLM2-6B",
        "readme_url": "https://raw.githubusercontent.com/THUDM/ChatGLM2-6B/main/README.md",
        "topic": [
            "chatglm",
            "chatglm-6b",
            "large-language-models",
            "llm"
        ],
        "text": "\u73af\u5883\u5b89\u88c5\n\u9996\u5148\u9700\u8981\u4e0b\u8f7d\u672c\u4ed3\u5e93\uff1a\n```shell\ngit clone https://github.com/THUDM/ChatGLM2-6B\ncd ChatGLM2-6B\n```\n\n\u7136\u540e\u4f7f\u7528 pip \u5b89\u88c5\u4f9d\u8d56\uff1a\n```\npip install -r requirements.txt\n```\n\u5176\u4e2d `transformers` \u5e93\u7248\u672c\u63a8\u8350\u4e3a `4.30.2`\uff0c`torch` \u63a8\u8350\u4f7f\u7528 2.0 \u53ca\u4ee5\u4e0a\u7684\u7248\u672c\uff0c\u4ee5\u83b7\u5f97\u6700\u4f73\u7684\u63a8\u7406\u6027\u80fd\u3002\n\n",
        "token": [
            "\u73af\u5883\u5b89\u88c5",
            "\u9996\u5148\u9700\u8981\u4e0b\u8f7d\u672c\u4ed3\u5e93\uff1a",
            "``",
            "`",
            "shell",
            "git",
            "clone",
            "https",
            ":",
            "//github.com/THUDM/ChatGLM2-6B",
            "cd",
            "ChatGLM2-6B",
            "``",
            "`",
            "\u7136\u540e\u4f7f\u7528",
            "pip",
            "\u5b89\u88c5\u4f9d\u8d56\uff1a",
            "``",
            "`",
            "pip",
            "install",
            "-r",
            "requirements.txt",
            "``",
            "`",
            "\u5176\u4e2d",
            "`",
            "transformers",
            "`",
            "\u5e93\u7248\u672c\u63a8\u8350\u4e3a",
            "`",
            "4.30.2",
            "`",
            "\uff0c",
            "`",
            "torch",
            "`",
            "\u63a8\u8350\u4f7f\u7528",
            "2.0",
            "\u53ca\u4ee5\u4e0a\u7684\u7248\u672c\uff0c\u4ee5\u83b7\u5f97\u6700\u4f73\u7684\u63a8\u7406\u6027\u80fd\u3002"
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/THUDM/ChatGLM2-6B",
        "readme_url": "https://raw.githubusercontent.com/THUDM/ChatGLM2-6B/main/README.md",
        "topic": [
            "chatglm",
            "chatglm-6b",
            "large-language-models",
            "llm"
        ],
        "text": "\u4ece\u672c\u5730\u52a0\u8f7d\u6a21\u578b\n\u4ee5\u4e0a\u4ee3\u7801\u4f1a\u7531 `transformers` \u81ea\u52a8\u4e0b\u8f7d\u6a21\u578b\u5b9e\u73b0\u548c\u53c2\u6570\u3002\u5b8c\u6574\u7684\u6a21\u578b\u5b9e\u73b0\u5728 [Hugging Face Hub](https://huggingface.co/THUDM/chatglm2-6b)\u3002\u5982\u679c\u4f60\u7684\u7f51\u7edc\u73af\u5883\u8f83\u5dee\uff0c\u4e0b\u8f7d\u6a21\u578b\u53c2\u6570\u53ef\u80fd\u4f1a\u82b1\u8d39\u8f83\u957f\u65f6\u95f4\u751a\u81f3\u5931\u8d25\u3002\u6b64\u65f6\u53ef\u4ee5\u5148\u5c06\u6a21\u578b\u4e0b\u8f7d\u5230\u672c\u5730\uff0c\u7136\u540e\u4ece\u672c\u5730\u52a0\u8f7d\u3002\n\n\u4ece Hugging Face Hub \u4e0b\u8f7d\u6a21\u578b\u9700\u8981\u5148[\u5b89\u88c5Git LFS](https://docs.github.com/zh/repositories/working-with-files/managing-large-files/installing-git-large-file-storage)\uff0c\u7136\u540e\u8fd0\u884c\n```Shell\ngit clone https://huggingface.co/THUDM/chatglm2-6b\n```\n\n\u5982\u679c\u4f60\u4ece Hugging Face Hub \u4e0a\u4e0b\u8f7d checkpoint \u7684\u901f\u5ea6\u8f83\u6162\uff0c\u53ef\u4ee5\u53ea\u4e0b\u8f7d\u6a21\u578b\u5b9e\u73b0\n```Shell\nGIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/THUDM/chatglm2-6b\n```\n\u7136\u540e\u4ece[\u8fd9\u91cc](https://cloud.tsinghua.edu.cn/d/674208019e314311ab5c/)\u624b\u52a8\u4e0b\u8f7d\u6a21\u578b\u53c2\u6570\u6587\u4ef6\uff0c\u5e76\u5c06\u4e0b\u8f7d\u7684\u6587\u4ef6\u66ff\u6362\u5230\u672c\u5730\u7684 `chatglm2-6b` \u76ee\u5f55\u4e0b\u3002\n\n\n\u5c06\u6a21\u578b\u4e0b\u8f7d\u5230\u672c\u5730\u4e4b\u540e\uff0c\u5c06\u4ee5\u4e0a\u4ee3\u7801\u4e2d\u7684 `THUDM/chatglm2-6b` \u66ff\u6362\u4e3a\u4f60\u672c\u5730\u7684 `chatglm2-6b` \u6587\u4ef6\u5939\u7684\u8def\u5f84\uff0c\u5373\u53ef\u4ece\u672c\u5730\u52a0\u8f7d\u6a21\u578b\u3002\n\n\u6a21\u578b\u7684\u5b9e\u73b0\u4ecd\u7136\u5904\u5728\u53d8\u52a8\u4e2d\u3002\u5982\u679c\u5e0c\u671b\u56fa\u5b9a\u4f7f\u7528\u7684\u6a21\u578b\u5b9e\u73b0\u4ee5\u4fdd\u8bc1\u517c\u5bb9\u6027\uff0c\u53ef\u4ee5\u5728 `from_pretrained` \u7684\u8c03\u7528\u4e2d\u589e\u52a0 `revision=\"v1.0\"` \u53c2\u6570\u3002`v1.0` \u662f\u5f53\u524d\u6700\u65b0\u7684\u7248\u672c\u53f7\uff0c\u5b8c\u6574\u7684\u7248\u672c\u5217\u8868\u53c2\u89c1 [Change Log](https://huggingface.co/THUDM/chatglm2-6b#change-log)\u3002\n\n",
        "token": [
            "\u4ece\u672c\u5730\u52a0\u8f7d\u6a21\u578b",
            "\u4ee5\u4e0a\u4ee3\u7801\u4f1a\u7531",
            "`",
            "transformers",
            "`",
            "\u81ea\u52a8\u4e0b\u8f7d\u6a21\u578b\u5b9e\u73b0\u548c\u53c2\u6570\u3002\u5b8c\u6574\u7684\u6a21\u578b\u5b9e\u73b0\u5728",
            "[",
            "Hugging",
            "Face",
            "Hub",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/THUDM/chatglm2-6b",
            ")",
            "\u3002\u5982\u679c\u4f60\u7684\u7f51\u7edc\u73af\u5883\u8f83\u5dee\uff0c\u4e0b\u8f7d\u6a21\u578b\u53c2\u6570\u53ef\u80fd\u4f1a\u82b1\u8d39\u8f83\u957f\u65f6\u95f4\u751a\u81f3\u5931\u8d25\u3002\u6b64\u65f6\u53ef\u4ee5\u5148\u5c06\u6a21\u578b\u4e0b\u8f7d\u5230\u672c\u5730\uff0c\u7136\u540e\u4ece\u672c\u5730\u52a0\u8f7d\u3002",
            "\u4ece",
            "Hugging",
            "Face",
            "Hub",
            "\u4e0b\u8f7d\u6a21\u578b\u9700\u8981\u5148",
            "[",
            "\u5b89\u88c5Git",
            "LFS",
            "]",
            "(",
            "https",
            ":",
            "//docs.github.com/zh/repositories/working-with-files/managing-large-files/installing-git-large-file-storage",
            ")",
            "\uff0c\u7136\u540e\u8fd0\u884c",
            "``",
            "`",
            "Shell",
            "git",
            "clone",
            "https",
            ":",
            "//huggingface.co/THUDM/chatglm2-6b",
            "``",
            "`",
            "\u5982\u679c\u4f60\u4ece",
            "Hugging",
            "Face",
            "Hub",
            "\u4e0a\u4e0b\u8f7d",
            "checkpoint",
            "\u7684\u901f\u5ea6\u8f83\u6162\uff0c\u53ef\u4ee5\u53ea\u4e0b\u8f7d\u6a21\u578b\u5b9e\u73b0",
            "``",
            "`",
            "Shell",
            "GIT_LFS_SKIP_SMUDGE=1",
            "git",
            "clone",
            "https",
            ":",
            "//huggingface.co/THUDM/chatglm2-6b",
            "``",
            "`",
            "\u7136\u540e\u4ece",
            "[",
            "\u8fd9\u91cc",
            "]",
            "(",
            "https",
            ":",
            "//cloud.tsinghua.edu.cn/d/674208019e314311ab5c/",
            ")",
            "\u624b\u52a8\u4e0b\u8f7d\u6a21\u578b\u53c2\u6570\u6587\u4ef6\uff0c\u5e76\u5c06\u4e0b\u8f7d\u7684\u6587\u4ef6\u66ff\u6362\u5230\u672c\u5730\u7684",
            "`",
            "chatglm2-6b",
            "`",
            "\u76ee\u5f55\u4e0b\u3002",
            "\u5c06\u6a21\u578b\u4e0b\u8f7d\u5230\u672c\u5730\u4e4b\u540e\uff0c\u5c06\u4ee5\u4e0a\u4ee3\u7801\u4e2d\u7684",
            "`",
            "THUDM/chatglm2-6b",
            "`",
            "\u66ff\u6362\u4e3a\u4f60\u672c\u5730\u7684",
            "`",
            "chatglm2-6b",
            "`",
            "\u6587\u4ef6\u5939\u7684\u8def\u5f84\uff0c\u5373\u53ef\u4ece\u672c\u5730\u52a0\u8f7d\u6a21\u578b\u3002",
            "\u6a21\u578b\u7684\u5b9e\u73b0\u4ecd\u7136\u5904\u5728\u53d8\u52a8\u4e2d\u3002\u5982\u679c\u5e0c\u671b\u56fa\u5b9a\u4f7f\u7528\u7684\u6a21\u578b\u5b9e\u73b0\u4ee5\u4fdd\u8bc1\u517c\u5bb9\u6027\uff0c\u53ef\u4ee5\u5728",
            "`",
            "from_pretrained",
            "`",
            "\u7684\u8c03\u7528\u4e2d\u589e\u52a0",
            "`",
            "revision=",
            "''",
            "v1.0",
            "''",
            "`",
            "\u53c2\u6570\u3002",
            "`",
            "v1.0",
            "`",
            "\u662f\u5f53\u524d\u6700\u65b0\u7684\u7248\u672c\u53f7\uff0c\u5b8c\u6574\u7684\u7248\u672c\u5217\u8868\u53c2\u89c1",
            "[",
            "Change",
            "Log",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/THUDM/chatglm2-6b",
            "#",
            "change-log",
            ")",
            "\u3002"
        ],
        "level of complexity": 2
    },
    {
        "url": "https://github.com/THUDM/ChatGLM2-6B",
        "readme_url": "https://raw.githubusercontent.com/THUDM/ChatGLM2-6B/main/README.md",
        "topic": [
            "chatglm",
            "chatglm-6b",
            "large-language-models",
            "llm"
        ],
        "text": "API \u90e8\u7f72\n\u9996\u5148\u9700\u8981\u5b89\u88c5\u989d\u5916\u7684\u4f9d\u8d56 `pip install fastapi uvicorn`\uff0c\u7136\u540e\u8fd0\u884c\u4ed3\u5e93\u4e2d\u7684 [api.py](api.py)\uff1a\n```shell\npython api.py\n```\n\u9ed8\u8ba4\u90e8\u7f72\u5728\u672c\u5730\u7684 8000 \u7aef\u53e3\uff0c\u901a\u8fc7 POST \u65b9\u6cd5\u8fdb\u884c\u8c03\u7528\n```shell\ncurl -X POST \"http://127.0.0.1:8000\" \\\n     -H 'Content-Type: application/json' \\\n     -d '{\"prompt\": \"\u4f60\u597d\", \"history\": []}'\n```\n\u5f97\u5230\u7684\u8fd4\u56de\u503c\u4e3a\n```shell\n{\n  \"response\":\"\u4f60\u597d\ud83d\udc4b\uff01\u6211\u662f\u4eba\u5de5\u667a\u80fd\u52a9\u624b ChatGLM2-6B\uff0c\u5f88\u9ad8\u5174\u89c1\u5230\u4f60\uff0c\u6b22\u8fce\u95ee\u6211\u4efb\u4f55\u95ee\u9898\u3002\",\n  \"history\":[[\"\u4f60\u597d\",\"\u4f60\u597d\ud83d\udc4b\uff01\u6211\u662f\u4eba\u5de5\u667a\u80fd\u52a9\u624b ChatGLM2-6B\uff0c\u5f88\u9ad8\u5174\u89c1\u5230\u4f60\uff0c\u6b22\u8fce\u95ee\u6211\u4efb\u4f55\u95ee\u9898\u3002\"]],\n  \"status\":200,\n  \"time\":\"2023-03-23 21:38:40\"\n}\n```\n\u611f\u8c22 [@hiyouga]() \u5b9e\u73b0\u4e86 OpenAI \u683c\u5f0f\u7684\u6d41\u5f0f API \u90e8\u7f72\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u4efb\u610f\u57fa\u4e8e ChatGPT \u7684\u5e94\u7528\u7684\u540e\u7aef\uff0c\u6bd4\u5982 [ChatGPT-Next-Web](https://github.com/Yidadaa/ChatGPT-Next-Web)\u3002\u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c\u4ed3\u5e93\u4e2d\u7684[openai_api.py](openai_api.py) \u8fdb\u884c\u90e8\u7f72\uff1a\n```shell\npython openai_api.py\n```\n\u8fdb\u884c API \u8c03\u7528\u7684\u793a\u4f8b\u4ee3\u7801\u4e3a\n```python\nimport openai\nif __name__ == \"__main__\":\n    openai.api_base = \"http://localhost:8000/v1\"\n    openai.api_key = \"none\"\n    for chunk in openai.ChatCompletion.create(\n        model=\"chatglm2-6b\",\n        messages=[\n            {\"role\": \"user\", \"content\": \"\u4f60\u597d\"}\n        ],\n        stream=True\n    ):\n        if hasattr(chunk.choices[0].delta, \"content\"):\n            print(chunk.choices[0].delta.content, end=\"\", flush=True)\n```\n\n\n",
        "token": [
            "API",
            "\u90e8\u7f72",
            "\u9996\u5148\u9700\u8981\u5b89\u88c5\u989d\u5916\u7684\u4f9d\u8d56",
            "`",
            "pip",
            "install",
            "fastapi",
            "uvicorn",
            "`",
            "\uff0c\u7136\u540e\u8fd0\u884c\u4ed3\u5e93\u4e2d\u7684",
            "[",
            "api.py",
            "]",
            "(",
            "api.py",
            ")",
            "\uff1a",
            "``",
            "`",
            "shell",
            "python",
            "api.py",
            "``",
            "`",
            "\u9ed8\u8ba4\u90e8\u7f72\u5728\u672c\u5730\u7684",
            "8000",
            "\u7aef\u53e3\uff0c\u901a\u8fc7",
            "POST",
            "\u65b9\u6cd5\u8fdb\u884c\u8c03\u7528",
            "``",
            "`",
            "shell",
            "curl",
            "-X",
            "POST",
            "``",
            "http",
            ":",
            "//127.0.0.1:8000",
            "''",
            "\\",
            "-H",
            "'Content-Type",
            ":",
            "application/json",
            "'",
            "\\",
            "-d",
            "'",
            "{",
            "``",
            "prompt",
            "''",
            ":",
            "``",
            "\u4f60\u597d",
            "''",
            ",",
            "``",
            "history",
            "''",
            ":",
            "[",
            "]",
            "}",
            "'",
            "``",
            "`",
            "\u5f97\u5230\u7684\u8fd4\u56de\u503c\u4e3a",
            "``",
            "`",
            "shell",
            "{",
            "``",
            "response",
            "''",
            ":",
            "''",
            "\u4f60\u597d\ud83d\udc4b\uff01\u6211\u662f\u4eba\u5de5\u667a\u80fd\u52a9\u624b",
            "ChatGLM2-6B\uff0c\u5f88\u9ad8\u5174\u89c1\u5230\u4f60\uff0c\u6b22\u8fce\u95ee\u6211\u4efb\u4f55\u95ee\u9898\u3002",
            "''",
            ",",
            "``",
            "history",
            "''",
            ":",
            "[",
            "[",
            "``",
            "\u4f60\u597d",
            "''",
            ",",
            "''",
            "\u4f60\u597d\ud83d\udc4b\uff01\u6211\u662f\u4eba\u5de5\u667a\u80fd\u52a9\u624b",
            "ChatGLM2-6B\uff0c\u5f88\u9ad8\u5174\u89c1\u5230\u4f60\uff0c\u6b22\u8fce\u95ee\u6211\u4efb\u4f55\u95ee\u9898\u3002",
            "''",
            "]",
            "]",
            ",",
            "``",
            "status",
            "''",
            ":200",
            ",",
            "``",
            "time",
            "''",
            ":",
            "''",
            "2023-03-23",
            "21:38:40",
            "''",
            "}",
            "``",
            "`",
            "\u611f\u8c22",
            "[",
            "@",
            "hiyouga",
            "]",
            "(",
            ")",
            "\u5b9e\u73b0\u4e86",
            "OpenAI",
            "\u683c\u5f0f\u7684\u6d41\u5f0f",
            "API",
            "\u90e8\u7f72\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u4efb\u610f\u57fa\u4e8e",
            "ChatGPT",
            "\u7684\u5e94\u7528\u7684\u540e\u7aef\uff0c\u6bd4\u5982",
            "[",
            "ChatGPT-Next-Web",
            "]",
            "(",
            "https",
            ":",
            "//github.com/Yidadaa/ChatGPT-Next-Web",
            ")",
            "\u3002\u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c\u4ed3\u5e93\u4e2d\u7684",
            "[",
            "openai_api.py",
            "]",
            "(",
            "openai_api.py",
            ")",
            "\u8fdb\u884c\u90e8\u7f72\uff1a",
            "``",
            "`",
            "shell",
            "python",
            "openai_api.py",
            "``",
            "`",
            "\u8fdb\u884c",
            "API",
            "\u8c03\u7528\u7684\u793a\u4f8b\u4ee3\u7801\u4e3a",
            "``",
            "`",
            "python",
            "import",
            "openai",
            "if",
            "__name__",
            "==",
            "``",
            "__main__",
            "''",
            ":",
            "openai.api_base",
            "=",
            "``",
            "http",
            ":",
            "//localhost:8000/v1",
            "''",
            "openai.api_key",
            "=",
            "``",
            "none",
            "''",
            "for",
            "chunk",
            "in",
            "openai.ChatCompletion.create",
            "(",
            "model=",
            "''",
            "chatglm2-6b",
            "''",
            ",",
            "messages=",
            "[",
            "{",
            "``",
            "role",
            "''",
            ":",
            "``",
            "user",
            "''",
            ",",
            "``",
            "content",
            "''",
            ":",
            "``",
            "\u4f60\u597d",
            "''",
            "}",
            "]",
            ",",
            "stream=True",
            ")",
            ":",
            "if",
            "hasattr",
            "(",
            "chunk.choices",
            "[",
            "0",
            "]",
            ".delta",
            ",",
            "``",
            "content",
            "''",
            ")",
            ":",
            "print",
            "(",
            "chunk.choices",
            "[",
            "0",
            "]",
            ".delta.content",
            ",",
            "end=",
            "''",
            "''",
            ",",
            "flush=True",
            ")",
            "``",
            "`"
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/THUDM/ChatGLM2-6B",
        "readme_url": "https://raw.githubusercontent.com/THUDM/ChatGLM2-6B/main/README.md",
        "topic": [
            "chatglm",
            "chatglm-6b",
            "large-language-models",
            "llm"
        ],
        "text": "\u591a\u5361\u90e8\u7f72\n\u5982\u679c\u4f60\u6709\u591a\u5f20 GPU\uff0c\u4f46\u662f\u6bcf\u5f20 GPU \u7684\u663e\u5b58\u5927\u5c0f\u90fd\u4e0d\u8db3\u4ee5\u5bb9\u7eb3\u5b8c\u6574\u7684\u6a21\u578b\uff0c\u90a3\u4e48\u53ef\u4ee5\u5c06\u6a21\u578b\u5207\u5206\u5728\u591a\u5f20GPU\u4e0a\u3002\u9996\u5148\u5b89\u88c5 accelerate: `pip install accelerate`\uff0c\u7136\u540e\u901a\u8fc7\u5982\u4e0b\u65b9\u6cd5\u52a0\u8f7d\u6a21\u578b\uff1a\n```python\nfrom utils import load_model_on_gpus\nmodel = load_model_on_gpus(\"THUDM/chatglm2-6b\", num_gpus=2)\n```\n\u5373\u53ef\u5c06\u6a21\u578b\u90e8\u7f72\u5230\u4e24\u5f20 GPU \u4e0a\u8fdb\u884c\u63a8\u7406\u3002\u4f60\u53ef\u4ee5\u5c06 `num_gpus` \u6539\u4e3a\u4f60\u5e0c\u671b\u4f7f\u7528\u7684 GPU \u6570\u3002\u9ed8\u8ba4\u662f\u5747\u5300\u5207\u5206\u7684\uff0c\u4f60\u4e5f\u53ef\u4ee5\u4f20\u5165 `device_map` \u53c2\u6570\u6765\u81ea\u5df1\u6307\u5b9a\u3002 \n\n",
        "token": [
            "\u591a\u5361\u90e8\u7f72",
            "\u5982\u679c\u4f60\u6709\u591a\u5f20",
            "GPU\uff0c\u4f46\u662f\u6bcf\u5f20",
            "GPU",
            "\u7684\u663e\u5b58\u5927\u5c0f\u90fd\u4e0d\u8db3\u4ee5\u5bb9\u7eb3\u5b8c\u6574\u7684\u6a21\u578b\uff0c\u90a3\u4e48\u53ef\u4ee5\u5c06\u6a21\u578b\u5207\u5206\u5728\u591a\u5f20GPU\u4e0a\u3002\u9996\u5148\u5b89\u88c5",
            "accelerate",
            ":",
            "`",
            "pip",
            "install",
            "accelerate",
            "`",
            "\uff0c\u7136\u540e\u901a\u8fc7\u5982\u4e0b\u65b9\u6cd5\u52a0\u8f7d\u6a21\u578b\uff1a",
            "``",
            "`",
            "python",
            "from",
            "utils",
            "import",
            "load_model_on_gpus",
            "model",
            "=",
            "load_model_on_gpus",
            "(",
            "``",
            "THUDM/chatglm2-6b",
            "''",
            ",",
            "num_gpus=2",
            ")",
            "``",
            "`",
            "\u5373\u53ef\u5c06\u6a21\u578b\u90e8\u7f72\u5230\u4e24\u5f20",
            "GPU",
            "\u4e0a\u8fdb\u884c\u63a8\u7406\u3002\u4f60\u53ef\u4ee5\u5c06",
            "`",
            "num_gpus",
            "`",
            "\u6539\u4e3a\u4f60\u5e0c\u671b\u4f7f\u7528\u7684",
            "GPU",
            "\u6570\u3002\u9ed8\u8ba4\u662f\u5747\u5300\u5207\u5206\u7684\uff0c\u4f60\u4e5f\u53ef\u4ee5\u4f20\u5165",
            "`",
            "device_map",
            "`",
            "\u53c2\u6570\u6765\u81ea\u5df1\u6307\u5b9a\u3002"
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/langgenius/dify",
        "readme_url": "https://raw.githubusercontent.com/langgenius/dify/main/README.md",
        "topic": [
            "ai",
            "backend-as-a-service",
            "gpt",
            "gpt-4",
            "langchain",
            "llama2",
            "llm",
            "openai",
            "orchestration",
            "python",
            "rag"
        ],
        "text": "Before You Start\n\n**Star us on GitHub, and be instantly notified for new releases!**\n\n![star-us](https://github.com/langgenius/dify/assets/100913391/95f37259-7370-4456-a9f0-0bc01ef8642f)\n\n- [Website](https://dify.ai)\n- [Docs](https://docs.dify.ai)\n- [Deployment Docs](https://docs.dify.ai/getting-started/install-self-hosted)\n- [FAQ](https://docs.dify.ai/getting-started/faq) \n\n\n",
        "token": [
            "Before",
            "You",
            "Start",
            "*",
            "*",
            "Star",
            "us",
            "on",
            "GitHub",
            ",",
            "and",
            "be",
            "instantly",
            "notified",
            "for",
            "new",
            "releases",
            "!",
            "*",
            "*",
            "!",
            "[",
            "star-us",
            "]",
            "(",
            "https",
            ":",
            "//github.com/langgenius/dify/assets/100913391/95f37259-7370-4456-a9f0-0bc01ef8642f",
            ")",
            "-",
            "[",
            "Website",
            "]",
            "(",
            "https",
            ":",
            "//dify.ai",
            ")",
            "-",
            "[",
            "Docs",
            "]",
            "(",
            "https",
            ":",
            "//docs.dify.ai",
            ")",
            "-",
            "[",
            "Deployment",
            "Docs",
            "]",
            "(",
            "https",
            ":",
            "//docs.dify.ai/getting-started/install-self-hosted",
            ")",
            "-",
            "[",
            "FAQ",
            "]",
            "(",
            "https",
            ":",
            "//docs.dify.ai/getting-started/faq",
            ")"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/langgenius/dify",
        "readme_url": "https://raw.githubusercontent.com/langgenius/dify/main/README.md",
        "topic": [
            "ai",
            "backend-as-a-service",
            "gpt",
            "gpt-4",
            "langchain",
            "llama2",
            "llm",
            "openai",
            "orchestration",
            "python",
            "rag"
        ],
        "text": "Install the Community Edition\n\n",
        "token": [
            "Install",
            "the",
            "Community",
            "Edition"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/langgenius/dify",
        "readme_url": "https://raw.githubusercontent.com/langgenius/dify/main/README.md",
        "topic": [
            "ai",
            "backend-as-a-service",
            "gpt",
            "gpt-4",
            "langchain",
            "llama2",
            "llm",
            "openai",
            "orchestration",
            "python",
            "rag"
        ],
        "text": "System Requirements\n\nBefore installing Dify, make sure your machine meets the following minimum system requirements:\n\n- CPU >= 2 Core\n- RAM >= 4GB\n\n",
        "token": [
            "System",
            "Requirements",
            "Before",
            "installing",
            "Dify",
            ",",
            "make",
            "sure",
            "your",
            "machine",
            "meets",
            "the",
            "following",
            "minimum",
            "system",
            "requirements",
            ":",
            "-",
            "CPU",
            ">",
            "=",
            "2",
            "Core",
            "-",
            "RAM",
            ">",
            "=",
            "4GB"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/langgenius/dify",
        "readme_url": "https://raw.githubusercontent.com/langgenius/dify/main/README.md",
        "topic": [
            "ai",
            "backend-as-a-service",
            "gpt",
            "gpt-4",
            "langchain",
            "llama2",
            "llm",
            "openai",
            "orchestration",
            "python",
            "rag"
        ],
        "text": "Quick Start\n\nThe easiest way to start the Dify server is to run our [docker-compose.yml](docker/docker-compose.yaml) file. Before running the installation command, make sure that [Docker](https://docs.docker.com/get-docker/) and [Docker Compose](https://docs.docker.com/compose/install/) are installed on your machine:\n\n```bash\ncd docker\ndocker compose up -d\n```\n\nAfter running, you can access the Dify dashboard in your browser at [http://localhost/install](http://localhost/install) and start the initialization installation process.\n\n",
        "token": [
            "Quick",
            "Start",
            "The",
            "easiest",
            "way",
            "to",
            "start",
            "the",
            "Dify",
            "server",
            "is",
            "to",
            "run",
            "our",
            "[",
            "docker-compose.yml",
            "]",
            "(",
            "docker/docker-compose.yaml",
            ")",
            "file",
            ".",
            "Before",
            "running",
            "the",
            "installation",
            "command",
            ",",
            "make",
            "sure",
            "that",
            "[",
            "Docker",
            "]",
            "(",
            "https",
            ":",
            "//docs.docker.com/get-docker/",
            ")",
            "and",
            "[",
            "Docker",
            "Compose",
            "]",
            "(",
            "https",
            ":",
            "//docs.docker.com/compose/install/",
            ")",
            "are",
            "installed",
            "on",
            "your",
            "machine",
            ":",
            "``",
            "`",
            "bash",
            "cd",
            "docker",
            "docker",
            "compose",
            "up",
            "-d",
            "``",
            "`",
            "After",
            "running",
            ",",
            "you",
            "can",
            "access",
            "the",
            "Dify",
            "dashboard",
            "in",
            "your",
            "browser",
            "at",
            "[",
            "http",
            ":",
            "//localhost/install",
            "]",
            "(",
            "http",
            ":",
            "//localhost/install",
            ")",
            "and",
            "start",
            "the",
            "initialization",
            "installation",
            "process",
            "."
        ],
        "level of complexity": 1
    },
    {
        "url": "https://github.com/langgenius/dify",
        "readme_url": "https://raw.githubusercontent.com/langgenius/dify/main/README.md",
        "topic": [
            "ai",
            "backend-as-a-service",
            "gpt",
            "gpt-4",
            "langchain",
            "llama2",
            "llm",
            "openai",
            "orchestration",
            "python",
            "rag"
        ],
        "text": "Configuration\n\nIf you need to customize the configuration, please refer to the comments in our [docker-compose.yml](docker/docker-compose.yaml) file and manually set the environment configuration. After making the changes, please run `docker-compose up -d` again. You can see the full list of environment variables in our [docs](https://docs.dify.ai/getting-started/install-self-hosted/environments).\n\n\n",
        "token": [
            "Configuration",
            "If",
            "you",
            "need",
            "to",
            "customize",
            "the",
            "configuration",
            ",",
            "please",
            "refer",
            "to",
            "the",
            "comments",
            "in",
            "our",
            "[",
            "docker-compose.yml",
            "]",
            "(",
            "docker/docker-compose.yaml",
            ")",
            "file",
            "and",
            "manually",
            "set",
            "the",
            "environment",
            "configuration",
            ".",
            "After",
            "making",
            "the",
            "changes",
            ",",
            "please",
            "run",
            "`",
            "docker-compose",
            "up",
            "-d",
            "`",
            "again",
            ".",
            "You",
            "can",
            "see",
            "the",
            "full",
            "list",
            "of",
            "environment",
            "variables",
            "in",
            "our",
            "[",
            "docs",
            "]",
            "(",
            "https",
            ":",
            "//docs.dify.ai/getting-started/install-self-hosted/environments",
            ")",
            "."
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/vllm-project/vllm",
        "readme_url": "https://raw.githubusercontent.com/vllm-project/vllm/main/README.md",
        "topic": [
            "gpt",
            "inference",
            "llama",
            "llm",
            "llm-serving",
            "llmops",
            "mlops",
            "model-serving",
            "pytorch",
            "transformer"
        ],
        "text": "About\nvLLM is a fast and easy-to-use library for LLM inference and serving.\n\nvLLM is fast with:\n\n- State-of-the-art serving throughput\n- Efficient management of attention key and value memory with **PagedAttention**\n- Continuous batching of incoming requests\n- Fast model execution with CUDA/HIP graph\n- Quantization: [GPTQ](https://arxiv.org/abs/2210.17323), [AWQ](https://arxiv.org/abs/2306.00978), [SqueezeLLM](https://arxiv.org/abs/2306.07629), FP8 KV Cache\n- Optimized CUDA kernels\n\nvLLM is flexible and easy to use with:\n\n- Seamless integration with popular Hugging Face models\n- High-throughput serving with various decoding algorithms, including *parallel sampling*, *beam search*, and more\n- Tensor parallelism support for distributed inference\n- Streaming outputs\n- OpenAI-compatible API server\n- Support NVIDIA GPUs and AMD GPUs\n- (Experimental) Prefix caching support\n- (Experimental) Multi-lora support\n\nvLLM seamlessly supports many Hugging Face models, including the following architectures:\n\n- Aquila & Aquila2 (`BAAI/AquilaChat2-7B`, `BAAI/AquilaChat2-34B`, `BAAI/Aquila-7B`, `BAAI/AquilaChat-7B`, etc.)\n- Baichuan & Baichuan2 (`baichuan-inc/Baichuan2-13B-Chat`, `baichuan-inc/Baichuan-7B`, etc.)\n- BLOOM (`bigscience/bloom`, `bigscience/bloomz`, etc.)\n- ChatGLM (`THUDM/chatglm2-6b`, `THUDM/chatglm3-6b`, etc.)\n- DeciLM (`Deci/DeciLM-7B`, `Deci/DeciLM-7B-instruct`, etc.)\n- Falcon (`tiiuae/falcon-7b`, `tiiuae/falcon-40b`, `tiiuae/falcon-rw-7b`, etc.)\n- GPT-2 (`gpt2`, `gpt2-xl`, etc.)\n- GPT BigCode (`bigcode/starcoder`, `bigcode/gpt_bigcode-santacoder`, etc.)\n- GPT-J (`EleutherAI/gpt-j-6b`, `nomic-ai/gpt4all-j`, etc.)\n- GPT-NeoX (`EleutherAI/gpt-neox-20b`, `databricks/dolly-v2-12b`, `stabilityai/stablelm-tuned-alpha-7b`, etc.)\n- InternLM (`internlm/internlm-7b`, `internlm/internlm-chat-7b`, etc.)\n- LLaMA & LLaMA-2 (`meta-llama/Llama-2-70b-hf`, `lmsys/vicuna-13b-v1.3`, `young-geng/koala`, `openlm-research/open_llama_13b`, etc.)\n- Mistral (`mistralai/Mistral-7B-v0.1`, `mistralai/Mistral-7B-Instruct-v0.1`, etc.)\n- Mixtral (`mistralai/Mixtral-8x7B-v0.1`, `mistralai/Mixtral-8x7B-Instruct-v0.1`, etc.)\n- MPT (`mosaicml/mpt-7b`, `mosaicml/mpt-30b`, etc.)\n- OPT (`facebook/opt-66b`, `facebook/opt-iml-max-30b`, etc.)\n- Phi (`microsoft/phi-1_5`, `microsoft/phi-2`, etc.)\n- Qwen (`Qwen/Qwen-7B`, `Qwen/Qwen-7B-Chat`, etc.)\n- Qwen2 (`Qwen/Qwen2-7B-beta`, `Qwen/Qwen-7B-Chat-beta`, etc.)\n- StableLM(`stabilityai/stablelm-3b-4e1t`, `stabilityai/stablelm-base-alpha-7b-v2`, etc.)\n- Yi (`01-ai/Yi-6B`, `01-ai/Yi-34B`, etc.)\n\nInstall vLLM with pip or [from source](https://vllm.readthedocs.io/en/latest/getting_started/installation.html#build-from-source):\n\n```bash\npip install vllm\n```\n\n",
        "token": [
            "About",
            "vLLM",
            "is",
            "a",
            "fast",
            "and",
            "easy-to-use",
            "library",
            "for",
            "LLM",
            "inference",
            "and",
            "serving",
            ".",
            "vLLM",
            "is",
            "fast",
            "with",
            ":",
            "-",
            "State-of-the-art",
            "serving",
            "throughput",
            "-",
            "Efficient",
            "management",
            "of",
            "attention",
            "key",
            "and",
            "value",
            "memory",
            "with",
            "*",
            "*",
            "PagedAttention",
            "*",
            "*",
            "-",
            "Continuous",
            "batching",
            "of",
            "incoming",
            "requests",
            "-",
            "Fast",
            "model",
            "execution",
            "with",
            "CUDA/HIP",
            "graph",
            "-",
            "Quantization",
            ":",
            "[",
            "GPTQ",
            "]",
            "(",
            "https",
            ":",
            "//arxiv.org/abs/2210.17323",
            ")",
            ",",
            "[",
            "AWQ",
            "]",
            "(",
            "https",
            ":",
            "//arxiv.org/abs/2306.00978",
            ")",
            ",",
            "[",
            "SqueezeLLM",
            "]",
            "(",
            "https",
            ":",
            "//arxiv.org/abs/2306.07629",
            ")",
            ",",
            "FP8",
            "KV",
            "Cache",
            "-",
            "Optimized",
            "CUDA",
            "kernels",
            "vLLM",
            "is",
            "flexible",
            "and",
            "easy",
            "to",
            "use",
            "with",
            ":",
            "-",
            "Seamless",
            "integration",
            "with",
            "popular",
            "Hugging",
            "Face",
            "models",
            "-",
            "High-throughput",
            "serving",
            "with",
            "various",
            "decoding",
            "algorithms",
            ",",
            "including",
            "*",
            "parallel",
            "sampling",
            "*",
            ",",
            "*",
            "beam",
            "search",
            "*",
            ",",
            "and",
            "more",
            "-",
            "Tensor",
            "parallelism",
            "support",
            "for",
            "distributed",
            "inference",
            "-",
            "Streaming",
            "outputs",
            "-",
            "OpenAI-compatible",
            "API",
            "server",
            "-",
            "Support",
            "NVIDIA",
            "GPUs",
            "and",
            "AMD",
            "GPUs",
            "-",
            "(",
            "Experimental",
            ")",
            "Prefix",
            "caching",
            "support",
            "-",
            "(",
            "Experimental",
            ")",
            "Multi-lora",
            "support",
            "vLLM",
            "seamlessly",
            "supports",
            "many",
            "Hugging",
            "Face",
            "models",
            ",",
            "including",
            "the",
            "following",
            "architectures",
            ":",
            "-",
            "Aquila",
            "&",
            "Aquila2",
            "(",
            "`",
            "BAAI/AquilaChat2-7B",
            "`",
            ",",
            "`",
            "BAAI/AquilaChat2-34B",
            "`",
            ",",
            "`",
            "BAAI/Aquila-7B",
            "`",
            ",",
            "`",
            "BAAI/AquilaChat-7B",
            "`",
            ",",
            "etc",
            ".",
            ")",
            "-",
            "Baichuan",
            "&",
            "Baichuan2",
            "(",
            "`",
            "baichuan-inc/Baichuan2-13B-Chat",
            "`",
            ",",
            "`",
            "baichuan-inc/Baichuan-7B",
            "`",
            ",",
            "etc",
            ".",
            ")",
            "-",
            "BLOOM",
            "(",
            "`",
            "bigscience/bloom",
            "`",
            ",",
            "`",
            "bigscience/bloomz",
            "`",
            ",",
            "etc",
            ".",
            ")",
            "-",
            "ChatGLM",
            "(",
            "`",
            "THUDM/chatglm2-6b",
            "`",
            ",",
            "`",
            "THUDM/chatglm3-6b",
            "`",
            ",",
            "etc",
            ".",
            ")",
            "-",
            "DeciLM",
            "(",
            "`",
            "Deci/DeciLM-7B",
            "`",
            ",",
            "`",
            "Deci/DeciLM-7B-instruct",
            "`",
            ",",
            "etc",
            ".",
            ")",
            "-",
            "Falcon",
            "(",
            "`",
            "tiiuae/falcon-7b",
            "`",
            ",",
            "`",
            "tiiuae/falcon-40b",
            "`",
            ",",
            "`",
            "tiiuae/falcon-rw-7b",
            "`",
            ",",
            "etc",
            ".",
            ")",
            "-",
            "GPT-2",
            "(",
            "`",
            "gpt2",
            "`",
            ",",
            "`",
            "gpt2-xl",
            "`",
            ",",
            "etc",
            ".",
            ")",
            "-",
            "GPT",
            "BigCode",
            "(",
            "`",
            "bigcode/starcoder",
            "`",
            ",",
            "`",
            "bigcode/gpt_bigcode-santacoder",
            "`",
            ",",
            "etc",
            ".",
            ")",
            "-",
            "GPT-J",
            "(",
            "`",
            "EleutherAI/gpt-j-6b",
            "`",
            ",",
            "`",
            "nomic-ai/gpt4all-j",
            "`",
            ",",
            "etc",
            ".",
            ")",
            "-",
            "GPT-NeoX",
            "(",
            "`",
            "EleutherAI/gpt-neox-20b",
            "`",
            ",",
            "`",
            "databricks/dolly-v2-12b",
            "`",
            ",",
            "`",
            "stabilityai/stablelm-tuned-alpha-7b",
            "`",
            ",",
            "etc",
            ".",
            ")",
            "-",
            "InternLM",
            "(",
            "`",
            "internlm/internlm-7b",
            "`",
            ",",
            "`",
            "internlm/internlm-chat-7b",
            "`",
            ",",
            "etc",
            ".",
            ")",
            "-",
            "LLaMA",
            "&",
            "LLaMA-2",
            "(",
            "`",
            "meta-llama/Llama-2-70b-hf",
            "`",
            ",",
            "`",
            "lmsys/vicuna-13b-v1.3",
            "`",
            ",",
            "`",
            "young-geng/koala",
            "`",
            ",",
            "`",
            "openlm-research/open_llama_13b",
            "`",
            ",",
            "etc",
            ".",
            ")",
            "-",
            "Mistral",
            "(",
            "`",
            "mistralai/Mistral-7B-v0.1",
            "`",
            ",",
            "`",
            "mistralai/Mistral-7B-Instruct-v0.1",
            "`",
            ",",
            "etc",
            ".",
            ")",
            "-",
            "Mixtral",
            "(",
            "`",
            "mistralai/Mixtral-8x7B-v0.1",
            "`",
            ",",
            "`",
            "mistralai/Mixtral-8x7B-Instruct-v0.1",
            "`",
            ",",
            "etc",
            ".",
            ")",
            "-",
            "MPT",
            "(",
            "`",
            "mosaicml/mpt-7b",
            "`",
            ",",
            "`",
            "mosaicml/mpt-30b",
            "`",
            ",",
            "etc",
            ".",
            ")",
            "-",
            "OPT",
            "(",
            "`",
            "facebook/opt-66b",
            "`",
            ",",
            "`",
            "facebook/opt-iml-max-30b",
            "`",
            ",",
            "etc",
            ".",
            ")",
            "-",
            "Phi",
            "(",
            "`",
            "microsoft/phi-1_5",
            "`",
            ",",
            "`",
            "microsoft/phi-2",
            "`",
            ",",
            "etc",
            ".",
            ")",
            "-",
            "Qwen",
            "(",
            "`",
            "Qwen/Qwen-7B",
            "`",
            ",",
            "`",
            "Qwen/Qwen-7B-Chat",
            "`",
            ",",
            "etc",
            ".",
            ")",
            "-",
            "Qwen2",
            "(",
            "`",
            "Qwen/Qwen2-7B-beta",
            "`",
            ",",
            "`",
            "Qwen/Qwen-7B-Chat-beta",
            "`",
            ",",
            "etc",
            ".",
            ")",
            "-",
            "StableLM",
            "(",
            "`",
            "stabilityai/stablelm-3b-4e1t",
            "`",
            ",",
            "`",
            "stabilityai/stablelm-base-alpha-7b-v2",
            "`",
            ",",
            "etc",
            ".",
            ")",
            "-",
            "Yi",
            "(",
            "`",
            "01-ai/Yi-6B",
            "`",
            ",",
            "`",
            "01-ai/Yi-34B",
            "`",
            ",",
            "etc",
            ".",
            ")",
            "Install",
            "vLLM",
            "with",
            "pip",
            "or",
            "[",
            "from",
            "source",
            "]",
            "(",
            "https",
            ":",
            "//vllm.readthedocs.io/en/latest/getting_started/installation.html",
            "#",
            "build-from-source",
            ")",
            ":",
            "``",
            "`",
            "bash",
            "pip",
            "install",
            "vllm",
            "``",
            "`"
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/vllm-project/vllm",
        "readme_url": "https://raw.githubusercontent.com/vllm-project/vllm/main/README.md",
        "topic": [
            "gpt",
            "inference",
            "llama",
            "llm",
            "llm-serving",
            "llmops",
            "mlops",
            "model-serving",
            "pytorch",
            "transformer"
        ],
        "text": "Getting Started\n\nVisit our [documentation](https://vllm.readthedocs.io/en/latest/) to get started.\n- [Installation](https://vllm.readthedocs.io/en/latest/getting_started/installation.html)\n- [Quickstart](https://vllm.readthedocs.io/en/latest/getting_started/quickstart.html)\n- [Supported Models](https://vllm.readthedocs.io/en/latest/models/supported_models.html)\n\n",
        "token": [
            "Getting",
            "Started",
            "Visit",
            "our",
            "[",
            "documentation",
            "]",
            "(",
            "https",
            ":",
            "//vllm.readthedocs.io/en/latest/",
            ")",
            "to",
            "get",
            "started",
            ".",
            "-",
            "[",
            "Installation",
            "]",
            "(",
            "https",
            ":",
            "//vllm.readthedocs.io/en/latest/getting_started/installation.html",
            ")",
            "-",
            "[",
            "Quickstart",
            "]",
            "(",
            "https",
            ":",
            "//vllm.readthedocs.io/en/latest/getting_started/quickstart.html",
            ")",
            "-",
            "[",
            "Supported",
            "Models",
            "]",
            "(",
            "https",
            ":",
            "//vllm.readthedocs.io/en/latest/models/supported_models.html",
            ")"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/vllm-project/vllm",
        "readme_url": "https://raw.githubusercontent.com/vllm-project/vllm/main/README.md",
        "topic": [
            "gpt",
            "inference",
            "llama",
            "llm",
            "llm-serving",
            "llmops",
            "mlops",
            "model-serving",
            "pytorch",
            "transformer"
        ],
        "text": "Contributing\n\nWe welcome and value any contributions and collaborations.\nPlease check out [CONTRIBUTING.md](./CONTRIBUTING.md) for how to get involved.\n\n",
        "token": [
            "Contributing",
            "We",
            "welcome",
            "and",
            "value",
            "any",
            "contributions",
            "and",
            "collaborations",
            ".",
            "Please",
            "check",
            "out",
            "[",
            "CONTRIBUTING.md",
            "]",
            "(",
            "./CONTRIBUTING.md",
            ")",
            "for",
            "how",
            "to",
            "get",
            "involved",
            "."
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/huggingface/peft",
        "readme_url": "https://raw.githubusercontent.com/huggingface/peft/main/README.md",
        "topic": [
            "adapter",
            "diffusion",
            "llm",
            "lora",
            "parameter-efficient-learning",
            "python",
            "pytorch",
            "transformers"
        ],
        "text": "Getting started\n\n```python\nfrom transformers import AutoModelForSeq2SeqLM\nfrom peft import get_peft_config, get_peft_model, LoraConfig, TaskType\nmodel_name_or_path = \"bigscience/mt0-large\"\ntokenizer_name_or_path = \"bigscience/mt0-large\"\n\npeft_config = LoraConfig(\n    task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1\n)\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\nmodel = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()\n",
        "token": [
            "Getting",
            "started",
            "``",
            "`",
            "python",
            "from",
            "transformers",
            "import",
            "AutoModelForSeq2SeqLM",
            "from",
            "peft",
            "import",
            "get_peft_config",
            ",",
            "get_peft_model",
            ",",
            "LoraConfig",
            ",",
            "TaskType",
            "model_name_or_path",
            "=",
            "``",
            "bigscience/mt0-large",
            "''",
            "tokenizer_name_or_path",
            "=",
            "``",
            "bigscience/mt0-large",
            "''",
            "peft_config",
            "=",
            "LoraConfig",
            "(",
            "task_type=TaskType.SEQ_2_SEQ_LM",
            ",",
            "inference_mode=False",
            ",",
            "r=8",
            ",",
            "lora_alpha=32",
            ",",
            "lora_dropout=0.1",
            ")",
            "model",
            "=",
            "AutoModelForSeq2SeqLM.from_pretrained",
            "(",
            "model_name_or_path",
            ")",
            "model",
            "=",
            "get_peft_model",
            "(",
            "model",
            ",",
            "peft_config",
            ")",
            "model.print_trainable_parameters",
            "(",
            ")"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/huggingface/peft",
        "readme_url": "https://raw.githubusercontent.com/huggingface/peft/main/README.md",
        "topic": [
            "adapter",
            "diffusion",
            "llm",
            "lora",
            "parameter-efficient-learning",
            "python",
            "pytorch",
            "transformers"
        ],
        "text": "INT8 training of large models in Colab using PEFT LoRA and bitsandbytes\n\n- Here is now a demo on how to fine tune [OPT-6.7b](https://huggingface.co/facebook/opt-6.7b) (14GB in fp16) in a Google Colab: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1jCkpikz0J2o20FBQmYmAGdiKmJGOMo-o?usp=sharing)\n\n- Here is now a demo on how to fine tune [whisper-large](https://huggingface.co/openai/whisper-large-v2) (1.5B params) (14GB in fp16) in a Google Colab: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1DOkD_5OUjFa0r5Ik3SgywJLJtEo2qLxO?usp=sharing) and [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1vhF8yueFqha3Y3CpTHN6q9EVcII9EYzs?usp=sharing)\n\n",
        "token": [
            "INT8",
            "training",
            "of",
            "large",
            "models",
            "in",
            "Colab",
            "using",
            "PEFT",
            "LoRA",
            "and",
            "bitsandbytes",
            "-",
            "Here",
            "is",
            "now",
            "a",
            "demo",
            "on",
            "how",
            "to",
            "fine",
            "tune",
            "[",
            "OPT-6.7b",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/facebook/opt-6.7b",
            ")",
            "(",
            "14GB",
            "in",
            "fp16",
            ")",
            "in",
            "a",
            "Google",
            "Colab",
            ":",
            "[",
            "!",
            "[",
            "Open",
            "In",
            "Colab",
            "]",
            "(",
            "https",
            ":",
            "//colab.research.google.com/assets/colab-badge.svg",
            ")",
            "]",
            "(",
            "https",
            ":",
            "//colab.research.google.com/drive/1jCkpikz0J2o20FBQmYmAGdiKmJGOMo-o",
            "?",
            "usp=sharing",
            ")",
            "-",
            "Here",
            "is",
            "now",
            "a",
            "demo",
            "on",
            "how",
            "to",
            "fine",
            "tune",
            "[",
            "whisper-large",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/openai/whisper-large-v2",
            ")",
            "(",
            "1.5B",
            "params",
            ")",
            "(",
            "14GB",
            "in",
            "fp16",
            ")",
            "in",
            "a",
            "Google",
            "Colab",
            ":",
            "[",
            "!",
            "[",
            "Open",
            "In",
            "Colab",
            "]",
            "(",
            "https",
            ":",
            "//colab.research.google.com/assets/colab-badge.svg",
            ")",
            "]",
            "(",
            "https",
            ":",
            "//colab.research.google.com/drive/1DOkD_5OUjFa0r5Ik3SgywJLJtEo2qLxO",
            "?",
            "usp=sharing",
            ")",
            "and",
            "[",
            "!",
            "[",
            "Open",
            "In",
            "Colab",
            "]",
            "(",
            "https",
            ":",
            "//colab.research.google.com/assets/colab-badge.svg",
            ")",
            "]",
            "(",
            "https",
            ":",
            "//colab.research.google.com/drive/1vhF8yueFqha3Y3CpTHN6q9EVcII9EYzs",
            "?",
            "usp=sharing",
            ")"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/hiyouga/LLaMA-Factory",
        "readme_url": "https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/README.md",
        "topic": [
            "agent",
            "baichuan",
            "chatglm",
            "fine-tuning",
            "generative-ai",
            "gpt",
            "instruction-tuning",
            "language-model",
            "large-language-models",
            "llama",
            "llm",
            "lora",
            "mistral",
            "mixture-of-experts",
            "peft",
            "qlora",
            "quantization",
            "qwen",
            "rlhf",
            "transformers"
        ],
        "text": "LLaMA Board: A One-stop Web UI for Getting Started with LLaMA Factory\n\nPreview LLaMA Board at **[\ud83e\udd17 Spaces](https://huggingface.co/spaces/hiyouga/LLaMA-Board)** or **[ModelScope](https://modelscope.cn/studios/hiyouga/LLaMA-Board)**.\n\nLaunch LLaMA Board via `CUDA_VISIBLE_DEVICES=0 python src/train_web.py`. (multiple GPUs are not supported yet in this mode)\n\nHere is an example of altering the self-cognition of an instruction-tuned language model within 10 minutes on a single GPU.\n\nhttps://github.com/hiyouga/LLaMA-Factory/assets/16256802/6ba60acc-e2e2-4bec-b846-2d88920d5ba1\n\n",
        "token": [
            "LLaMA",
            "Board",
            ":",
            "A",
            "One-stop",
            "Web",
            "UI",
            "for",
            "Getting",
            "Started",
            "with",
            "LLaMA",
            "Factory",
            "Preview",
            "LLaMA",
            "Board",
            "at",
            "*",
            "*",
            "[",
            "\ud83e\udd17",
            "Spaces",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/spaces/hiyouga/LLaMA-Board",
            ")",
            "*",
            "*",
            "or",
            "*",
            "*",
            "[",
            "ModelScope",
            "]",
            "(",
            "https",
            ":",
            "//modelscope.cn/studios/hiyouga/LLaMA-Board",
            ")",
            "*",
            "*",
            ".",
            "Launch",
            "LLaMA",
            "Board",
            "via",
            "`",
            "CUDA_VISIBLE_DEVICES=0",
            "python",
            "src/train_web.py",
            "`",
            ".",
            "(",
            "multiple",
            "GPUs",
            "are",
            "not",
            "supported",
            "yet",
            "in",
            "this",
            "mode",
            ")",
            "Here",
            "is",
            "an",
            "example",
            "of",
            "altering",
            "the",
            "self-cognition",
            "of",
            "an",
            "instruction-tuned",
            "language",
            "model",
            "within",
            "10",
            "minutes",
            "on",
            "a",
            "single",
            "GPU",
            ".",
            "https",
            ":",
            "//github.com/hiyouga/LLaMA-Factory/assets/16256802/6ba60acc-e2e2-4bec-b846-2d88920d5ba1"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/hiyouga/LLaMA-Factory",
        "readme_url": "https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/README.md",
        "topic": [
            "agent",
            "baichuan",
            "chatglm",
            "fine-tuning",
            "generative-ai",
            "gpt",
            "instruction-tuning",
            "language-model",
            "large-language-models",
            "llama",
            "llm",
            "lora",
            "mistral",
            "mixture-of-experts",
            "peft",
            "qlora",
            "quantization",
            "qwen",
            "rlhf",
            "transformers"
        ],
        "text": "Table of Contents\n\n- [Benchmark](#benchmark)\n- [Changelog](#changelog)\n- [Supported Models](#supported-models)\n- [Supported Training Approaches](#supported-training-approaches)\n- [Provided Datasets](#provided-datasets)\n- [Requirement](#requirement)\n- [Getting Started](#getting-started)\n- [Projects using LLaMA Factory](#projects-using-llama-factory)\n- [License](#license)\n- [Citation](#citation)\n- [Acknowledgement](#acknowledgement)\n\n",
        "token": [
            "Table",
            "of",
            "Contents",
            "-",
            "[",
            "Benchmark",
            "]",
            "(",
            "#",
            "benchmark",
            ")",
            "-",
            "[",
            "Changelog",
            "]",
            "(",
            "#",
            "changelog",
            ")",
            "-",
            "[",
            "Supported",
            "Models",
            "]",
            "(",
            "#",
            "supported-models",
            ")",
            "-",
            "[",
            "Supported",
            "Training",
            "Approaches",
            "]",
            "(",
            "#",
            "supported-training-approaches",
            ")",
            "-",
            "[",
            "Provided",
            "Datasets",
            "]",
            "(",
            "#",
            "provided-datasets",
            ")",
            "-",
            "[",
            "Requirement",
            "]",
            "(",
            "#",
            "requirement",
            ")",
            "-",
            "[",
            "Getting",
            "Started",
            "]",
            "(",
            "#",
            "getting-started",
            ")",
            "-",
            "[",
            "Projects",
            "using",
            "LLaMA",
            "Factory",
            "]",
            "(",
            "#",
            "projects-using-llama-factory",
            ")",
            "-",
            "[",
            "License",
            "]",
            "(",
            "#",
            "license",
            ")",
            "-",
            "[",
            "Citation",
            "]",
            "(",
            "#",
            "citation",
            ")",
            "-",
            "[",
            "Acknowledgement",
            "]",
            "(",
            "#",
            "acknowledgement",
            ")"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/hiyouga/LLaMA-Factory",
        "readme_url": "https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/README.md",
        "topic": [
            "agent",
            "baichuan",
            "chatglm",
            "fine-tuning",
            "generative-ai",
            "gpt",
            "instruction-tuning",
            "language-model",
            "large-language-models",
            "llama",
            "llm",
            "lora",
            "mistral",
            "mixture-of-experts",
            "peft",
            "qlora",
            "quantization",
            "qwen",
            "rlhf",
            "transformers"
        ],
        "text": "Provided Datasets\n\n<details><summary>Pre-training datasets</summary>\n\n- [Wiki Demo (en)](data/wiki_demo.txt)\n- [RefinedWeb (en)](https://huggingface.co/datasets/tiiuae/falcon-refinedweb)\n- [RedPajama V2 (en)](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-V2)\n- [Wikipedia (en)](https://huggingface.co/datasets/olm/olm-wikipedia-20221220)\n- [Wikipedia (zh)](https://huggingface.co/datasets/pleisto/wikipedia-cn-20230720-filtered)\n- [Pile (en)](https://huggingface.co/datasets/EleutherAI/pile)\n- [SkyPile (zh)](https://huggingface.co/datasets/Skywork/SkyPile-150B)\n- [The Stack (en)](https://huggingface.co/datasets/bigcode/the-stack)\n- [StarCoder (en)](https://huggingface.co/datasets/bigcode/starcoderdata)\n\n</details>\n\n<details><summary>Supervised fine-tuning datasets</summary>\n\n- [Stanford Alpaca (en)](https://github.com/tatsu-lab/stanford_alpaca)\n- [Stanford Alpaca (zh)](https://github.com/ymcui/Chinese-LLaMA-Alpaca)\n- [GPT-4 Generated Data (en&zh)](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)\n- [Self-cognition (zh)](data/self_cognition.json)\n- [Open Assistant (multilingual)](https://huggingface.co/datasets/OpenAssistant/oasst1)\n- [ShareGPT (zh)](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/Chinese-instruction-collection)\n- [Guanaco Dataset (multilingual)](https://huggingface.co/datasets/JosephusCheung/GuanacoDataset)\n- [BELLE 2M (zh)](https://huggingface.co/datasets/BelleGroup/train_2M_CN)\n- [BELLE 1M (zh)](https://huggingface.co/datasets/BelleGroup/train_1M_CN)\n- [BELLE 0.5M (zh)](https://huggingface.co/datasets/BelleGroup/train_0.5M_CN)\n- [BELLE Dialogue 0.4M (zh)](https://huggingface.co/datasets/BelleGroup/generated_chat_0.4M)\n- [BELLE School Math 0.25M (zh)](https://huggingface.co/datasets/BelleGroup/school_math_0.25M)\n- [BELLE Multiturn Chat 0.8M (zh)](https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M)\n- [UltraChat (en)](https://github.com/thunlp/UltraChat)\n- [LIMA (en)](https://huggingface.co/datasets/GAIR/lima)\n- [OpenPlatypus (en)](https://huggingface.co/datasets/garage-bAInd/Open-Platypus)\n- [CodeAlpaca 20k (en)](https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k)\n- [Alpaca CoT (multilingual)](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT)\n- [OpenOrca (en)](https://huggingface.co/datasets/Open-Orca/OpenOrca)\n- [MathInstruct (en)](https://huggingface.co/datasets/TIGER-Lab/MathInstruct)\n- [Firefly 1.1M (zh)](https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M)\n- [Web QA (zh)](https://huggingface.co/datasets/suolyer/webqa)\n- [WebNovel (zh)](https://huggingface.co/datasets/zxbsmk/webnovel_cn)\n- [Nectar (en)](https://huggingface.co/datasets/berkeley-nest/Nectar)\n- [deepctrl (en&zh)](https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data)\n- [Ad Gen (zh)](https://huggingface.co/datasets/HasturOfficial/adgen)\n- [ShareGPT Hyperfiltered (en)](https://huggingface.co/datasets/totally-not-an-llm/sharegpt-hyperfiltered-3k)\n- [ShareGPT4 (en&zh)](https://huggingface.co/datasets/shibing624/sharegpt_gpt4)\n- [UltraChat 200k (en)](https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k)\n- [AgentInstruct (en)](https://huggingface.co/datasets/THUDM/AgentInstruct)\n- [LMSYS Chat 1M (en)](https://huggingface.co/datasets/lmsys/lmsys-chat-1m)\n- [Evol Instruct V2 (en)](https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_V2_196k)\n- [Glaive Function Calling V2 (en)](https://huggingface.co/datasets/glaiveai/glaive-function-calling-v2)\n\n</details>\n\n<details><summary>Preference datasets</summary>\n\n- [HH-RLHF (en)](https://huggingface.co/datasets/Anthropic/hh-rlhf)\n- [Open Assistant (multilingual)](https://huggingface.co/datasets/OpenAssistant/oasst1)\n- [GPT-4 Generated Data (en&zh)](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)\n- [Nectar (en)](https://huggingface.co/datasets/berkeley-nest/Nectar)\n\n</details>\n\nPlease refer to [data/README.md](data/README.md) for details.\n\nSome datasets require confirmation before using them, so we recommend logging in with your Hugging Face account using these commands.\n\n```bash\npip install --upgrade huggingface_hub\nhuggingface-cli login\n```\n\n",
        "token": [
            "Provided",
            "Datasets",
            "<",
            "details",
            ">",
            "<",
            "summary",
            ">",
            "Pre-training",
            "datasets",
            "<",
            "/summary",
            ">",
            "-",
            "[",
            "Wiki",
            "Demo",
            "(",
            "en",
            ")",
            "]",
            "(",
            "data/wiki_demo.txt",
            ")",
            "-",
            "[",
            "RefinedWeb",
            "(",
            "en",
            ")",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/datasets/tiiuae/falcon-refinedweb",
            ")",
            "-",
            "[",
            "RedPajama",
            "V2",
            "(",
            "en",
            ")",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/datasets/togethercomputer/RedPajama-Data-V2",
            ")",
            "-",
            "[",
            "Wikipedia",
            "(",
            "en",
            ")",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/datasets/olm/olm-wikipedia-20221220",
            ")",
            "-",
            "[",
            "Wikipedia",
            "(",
            "zh",
            ")",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/datasets/pleisto/wikipedia-cn-20230720-filtered",
            ")",
            "-",
            "[",
            "Pile",
            "(",
            "en",
            ")",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/datasets/EleutherAI/pile",
            ")",
            "-",
            "[",
            "SkyPile",
            "(",
            "zh",
            ")",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/datasets/Skywork/SkyPile-150B",
            ")",
            "-",
            "[",
            "The",
            "Stack",
            "(",
            "en",
            ")",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/datasets/bigcode/the-stack",
            ")",
            "-",
            "[",
            "StarCoder",
            "(",
            "en",
            ")",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/datasets/bigcode/starcoderdata",
            ")",
            "<",
            "/details",
            ">",
            "<",
            "details",
            ">",
            "<",
            "summary",
            ">",
            "Supervised",
            "fine-tuning",
            "datasets",
            "<",
            "/summary",
            ">",
            "-",
            "[",
            "Stanford",
            "Alpaca",
            "(",
            "en",
            ")",
            "]",
            "(",
            "https",
            ":",
            "//github.com/tatsu-lab/stanford_alpaca",
            ")",
            "-",
            "[",
            "Stanford",
            "Alpaca",
            "(",
            "zh",
            ")",
            "]",
            "(",
            "https",
            ":",
            "//github.com/ymcui/Chinese-LLaMA-Alpaca",
            ")",
            "-",
            "[",
            "GPT-4",
            "Generated",
            "Data",
            "(",
            "en",
            "&",
            "zh",
            ")",
            "]",
            "(",
            "https",
            ":",
            "//github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM",
            ")",
            "-",
            "[",
            "Self-cognition",
            "(",
            "zh",
            ")",
            "]",
            "(",
            "data/self_cognition.json",
            ")",
            "-",
            "[",
            "Open",
            "Assistant",
            "(",
            "multilingual",
            ")",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/datasets/OpenAssistant/oasst1",
            ")",
            "-",
            "[",
            "ShareGPT",
            "(",
            "zh",
            ")",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/Chinese-instruction-collection",
            ")",
            "-",
            "[",
            "Guanaco",
            "Dataset",
            "(",
            "multilingual",
            ")",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/datasets/JosephusCheung/GuanacoDataset",
            ")",
            "-",
            "[",
            "BELLE",
            "2M",
            "(",
            "zh",
            ")",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/datasets/BelleGroup/train_2M_CN",
            ")",
            "-",
            "[",
            "BELLE",
            "1M",
            "(",
            "zh",
            ")",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/datasets/BelleGroup/train_1M_CN",
            ")",
            "-",
            "[",
            "BELLE",
            "0.5M",
            "(",
            "zh",
            ")",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/datasets/BelleGroup/train_0.5M_CN",
            ")",
            "-",
            "[",
            "BELLE",
            "Dialogue",
            "0.4M",
            "(",
            "zh",
            ")",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/datasets/BelleGroup/generated_chat_0.4M",
            ")",
            "-",
            "[",
            "BELLE",
            "School",
            "Math",
            "0.25M",
            "(",
            "zh",
            ")",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/datasets/BelleGroup/school_math_0.25M",
            ")",
            "-",
            "[",
            "BELLE",
            "Multiturn",
            "Chat",
            "0.8M",
            "(",
            "zh",
            ")",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M",
            ")",
            "-",
            "[",
            "UltraChat",
            "(",
            "en",
            ")",
            "]",
            "(",
            "https",
            ":",
            "//github.com/thunlp/UltraChat",
            ")",
            "-",
            "[",
            "LIMA",
            "(",
            "en",
            ")",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/datasets/GAIR/lima",
            ")",
            "-",
            "[",
            "OpenPlatypus",
            "(",
            "en",
            ")",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/datasets/garage-bAInd/Open-Platypus",
            ")",
            "-",
            "[",
            "CodeAlpaca",
            "20k",
            "(",
            "en",
            ")",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/datasets/sahil2801/CodeAlpaca-20k",
            ")",
            "-",
            "[",
            "Alpaca",
            "CoT",
            "(",
            "multilingual",
            ")",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/datasets/QingyiSi/Alpaca-CoT",
            ")",
            "-",
            "[",
            "OpenOrca",
            "(",
            "en",
            ")",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/datasets/Open-Orca/OpenOrca",
            ")",
            "-",
            "[",
            "MathInstruct",
            "(",
            "en",
            ")",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/datasets/TIGER-Lab/MathInstruct",
            ")",
            "-",
            "[",
            "Firefly",
            "1.1M",
            "(",
            "zh",
            ")",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/datasets/YeungNLP/firefly-train-1.1M",
            ")",
            "-",
            "[",
            "Web",
            "QA",
            "(",
            "zh",
            ")",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/datasets/suolyer/webqa",
            ")",
            "-",
            "[",
            "WebNovel",
            "(",
            "zh",
            ")",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/datasets/zxbsmk/webnovel_cn",
            ")",
            "-",
            "[",
            "Nectar",
            "(",
            "en",
            ")",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/datasets/berkeley-nest/Nectar",
            ")",
            "-",
            "[",
            "deepctrl",
            "(",
            "en",
            "&",
            "zh",
            ")",
            "]",
            "(",
            "https",
            ":",
            "//www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data",
            ")",
            "-",
            "[",
            "Ad",
            "Gen",
            "(",
            "zh",
            ")",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/datasets/HasturOfficial/adgen",
            ")",
            "-",
            "[",
            "ShareGPT",
            "Hyperfiltered",
            "(",
            "en",
            ")",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/datasets/totally-not-an-llm/sharegpt-hyperfiltered-3k",
            ")",
            "-",
            "[",
            "ShareGPT4",
            "(",
            "en",
            "&",
            "zh",
            ")",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/datasets/shibing624/sharegpt_gpt4",
            ")",
            "-",
            "[",
            "UltraChat",
            "200k",
            "(",
            "en",
            ")",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/datasets/HuggingFaceH4/ultrachat_200k",
            ")",
            "-",
            "[",
            "AgentInstruct",
            "(",
            "en",
            ")",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/datasets/THUDM/AgentInstruct",
            ")",
            "-",
            "[",
            "LMSYS",
            "Chat",
            "1M",
            "(",
            "en",
            ")",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/datasets/lmsys/lmsys-chat-1m",
            ")",
            "-",
            "[",
            "Evol",
            "Instruct",
            "V2",
            "(",
            "en",
            ")",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_V2_196k",
            ")",
            "-",
            "[",
            "Glaive",
            "Function",
            "Calling",
            "V2",
            "(",
            "en",
            ")",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/datasets/glaiveai/glaive-function-calling-v2",
            ")",
            "<",
            "/details",
            ">",
            "<",
            "details",
            ">",
            "<",
            "summary",
            ">",
            "Preference",
            "datasets",
            "<",
            "/summary",
            ">",
            "-",
            "[",
            "HH-RLHF",
            "(",
            "en",
            ")",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/datasets/Anthropic/hh-rlhf",
            ")",
            "-",
            "[",
            "Open",
            "Assistant",
            "(",
            "multilingual",
            ")",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/datasets/OpenAssistant/oasst1",
            ")",
            "-",
            "[",
            "GPT-4",
            "Generated",
            "Data",
            "(",
            "en",
            "&",
            "zh",
            ")",
            "]",
            "(",
            "https",
            ":",
            "//github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM",
            ")",
            "-",
            "[",
            "Nectar",
            "(",
            "en",
            ")",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/datasets/berkeley-nest/Nectar",
            ")",
            "<",
            "/details",
            ">",
            "Please",
            "refer",
            "to",
            "[",
            "data/README.md",
            "]",
            "(",
            "data/README.md",
            ")",
            "for",
            "details",
            ".",
            "Some",
            "datasets",
            "require",
            "confirmation",
            "before",
            "using",
            "them",
            ",",
            "so",
            "we",
            "recommend",
            "logging",
            "in",
            "with",
            "your",
            "Hugging",
            "Face",
            "account",
            "using",
            "these",
            "commands",
            ".",
            "``",
            "`",
            "bash",
            "pip",
            "install",
            "--",
            "upgrade",
            "huggingface_hub",
            "huggingface-cli",
            "login",
            "``",
            "`"
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/hiyouga/LLaMA-Factory",
        "readme_url": "https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/README.md",
        "topic": [
            "agent",
            "baichuan",
            "chatglm",
            "fine-tuning",
            "generative-ai",
            "gpt",
            "instruction-tuning",
            "language-model",
            "large-language-models",
            "llama",
            "llm",
            "lora",
            "mistral",
            "mixture-of-experts",
            "peft",
            "qlora",
            "quantization",
            "qwen",
            "rlhf",
            "transformers"
        ],
        "text": "Getting Started\n\n",
        "token": [
            "Getting",
            "Started"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/hiyouga/LLaMA-Factory",
        "readme_url": "https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/README.md",
        "topic": [
            "agent",
            "baichuan",
            "chatglm",
            "fine-tuning",
            "generative-ai",
            "gpt",
            "instruction-tuning",
            "language-model",
            "large-language-models",
            "llama",
            "llm",
            "lora",
            "mistral",
            "mixture-of-experts",
            "peft",
            "qlora",
            "quantization",
            "qwen",
            "rlhf",
            "transformers"
        ],
        "text": "Dependence Installation (optional)\n\n```bash\ngit clone https://github.com/hiyouga/LLaMA-Factory.git\nconda create -n llama_factory python=3.10\nconda activate llama_factory\ncd LLaMA-Factory\npip install -r requirements.txt\n```\n\nIf you want to enable the quantized LoRA (QLoRA) on the Windows platform, you will be required to install a pre-built version of `bitsandbytes` library, which supports CUDA 11.1 to 12.1.\n\n```bash\npip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.39.1-py3-none-win_amd64.whl\n```\n\n",
        "token": [
            "Dependence",
            "Installation",
            "(",
            "optional",
            ")",
            "``",
            "`",
            "bash",
            "git",
            "clone",
            "https",
            ":",
            "//github.com/hiyouga/LLaMA-Factory.git",
            "conda",
            "create",
            "-n",
            "llama_factory",
            "python=3.10",
            "conda",
            "activate",
            "llama_factory",
            "cd",
            "LLaMA-Factory",
            "pip",
            "install",
            "-r",
            "requirements.txt",
            "``",
            "`",
            "If",
            "you",
            "want",
            "to",
            "enable",
            "the",
            "quantized",
            "LoRA",
            "(",
            "QLoRA",
            ")",
            "on",
            "the",
            "Windows",
            "platform",
            ",",
            "you",
            "will",
            "be",
            "required",
            "to",
            "install",
            "a",
            "pre-built",
            "version",
            "of",
            "`",
            "bitsandbytes",
            "`",
            "library",
            ",",
            "which",
            "supports",
            "CUDA",
            "11.1",
            "to",
            "12.1",
            ".",
            "``",
            "`",
            "bash",
            "pip",
            "install",
            "https",
            ":",
            "//github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.39.1-py3-none-win_amd64.whl",
            "``",
            "`"
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/PaddlePaddle/PaddleNLP",
        "readme_url": "https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/main/README.md",
        "topic": [
            "bert",
            "compression",
            "distributed-training",
            "document-intelligence",
            "embedding",
            "ernie",
            "information-extraction",
            "llama",
            "llm",
            "neural-search",
            "nlp",
            "paddlenlp",
            "pretrained-models",
            "question-answering",
            "search-engine",
            "semantic-analysis",
            "sentiment-analysis",
            "transformers",
            "uie"
        ],
        "text": "pip\u5b89\u88c5\n\n```shell\npip install --upgrade paddlenlp\n```\n\n\u6216\u8005\u53ef\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5\u6700\u65b0 develop \u5206\u652f\u4ee3\u7801\uff1a\n\n```shell\npip install --pre --upgrade paddlenlp -f https://www.paddlepaddle.org.cn/whl/paddlenlp.html\n```\n\n\u66f4\u591a\u5173\u4e8ePaddlePaddle\u548cPaddleNLP\u5b89\u88c5\u7684\u8be6\u7ec6\u6559\u7a0b\u8bf7\u67e5\u770b[Installation](./docs/get_started/installation.rst)\u3002\n\n",
        "token": [
            "pip\u5b89\u88c5",
            "``",
            "`",
            "shell",
            "pip",
            "install",
            "--",
            "upgrade",
            "paddlenlp",
            "``",
            "`",
            "\u6216\u8005\u53ef\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5\u6700\u65b0",
            "develop",
            "\u5206\u652f\u4ee3\u7801\uff1a",
            "``",
            "`",
            "shell",
            "pip",
            "install",
            "--",
            "pre",
            "--",
            "upgrade",
            "paddlenlp",
            "-f",
            "https",
            ":",
            "//www.paddlepaddle.org.cn/whl/paddlenlp.html",
            "``",
            "`",
            "\u66f4\u591a\u5173\u4e8ePaddlePaddle\u548cPaddleNLP\u5b89\u88c5\u7684\u8be6\u7ec6\u6559\u7a0b\u8bf7\u67e5\u770b",
            "[",
            "Installation",
            "]",
            "(",
            "./docs/get_started/installation.rst",
            ")",
            "\u3002"
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/eosphoros-ai/DB-GPT",
        "readme_url": "https://raw.githubusercontent.com/eosphoros-ai/DB-GPT/main/README.md",
        "topic": [
            "agents",
            "bgi",
            "database",
            "gpt",
            "gpt-4",
            "langchain",
            "llm",
            "private",
            "rag",
            "security",
            "vicuna"
        ],
        "text": "Contents\n- [Introduction](#introduction)\n- [Install](#install)\n- [Features](#features)\n- [Contribution](#contribution)\n- [Contact](#contact-information)\n\n",
        "token": [
            "Contents",
            "-",
            "[",
            "Introduction",
            "]",
            "(",
            "#",
            "introduction",
            ")",
            "-",
            "[",
            "Install",
            "]",
            "(",
            "#",
            "install",
            ")",
            "-",
            "[",
            "Features",
            "]",
            "(",
            "#",
            "features",
            ")",
            "-",
            "[",
            "Contribution",
            "]",
            "(",
            "#",
            "contribution",
            ")",
            "-",
            "[",
            "Contact",
            "]",
            "(",
            "#",
            "contact-information",
            ")"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/eosphoros-ai/DB-GPT",
        "readme_url": "https://raw.githubusercontent.com/eosphoros-ai/DB-GPT/main/README.md",
        "topic": [
            "agents",
            "bgi",
            "database",
            "gpt",
            "gpt-4",
            "langchain",
            "llm",
            "private",
            "rag",
            "security",
            "vicuna"
        ],
        "text": "Install \n![Docker](https://img.shields.io/badge/docker-%230db7ed.svg?style=for-the-badge&logo=docker&logoColor=white)\n![Linux](https://img.shields.io/badge/Linux-FCC624?style=for-the-badge&logo=linux&logoColor=black)\n![macOS](https://img.shields.io/badge/mac%20os-000000?style=for-the-badge&logo=macos&logoColor=F0F0F0)\n![Windows](https://img.shields.io/badge/Windows-0078D6?style=for-the-badge&logo=windows&logoColor=white)\n\n[**Usage Tutorial**](http://docs.dbgpt.site/docs/overview)\n- [**Install**](http://docs.dbgpt.site/docs/installation)\n- [**Quickstart**](http://docs.dbgpt.site/docs/quickstart)\n- [**Application**](http://docs.dbgpt.site/docs/operation_manual)\n- [**Debugging**](http://docs.dbgpt.site/docs/operation_manual/advanced_tutorial/debugging)\n\n\n",
        "token": [
            "Install",
            "!",
            "[",
            "Docker",
            "]",
            "(",
            "https",
            ":",
            "//img.shields.io/badge/docker-",
            "%",
            "230db7ed.svg",
            "?",
            "style=for-the-badge",
            "&",
            "logo=docker",
            "&",
            "logoColor=white",
            ")",
            "!",
            "[",
            "Linux",
            "]",
            "(",
            "https",
            ":",
            "//img.shields.io/badge/Linux-FCC624",
            "?",
            "style=for-the-badge",
            "&",
            "logo=linux",
            "&",
            "logoColor=black",
            ")",
            "!",
            "[",
            "macOS",
            "]",
            "(",
            "https",
            ":",
            "//img.shields.io/badge/mac",
            "%",
            "20os-000000",
            "?",
            "style=for-the-badge",
            "&",
            "logo=macos",
            "&",
            "logoColor=F0F0F0",
            ")",
            "!",
            "[",
            "Windows",
            "]",
            "(",
            "https",
            ":",
            "//img.shields.io/badge/Windows-0078D6",
            "?",
            "style=for-the-badge",
            "&",
            "logo=windows",
            "&",
            "logoColor=white",
            ")",
            "[",
            "*",
            "*",
            "Usage",
            "Tutorial",
            "*",
            "*",
            "]",
            "(",
            "http",
            ":",
            "//docs.dbgpt.site/docs/overview",
            ")",
            "-",
            "[",
            "*",
            "*",
            "Install",
            "*",
            "*",
            "]",
            "(",
            "http",
            ":",
            "//docs.dbgpt.site/docs/installation",
            ")",
            "-",
            "[",
            "*",
            "*",
            "Quickstart",
            "*",
            "*",
            "]",
            "(",
            "http",
            ":",
            "//docs.dbgpt.site/docs/quickstart",
            ")",
            "-",
            "[",
            "*",
            "*",
            "Application",
            "*",
            "*",
            "]",
            "(",
            "http",
            ":",
            "//docs.dbgpt.site/docs/operation_manual",
            ")",
            "-",
            "[",
            "*",
            "*",
            "Debugging",
            "*",
            "*",
            "]",
            "(",
            "http",
            ":",
            "//docs.dbgpt.site/docs/operation_manual/advanced_tutorial/debugging",
            ")"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/eosphoros-ai/DB-GPT",
        "readme_url": "https://raw.githubusercontent.com/eosphoros-ai/DB-GPT/main/README.md",
        "topic": [
            "agents",
            "bgi",
            "database",
            "gpt",
            "gpt-4",
            "langchain",
            "llm",
            "private",
            "rag",
            "security",
            "vicuna"
        ],
        "text": "Contribution\n\n- Please run `black .` before submitting the code.\n- To check detailed guidelines for new contributions, please refer [how to contribute](https://github.com/eosphoros-ai/DB-GPT/blob/main/CONTRIBUTING.md)\n\n",
        "token": [
            "Contribution",
            "-",
            "Please",
            "run",
            "`",
            "black",
            ".",
            "`",
            "before",
            "submitting",
            "the",
            "code",
            ".",
            "-",
            "To",
            "check",
            "detailed",
            "guidelines",
            "for",
            "new",
            "contributions",
            ",",
            "please",
            "refer",
            "[",
            "how",
            "to",
            "contribute",
            "]",
            "(",
            "https",
            ":",
            "//github.com/eosphoros-ai/DB-GPT/blob/main/CONTRIBUTING.md",
            ")"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/gventuri/pandas-ai",
        "readme_url": "https://raw.githubusercontent.com/gventuri/pandas-ai/main/README.md",
        "topic": [
            "ai",
            "csv",
            "data",
            "data-analysis",
            "data-science",
            "gpt-3",
            "gpt-4",
            "llm",
            "pandas",
            "sql"
        ],
        "text": "\ud83d\udd27 Quick install\n\n```bash\npip install pandasai\n```\n\n",
        "token": [
            "\ud83d\udd27",
            "Quick",
            "install",
            "``",
            "`",
            "bash",
            "pip",
            "install",
            "pandasai",
            "``",
            "`"
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/gventuri/pandas-ai",
        "readme_url": "https://raw.githubusercontent.com/gventuri/pandas-ai/main/README.md",
        "topic": [
            "ai",
            "csv",
            "data",
            "data-analysis",
            "data-science",
            "gpt-3",
            "gpt-4",
            "llm",
            "pandas",
            "sql"
        ],
        "text": "\ud83e\udd1d Contributing\n\nContributions are welcome! Please check out the todos below, and feel free to open a pull request.\nFor more information, please see the [contributing guidelines](CONTRIBUTING.md).\n\nAfter installing the virtual environment, please remember to install `pre-commit` to be compliant with our standards:\n\n```bash\npre-commit install\n```\n\n",
        "token": [
            "\ud83e\udd1d",
            "Contributing",
            "Contributions",
            "are",
            "welcome",
            "!",
            "Please",
            "check",
            "out",
            "the",
            "todos",
            "below",
            ",",
            "and",
            "feel",
            "free",
            "to",
            "open",
            "a",
            "pull",
            "request",
            ".",
            "For",
            "more",
            "information",
            ",",
            "please",
            "see",
            "the",
            "[",
            "contributing",
            "guidelines",
            "]",
            "(",
            "CONTRIBUTING.md",
            ")",
            ".",
            "After",
            "installing",
            "the",
            "virtual",
            "environment",
            ",",
            "please",
            "remember",
            "to",
            "install",
            "`",
            "pre-commit",
            "`",
            "to",
            "be",
            "compliant",
            "with",
            "our",
            "standards",
            ":",
            "``",
            "`",
            "bash",
            "pre-commit",
            "install",
            "``",
            "`"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/h2oai/h2ogpt",
        "readme_url": "https://raw.githubusercontent.com/h2oai/h2ogpt/main/README.md",
        "topic": [
            "ai",
            "chatgpt",
            "embeddings",
            "generative",
            "gpt",
            "gpt4all",
            "llama2",
            "llm",
            "mixtral",
            "pdf",
            "private",
            "privategpt",
            "vectorstore"
        ],
        "text": "h2oGPT\n\nTurn \u2605 into \u2b50 (top-right corner) if you like the project!\n\nQuery and summarize your documents or just chat with local private GPT LLMs using h2oGPT, an Apache V2 open-source project.\n\n- **Private** offline database of any documents [(PDFs, Excel, Word, Images, Video Frames, Youtube, Audio, Code, Text, MarkDown, etc.)](docs/README_LangChain.md#supported-datatypes)\n  - **Persistent** database (Chroma, Weaviate, or in-memory FAISS) using accurate embeddings (instructor-large, all-MiniLM-L6-v2, etc.)\n  - **Efficient** use of context using instruct-tuned LLMs (no need for LangChain's few-shot approach)\n  - **Parallel** summarization and extraction, reaching an output of 80 tokens per second with the 13B LLaMa2 model\n  - **HYDE** (Hypothetical Document Embeddings) for enhanced retrieval based upon LLM responses\n- **Variety** of models supported (LLaMa2, Mistral, Falcon, Vicuna, WizardLM.  With AutoGPTQ, 4-bit/8-bit, LORA, etc.)\n  - **GPU** support from HF and LLaMa.cpp GGML models, and **CPU** support using HF, LLaMa.cpp, and GPT4ALL models\n  - **Attention Sinks** for [arbitrarily long](https://github.com/tomaarsen/attention_sinks) generation (LLaMa-2, Mistral, MPT, Pythia, Falcon, etc.)\n- **UI** or CLI with streaming of all models\n  - **Upload** and **View** documents through the UI (control multiple collaborative or personal collections)\n  - **Vision LLaVa** Model and **Stable Diffusion** Image Generation\n  - **Voice STT** using Whisper with streaming audio conversion\n  - **Voice TTS** using MIT-Licensed Microsoft Speech T5 with multiple voices and Streaming audio conversion\n  - **Voice TTS** using MPL2-Licensed TTS including Voice Cloning and Streaming audio conversion\n  - **AI Assistant Voice Control Mode** for hands-free control of h2oGPT chat\n  - **Bake-off** UI mode against many models at the same time\n  - **Easy Download** of model artifacts and control over models like LLaMa.cpp through the UI\n  - **Authentication** in the UI by user/password\n  - **State Preservation** in the UI by user/password\n- **Linux, Docker, macOS, and Windows** support\n  - [**Easy Windows Installer**](#windows-1011-64-bit-with-full-document-qa-capability) for Windows 10 64-bit (CPU/CUDA)\n  - [**Easy macOS Installer**](#macos-cpum1m2-with-full-document-qa-capability) for macOS (CPU/M1/M2)\n- **Inference Servers** support (HF TGI server, vLLM, Gradio, ExLLaMa, Replicate, OpenAI, Azure OpenAI, Anthropic)\n- **OpenAI-compliant**\n  - Server Proxy API (h2oGPT acts as drop-in-replacement to OpenAI server)\n  - Python client API (to talk to Gradio server)\n- **Web-Search** integration with Chat and Document Q/A\n- **Agents** for Search, Document Q/A, Python Code, CSV frames (Experimental, best with OpenAI currently)\n- **Evaluate** performance using reward models\n- **Quality** maintained with over 1000 unit and integration tests taking over 4 GPU-hours\n\n",
        "token": [
            "h2oGPT",
            "Turn",
            "\u2605",
            "into",
            "\u2b50",
            "(",
            "top-right",
            "corner",
            ")",
            "if",
            "you",
            "like",
            "the",
            "project",
            "!",
            "Query",
            "and",
            "summarize",
            "your",
            "documents",
            "or",
            "just",
            "chat",
            "with",
            "local",
            "private",
            "GPT",
            "LLMs",
            "using",
            "h2oGPT",
            ",",
            "an",
            "Apache",
            "V2",
            "open-source",
            "project",
            ".",
            "-",
            "*",
            "*",
            "Private",
            "*",
            "*",
            "offline",
            "database",
            "of",
            "any",
            "documents",
            "[",
            "(",
            "PDFs",
            ",",
            "Excel",
            ",",
            "Word",
            ",",
            "Images",
            ",",
            "Video",
            "Frames",
            ",",
            "Youtube",
            ",",
            "Audio",
            ",",
            "Code",
            ",",
            "Text",
            ",",
            "MarkDown",
            ",",
            "etc",
            ".",
            ")",
            "]",
            "(",
            "docs/README_LangChain.md",
            "#",
            "supported-datatypes",
            ")",
            "-",
            "*",
            "*",
            "Persistent",
            "*",
            "*",
            "database",
            "(",
            "Chroma",
            ",",
            "Weaviate",
            ",",
            "or",
            "in-memory",
            "FAISS",
            ")",
            "using",
            "accurate",
            "embeddings",
            "(",
            "instructor-large",
            ",",
            "all-MiniLM-L6-v2",
            ",",
            "etc",
            ".",
            ")",
            "-",
            "*",
            "*",
            "Efficient",
            "*",
            "*",
            "use",
            "of",
            "context",
            "using",
            "instruct-tuned",
            "LLMs",
            "(",
            "no",
            "need",
            "for",
            "LangChain",
            "'s",
            "few-shot",
            "approach",
            ")",
            "-",
            "*",
            "*",
            "Parallel",
            "*",
            "*",
            "summarization",
            "and",
            "extraction",
            ",",
            "reaching",
            "an",
            "output",
            "of",
            "80",
            "tokens",
            "per",
            "second",
            "with",
            "the",
            "13B",
            "LLaMa2",
            "model",
            "-",
            "*",
            "*",
            "HYDE",
            "*",
            "*",
            "(",
            "Hypothetical",
            "Document",
            "Embeddings",
            ")",
            "for",
            "enhanced",
            "retrieval",
            "based",
            "upon",
            "LLM",
            "responses",
            "-",
            "*",
            "*",
            "Variety",
            "*",
            "*",
            "of",
            "models",
            "supported",
            "(",
            "LLaMa2",
            ",",
            "Mistral",
            ",",
            "Falcon",
            ",",
            "Vicuna",
            ",",
            "WizardLM",
            ".",
            "With",
            "AutoGPTQ",
            ",",
            "4-bit/8-bit",
            ",",
            "LORA",
            ",",
            "etc",
            ".",
            ")",
            "-",
            "*",
            "*",
            "GPU",
            "*",
            "*",
            "support",
            "from",
            "HF",
            "and",
            "LLaMa.cpp",
            "GGML",
            "models",
            ",",
            "and",
            "*",
            "*",
            "CPU",
            "*",
            "*",
            "support",
            "using",
            "HF",
            ",",
            "LLaMa.cpp",
            ",",
            "and",
            "GPT4ALL",
            "models",
            "-",
            "*",
            "*",
            "Attention",
            "Sinks",
            "*",
            "*",
            "for",
            "[",
            "arbitrarily",
            "long",
            "]",
            "(",
            "https",
            ":",
            "//github.com/tomaarsen/attention_sinks",
            ")",
            "generation",
            "(",
            "LLaMa-2",
            ",",
            "Mistral",
            ",",
            "MPT",
            ",",
            "Pythia",
            ",",
            "Falcon",
            ",",
            "etc",
            ".",
            ")",
            "-",
            "*",
            "*",
            "UI",
            "*",
            "*",
            "or",
            "CLI",
            "with",
            "streaming",
            "of",
            "all",
            "models",
            "-",
            "*",
            "*",
            "Upload",
            "*",
            "*",
            "and",
            "*",
            "*",
            "View",
            "*",
            "*",
            "documents",
            "through",
            "the",
            "UI",
            "(",
            "control",
            "multiple",
            "collaborative",
            "or",
            "personal",
            "collections",
            ")",
            "-",
            "*",
            "*",
            "Vision",
            "LLaVa",
            "*",
            "*",
            "Model",
            "and",
            "*",
            "*",
            "Stable",
            "Diffusion",
            "*",
            "*",
            "Image",
            "Generation",
            "-",
            "*",
            "*",
            "Voice",
            "STT",
            "*",
            "*",
            "using",
            "Whisper",
            "with",
            "streaming",
            "audio",
            "conversion",
            "-",
            "*",
            "*",
            "Voice",
            "TTS",
            "*",
            "*",
            "using",
            "MIT-Licensed",
            "Microsoft",
            "Speech",
            "T5",
            "with",
            "multiple",
            "voices",
            "and",
            "Streaming",
            "audio",
            "conversion",
            "-",
            "*",
            "*",
            "Voice",
            "TTS",
            "*",
            "*",
            "using",
            "MPL2-Licensed",
            "TTS",
            "including",
            "Voice",
            "Cloning",
            "and",
            "Streaming",
            "audio",
            "conversion",
            "-",
            "*",
            "*",
            "AI",
            "Assistant",
            "Voice",
            "Control",
            "Mode",
            "*",
            "*",
            "for",
            "hands-free",
            "control",
            "of",
            "h2oGPT",
            "chat",
            "-",
            "*",
            "*",
            "Bake-off",
            "*",
            "*",
            "UI",
            "mode",
            "against",
            "many",
            "models",
            "at",
            "the",
            "same",
            "time",
            "-",
            "*",
            "*",
            "Easy",
            "Download",
            "*",
            "*",
            "of",
            "model",
            "artifacts",
            "and",
            "control",
            "over",
            "models",
            "like",
            "LLaMa.cpp",
            "through",
            "the",
            "UI",
            "-",
            "*",
            "*",
            "Authentication",
            "*",
            "*",
            "in",
            "the",
            "UI",
            "by",
            "user/password",
            "-",
            "*",
            "*",
            "State",
            "Preservation",
            "*",
            "*",
            "in",
            "the",
            "UI",
            "by",
            "user/password",
            "-",
            "*",
            "*",
            "Linux",
            ",",
            "Docker",
            ",",
            "macOS",
            ",",
            "and",
            "Windows",
            "*",
            "*",
            "support",
            "-",
            "[",
            "*",
            "*",
            "Easy",
            "Windows",
            "Installer",
            "*",
            "*",
            "]",
            "(",
            "#",
            "windows-1011-64-bit-with-full-document-qa-capability",
            ")",
            "for",
            "Windows",
            "10",
            "64-bit",
            "(",
            "CPU/CUDA",
            ")",
            "-",
            "[",
            "*",
            "*",
            "Easy",
            "macOS",
            "Installer",
            "*",
            "*",
            "]",
            "(",
            "#",
            "macos-cpum1m2-with-full-document-qa-capability",
            ")",
            "for",
            "macOS",
            "(",
            "CPU/M1/M2",
            ")",
            "-",
            "*",
            "*",
            "Inference",
            "Servers",
            "*",
            "*",
            "support",
            "(",
            "HF",
            "TGI",
            "server",
            ",",
            "vLLM",
            ",",
            "Gradio",
            ",",
            "ExLLaMa",
            ",",
            "Replicate",
            ",",
            "OpenAI",
            ",",
            "Azure",
            "OpenAI",
            ",",
            "Anthropic",
            ")",
            "-",
            "*",
            "*",
            "OpenAI-compliant",
            "*",
            "*",
            "-",
            "Server",
            "Proxy",
            "API",
            "(",
            "h2oGPT",
            "acts",
            "as",
            "drop-in-replacement",
            "to",
            "OpenAI",
            "server",
            ")",
            "-",
            "Python",
            "client",
            "API",
            "(",
            "to",
            "talk",
            "to",
            "Gradio",
            "server",
            ")",
            "-",
            "*",
            "*",
            "Web-Search",
            "*",
            "*",
            "integration",
            "with",
            "Chat",
            "and",
            "Document",
            "Q/A",
            "-",
            "*",
            "*",
            "Agents",
            "*",
            "*",
            "for",
            "Search",
            ",",
            "Document",
            "Q/A",
            ",",
            "Python",
            "Code",
            ",",
            "CSV",
            "frames",
            "(",
            "Experimental",
            ",",
            "best",
            "with",
            "OpenAI",
            "currently",
            ")",
            "-",
            "*",
            "*",
            "Evaluate",
            "*",
            "*",
            "performance",
            "using",
            "reward",
            "models",
            "-",
            "*",
            "*",
            "Quality",
            "*",
            "*",
            "maintained",
            "with",
            "over",
            "1000",
            "unit",
            "and",
            "integration",
            "tests",
            "taking",
            "over",
            "4",
            "GPU-hours"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/h2oai/h2ogpt",
        "readme_url": "https://raw.githubusercontent.com/h2oai/h2ogpt/main/README.md",
        "topic": [
            "ai",
            "chatgpt",
            "embeddings",
            "generative",
            "gpt",
            "gpt4all",
            "llama2",
            "llm",
            "mixtral",
            "pdf",
            "private",
            "privategpt",
            "vectorstore"
        ],
        "text": "for windows/mac use \"set\" or relevant environment setting mechanism\n   export PIP_EXTRA_INDEX_URL=\"https://download.pytorch.org/whl/cu118\"\n   ```\nThen run the following commands on any system:\n   ```bash\n   git clone https://github.com/h2oai/h2ogpt.git\n   cd h2ogpt\n   pip install -r requirements.txt\n   pip install -r reqs_optional/requirements_optional_langchain.txt\n   pip install -r reqs_optional/requirements_optional_gpt4all.txt\n   pip install -r reqs_optional/requirements_optional_langchain.urls.txt\n   ",
        "token": [
            "for",
            "windows/mac",
            "use",
            "``",
            "set",
            "''",
            "or",
            "relevant",
            "environment",
            "setting",
            "mechanism",
            "export",
            "PIP_EXTRA_INDEX_URL=",
            "''",
            "https",
            ":",
            "//download.pytorch.org/whl/cu118",
            "''",
            "``",
            "`",
            "Then",
            "run",
            "the",
            "following",
            "commands",
            "on",
            "any",
            "system",
            ":",
            "``",
            "`",
            "bash",
            "git",
            "clone",
            "https",
            ":",
            "//github.com/h2oai/h2ogpt.git",
            "cd",
            "h2ogpt",
            "pip",
            "install",
            "-r",
            "requirements.txt",
            "pip",
            "install",
            "-r",
            "reqs_optional/requirements_optional_langchain.txt",
            "pip",
            "install",
            "-r",
            "reqs_optional/requirements_optional_gpt4all.txt",
            "pip",
            "install",
            "-r",
            "reqs_optional/requirements_optional_langchain.urls.txt"
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/h2oai/h2ogpt",
        "readme_url": "https://raw.githubusercontent.com/h2oai/h2ogpt/main/README.md",
        "topic": [
            "ai",
            "chatgpt",
            "embeddings",
            "generative",
            "gpt",
            "gpt4all",
            "llama2",
            "llm",
            "mixtral",
            "pdf",
            "private",
            "privategpt",
            "vectorstore"
        ],
        "text": "pip install -r reqs_optional/requirements_optional_langchain.gpllike.txt\n\n   python generate.py --base_model=TheBloke/zephyr-7B-beta-GGUF --prompt_type=zephyr --max_seq_len=4096\n   ```\nNext, go to your browser by visiting [http://127.0.0.1:7860](http://127.0.0.1:7860) or [http://localhost:7860](http://localhost:7860).  Choose 13B for a better model than 7B.\nIf you encounter issues with `llama-cpp-python` or other packages that try to compile and fail, try binary wheels for your platform as linked in the detailed instructions below.  For AVX1 or AMD ROC systems, edit `reqs_optional/requirements_optional_gpt4all.txt` to choose valid packages.\n\nWe recommend quantized models for most small-GPU systems, e.g. [LLaMa-2-7B-Chat-GGUF](https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF/resolve/main/llama-2-7b-chat.Q6_K.gguf) for 9GB+ GPU memory or larger models like [LLaMa-2-13B-Chat-GGUF](https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF/resolve/main/llama-2-13b-chat.Q6_K.gguf) if you have 16GB+ GPU memory.\n\n---\n\nNote that for all platforms, some packages such as DocTR, Unstructured, BLIP, Stable Diffusion, etc. download models at runtime that appear to delay operations in the UI. The progress appears in the console logs.\n\n",
        "token": [
            "pip",
            "install",
            "-r",
            "reqs_optional/requirements_optional_langchain.gpllike.txt",
            "python",
            "generate.py",
            "--",
            "base_model=TheBloke/zephyr-7B-beta-GGUF",
            "--",
            "prompt_type=zephyr",
            "--",
            "max_seq_len=4096",
            "``",
            "`",
            "Next",
            ",",
            "go",
            "to",
            "your",
            "browser",
            "by",
            "visiting",
            "[",
            "http",
            ":",
            "//127.0.0.1:7860",
            "]",
            "(",
            "http",
            ":",
            "//127.0.0.1:7860",
            ")",
            "or",
            "[",
            "http",
            ":",
            "//localhost:7860",
            "]",
            "(",
            "http",
            ":",
            "//localhost:7860",
            ")",
            ".",
            "Choose",
            "13B",
            "for",
            "a",
            "better",
            "model",
            "than",
            "7B",
            ".",
            "If",
            "you",
            "encounter",
            "issues",
            "with",
            "`",
            "llama-cpp-python",
            "`",
            "or",
            "other",
            "packages",
            "that",
            "try",
            "to",
            "compile",
            "and",
            "fail",
            ",",
            "try",
            "binary",
            "wheels",
            "for",
            "your",
            "platform",
            "as",
            "linked",
            "in",
            "the",
            "detailed",
            "instructions",
            "below",
            ".",
            "For",
            "AVX1",
            "or",
            "AMD",
            "ROC",
            "systems",
            ",",
            "edit",
            "`",
            "reqs_optional/requirements_optional_gpt4all.txt",
            "`",
            "to",
            "choose",
            "valid",
            "packages",
            ".",
            "We",
            "recommend",
            "quantized",
            "models",
            "for",
            "most",
            "small-GPU",
            "systems",
            ",",
            "e.g",
            ".",
            "[",
            "LLaMa-2-7B-Chat-GGUF",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF/resolve/main/llama-2-7b-chat.Q6_K.gguf",
            ")",
            "for",
            "9GB+",
            "GPU",
            "memory",
            "or",
            "larger",
            "models",
            "like",
            "[",
            "LLaMa-2-13B-Chat-GGUF",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF/resolve/main/llama-2-13b-chat.Q6_K.gguf",
            ")",
            "if",
            "you",
            "have",
            "16GB+",
            "GPU",
            "memory",
            ".",
            "--",
            "-",
            "Note",
            "that",
            "for",
            "all",
            "platforms",
            ",",
            "some",
            "packages",
            "such",
            "as",
            "DocTR",
            ",",
            "Unstructured",
            ",",
            "BLIP",
            ",",
            "Stable",
            "Diffusion",
            ",",
            "etc",
            ".",
            "download",
            "models",
            "at",
            "runtime",
            "that",
            "appear",
            "to",
            "delay",
            "operations",
            "in",
            "the",
            "UI",
            ".",
            "The",
            "progress",
            "appears",
            "in",
            "the",
            "console",
            "logs",
            "."
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/h2oai/h2ogpt",
        "readme_url": "https://raw.githubusercontent.com/h2oai/h2ogpt/main/README.md",
        "topic": [
            "ai",
            "chatgpt",
            "embeddings",
            "generative",
            "gpt",
            "gpt4all",
            "llama2",
            "llm",
            "mixtral",
            "pdf",
            "private",
            "privategpt",
            "vectorstore"
        ],
        "text": "Windows 10/11 64-bit with full document Q/A capability\n  * One-Click Installer\n    * CPU or GPU: Download [h2oGPT Windows Installer](https://h2o-release.s3.amazonaws.com/h2ogpt/Jan2024/h2oGPT_0.0.1.exe) (1.3GB file)\n      * Once installed, feel free to change start directory for icon from `%HOMEDRIVE%\\%HOMEPATH%` to (e.g.) `%HOMEDRIVE%\\%HOMEPATH%\\h2ogpt_data` so all created files (like database) go there.  All paths saved are relative to this path.\n    * CPU: Click the h2oGPT icon in the Start menu.  Give it about 15 seconds to open in a browser if many optional packages are included.  By default, the browser will launch with the actual local IP address, not localhost.\n    * GPU: Before starting, run the following commands (replace `pseud` with your user):\n      ```\n      C:\\Users\\pseud\\AppData\\Local\\Programs\\h2oGPT\\Python\\python.exe -m pip uninstall -y torch\n      C:\\Users\\pseud\\AppData\\Local\\Programs\\h2oGPT\\Python\\python.exe -m pip install https://h2o-release.s3.amazonaws.com/h2ogpt/torch-2.1.2%2Bcu118-cp310-cp310-win_amd64.whl\n      ```\n      Now click the h2oGPT icon in the Start menu.  Give it about 20 seconds to open in a browser if many optional packages are included.  By default, the browser will launch with the actual local IP address, not localhost.\n    * To debug any issues, run the following (replace `pseud` with your user):\n      ```\n      C:\\Users\\pseud\\AppData\\Local\\Programs\\h2oGPT\\Python\\python.exe \"C:\\Users\\pseud\\AppData\\Local\\Programs\\h2oGPT\\h2oGPT.launch.pyw\"\n      ```\n      Any start-up exceptions are appended to log, e.g. `C:\\Users\\pseud\\h2ogpt_exception.log`.\n  * To control startup, tweak the python startup file, e.g. for user `pseud`: `C:\\Users\\pseud\\AppData\\Local\\Programs\\h2oGPT\\pkgs\\win_run_app.py`\n    * In this Python code, set ENVs anywhere before main_h2ogpt() is called\n      * E.g. `os.environ['name'] = 'value'`, e.g. `os.environ['n_jobs'] = '10'` (must be always a string).\n    * Environment variables can be changed, e.g.:\n      * `n_jobs`: number of cores for various tasks\n      * `OMP_NUM_THREADS` thread count for LLaMa\n      * `CUDA_VISIBLE_DEVICES` which GPUs are used.  Recommend set to single fast GPU, e.g. `CUDA_VISIBLE_DEVICES=0` if have multiple GPUs.  Note that UI cannot control which GPUs (or CPU mode) for LLaMa models.\n      * Any CLI argument from `python generate.py --help` with environment variable set as `h2ogpt_x`, e.g. `h2ogpt_h2ocolors` to `False`.\n      * Set env `h2ogpt_server_name` to actual IP address for LAN to see app, e.g. `h2ogpt_server_name` to `192.168.1.172` and allow access through firewall if have Windows Defender activated.\n  * One can tweak installed h2oGPT code at, e.g. `C:\\Users\\pseud\\AppData\\Local\\Programs\\h2oGPT`.\n  * To terminate the app, go to System Tab and click Admin and click Shutdown h2oGPT.\n    * If startup fails, run as console and check for errors, e.g. and kill any old Python processes.\n\n  * [Full Windows 10/11 Manual Installation Script](docs/README_WINDOWS.md)\n    * Single `.bat` file for installation (if you do not skip any optional packages, takes about 9GB filled on disk).\n    * Recommend base Conda env, which allows for DocTR that requires pygobject that has otherwise no support (except `mysys2` that cannot be used by h2oGPT).\n    * Also allows for the TTS package by Coqui, which is otherwise not currently enabled in the one-click installer.\n\n---\n\n",
        "token": [
            "Windows",
            "10/11",
            "64-bit",
            "with",
            "full",
            "document",
            "Q/A",
            "capability",
            "*",
            "One-Click",
            "Installer",
            "*",
            "CPU",
            "or",
            "GPU",
            ":",
            "Download",
            "[",
            "h2oGPT",
            "Windows",
            "Installer",
            "]",
            "(",
            "https",
            ":",
            "//h2o-release.s3.amazonaws.com/h2ogpt/Jan2024/h2oGPT_0.0.1.exe",
            ")",
            "(",
            "1.3GB",
            "file",
            ")",
            "*",
            "Once",
            "installed",
            ",",
            "feel",
            "free",
            "to",
            "change",
            "start",
            "directory",
            "for",
            "icon",
            "from",
            "`",
            "%",
            "HOMEDRIVE",
            "%",
            "\\",
            "%",
            "HOMEPATH",
            "%",
            "`",
            "to",
            "(",
            "e.g",
            ".",
            ")",
            "`",
            "%",
            "HOMEDRIVE",
            "%",
            "\\",
            "%",
            "HOMEPATH",
            "%",
            "\\h2ogpt_data",
            "`",
            "so",
            "all",
            "created",
            "files",
            "(",
            "like",
            "database",
            ")",
            "go",
            "there",
            ".",
            "All",
            "paths",
            "saved",
            "are",
            "relative",
            "to",
            "this",
            "path",
            ".",
            "*",
            "CPU",
            ":",
            "Click",
            "the",
            "h2oGPT",
            "icon",
            "in",
            "the",
            "Start",
            "menu",
            ".",
            "Give",
            "it",
            "about",
            "15",
            "seconds",
            "to",
            "open",
            "in",
            "a",
            "browser",
            "if",
            "many",
            "optional",
            "packages",
            "are",
            "included",
            ".",
            "By",
            "default",
            ",",
            "the",
            "browser",
            "will",
            "launch",
            "with",
            "the",
            "actual",
            "local",
            "IP",
            "address",
            ",",
            "not",
            "localhost",
            ".",
            "*",
            "GPU",
            ":",
            "Before",
            "starting",
            ",",
            "run",
            "the",
            "following",
            "commands",
            "(",
            "replace",
            "`",
            "pseud",
            "`",
            "with",
            "your",
            "user",
            ")",
            ":",
            "``",
            "`",
            "C",
            ":",
            "\\Users\\pseud\\AppData\\Local\\Programs\\h2oGPT\\Python\\python.exe",
            "-m",
            "pip",
            "uninstall",
            "-y",
            "torch",
            "C",
            ":",
            "\\Users\\pseud\\AppData\\Local\\Programs\\h2oGPT\\Python\\python.exe",
            "-m",
            "pip",
            "install",
            "https",
            ":",
            "//h2o-release.s3.amazonaws.com/h2ogpt/torch-2.1.2",
            "%",
            "2Bcu118-cp310-cp310-win_amd64.whl",
            "``",
            "`",
            "Now",
            "click",
            "the",
            "h2oGPT",
            "icon",
            "in",
            "the",
            "Start",
            "menu",
            ".",
            "Give",
            "it",
            "about",
            "20",
            "seconds",
            "to",
            "open",
            "in",
            "a",
            "browser",
            "if",
            "many",
            "optional",
            "packages",
            "are",
            "included",
            ".",
            "By",
            "default",
            ",",
            "the",
            "browser",
            "will",
            "launch",
            "with",
            "the",
            "actual",
            "local",
            "IP",
            "address",
            ",",
            "not",
            "localhost",
            ".",
            "*",
            "To",
            "debug",
            "any",
            "issues",
            ",",
            "run",
            "the",
            "following",
            "(",
            "replace",
            "`",
            "pseud",
            "`",
            "with",
            "your",
            "user",
            ")",
            ":",
            "``",
            "`",
            "C",
            ":",
            "\\Users\\pseud\\AppData\\Local\\Programs\\h2oGPT\\Python\\python.exe",
            "``",
            "C",
            ":",
            "\\Users\\pseud\\AppData\\Local\\Programs\\h2oGPT\\h2oGPT.launch.pyw",
            "''",
            "``",
            "`",
            "Any",
            "start-up",
            "exceptions",
            "are",
            "appended",
            "to",
            "log",
            ",",
            "e.g",
            ".",
            "`",
            "C",
            ":",
            "\\Users\\pseud\\h2ogpt_exception.log",
            "`",
            ".",
            "*",
            "To",
            "control",
            "startup",
            ",",
            "tweak",
            "the",
            "python",
            "startup",
            "file",
            ",",
            "e.g",
            ".",
            "for",
            "user",
            "`",
            "pseud",
            "`",
            ":",
            "`",
            "C",
            ":",
            "\\Users\\pseud\\AppData\\Local\\Programs\\h2oGPT\\pkgs\\win_run_app.py",
            "`",
            "*",
            "In",
            "this",
            "Python",
            "code",
            ",",
            "set",
            "ENVs",
            "anywhere",
            "before",
            "main_h2ogpt",
            "(",
            ")",
            "is",
            "called",
            "*",
            "E.g",
            ".",
            "`",
            "os.environ",
            "[",
            "'name",
            "'",
            "]",
            "=",
            "'value",
            "'",
            "`",
            ",",
            "e.g",
            ".",
            "`",
            "os.environ",
            "[",
            "'n_jobs",
            "'",
            "]",
            "=",
            "'10",
            "'",
            "`",
            "(",
            "must",
            "be",
            "always",
            "a",
            "string",
            ")",
            ".",
            "*",
            "Environment",
            "variables",
            "can",
            "be",
            "changed",
            ",",
            "e.g",
            ".",
            ":",
            "*",
            "`",
            "n_jobs",
            "`",
            ":",
            "number",
            "of",
            "cores",
            "for",
            "various",
            "tasks",
            "*",
            "`",
            "OMP_NUM_THREADS",
            "`",
            "thread",
            "count",
            "for",
            "LLaMa",
            "*",
            "`",
            "CUDA_VISIBLE_DEVICES",
            "`",
            "which",
            "GPUs",
            "are",
            "used",
            ".",
            "Recommend",
            "set",
            "to",
            "single",
            "fast",
            "GPU",
            ",",
            "e.g",
            ".",
            "`",
            "CUDA_VISIBLE_DEVICES=0",
            "`",
            "if",
            "have",
            "multiple",
            "GPUs",
            ".",
            "Note",
            "that",
            "UI",
            "can",
            "not",
            "control",
            "which",
            "GPUs",
            "(",
            "or",
            "CPU",
            "mode",
            ")",
            "for",
            "LLaMa",
            "models",
            ".",
            "*",
            "Any",
            "CLI",
            "argument",
            "from",
            "`",
            "python",
            "generate.py",
            "--",
            "help",
            "`",
            "with",
            "environment",
            "variable",
            "set",
            "as",
            "`",
            "h2ogpt_x",
            "`",
            ",",
            "e.g",
            ".",
            "`",
            "h2ogpt_h2ocolors",
            "`",
            "to",
            "`",
            "False",
            "`",
            ".",
            "*",
            "Set",
            "env",
            "`",
            "h2ogpt_server_name",
            "`",
            "to",
            "actual",
            "IP",
            "address",
            "for",
            "LAN",
            "to",
            "see",
            "app",
            ",",
            "e.g",
            ".",
            "`",
            "h2ogpt_server_name",
            "`",
            "to",
            "`",
            "192.168.1.172",
            "`",
            "and",
            "allow",
            "access",
            "through",
            "firewall",
            "if",
            "have",
            "Windows",
            "Defender",
            "activated",
            ".",
            "*",
            "One",
            "can",
            "tweak",
            "installed",
            "h2oGPT",
            "code",
            "at",
            ",",
            "e.g",
            ".",
            "`",
            "C",
            ":",
            "\\Users\\pseud\\AppData\\Local\\Programs\\h2oGPT",
            "`",
            ".",
            "*",
            "To",
            "terminate",
            "the",
            "app",
            ",",
            "go",
            "to",
            "System",
            "Tab",
            "and",
            "click",
            "Admin",
            "and",
            "click",
            "Shutdown",
            "h2oGPT",
            ".",
            "*",
            "If",
            "startup",
            "fails",
            ",",
            "run",
            "as",
            "console",
            "and",
            "check",
            "for",
            "errors",
            ",",
            "e.g",
            ".",
            "and",
            "kill",
            "any",
            "old",
            "Python",
            "processes",
            ".",
            "*",
            "[",
            "Full",
            "Windows",
            "10/11",
            "Manual",
            "Installation",
            "Script",
            "]",
            "(",
            "docs/README_WINDOWS.md",
            ")",
            "*",
            "Single",
            "`",
            ".bat",
            "`",
            "file",
            "for",
            "installation",
            "(",
            "if",
            "you",
            "do",
            "not",
            "skip",
            "any",
            "optional",
            "packages",
            ",",
            "takes",
            "about",
            "9GB",
            "filled",
            "on",
            "disk",
            ")",
            ".",
            "*",
            "Recommend",
            "base",
            "Conda",
            "env",
            ",",
            "which",
            "allows",
            "for",
            "DocTR",
            "that",
            "requires",
            "pygobject",
            "that",
            "has",
            "otherwise",
            "no",
            "support",
            "(",
            "except",
            "`",
            "mysys2",
            "`",
            "that",
            "can",
            "not",
            "be",
            "used",
            "by",
            "h2oGPT",
            ")",
            ".",
            "*",
            "Also",
            "allows",
            "for",
            "the",
            "TTS",
            "package",
            "by",
            "Coqui",
            ",",
            "which",
            "is",
            "otherwise",
            "not",
            "currently",
            "enabled",
            "in",
            "the",
            "one-click",
            "installer",
            ".",
            "--",
            "-"
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/h2oai/h2ogpt",
        "readme_url": "https://raw.githubusercontent.com/h2oai/h2ogpt/main/README.md",
        "topic": [
            "ai",
            "chatgpt",
            "embeddings",
            "generative",
            "gpt",
            "gpt4all",
            "llama2",
            "llm",
            "mixtral",
            "pdf",
            "private",
            "privategpt",
            "vectorstore"
        ],
        "text": "Linux (CPU/CUDA) with full document Q/A capability\n  * [Docker Build and Run Docs](docs/README_DOCKER.md)\n  * [Linux Manual Install and Run Docs](docs/README_LINUX.md)\n\n---\n\n",
        "token": [
            "Linux",
            "(",
            "CPU/CUDA",
            ")",
            "with",
            "full",
            "document",
            "Q/A",
            "capability",
            "*",
            "[",
            "Docker",
            "Build",
            "and",
            "Run",
            "Docs",
            "]",
            "(",
            "docs/README_DOCKER.md",
            ")",
            "*",
            "[",
            "Linux",
            "Manual",
            "Install",
            "and",
            "Run",
            "Docs",
            "]",
            "(",
            "docs/README_LINUX.md",
            ")",
            "--",
            "-"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/h2oai/h2ogpt",
        "readme_url": "https://raw.githubusercontent.com/h2oai/h2ogpt/main/README.md",
        "topic": [
            "ai",
            "chatgpt",
            "embeddings",
            "generative",
            "gpt",
            "gpt4all",
            "llama2",
            "llm",
            "mixtral",
            "pdf",
            "private",
            "privategpt",
            "vectorstore"
        ],
        "text": "macOS (CPU/M1/M2) with full document Q/A capability\n* One-click Installers (Experimental and subject to changes)\n\n  Nov 08, 2023\n  - [h2ogpt-osx-m1-cpu](https://h2o-release.s3.amazonaws.com/h2ogpt/Nov2023/h2ogpt-osx-m1-cpu)\n  - [h2ogpt-osx-m1-gpu](https://h2o-release.s3.amazonaws.com/h2ogpt/Nov2023/h2ogpt-osx-m1-gpu)\n  \n  Download the runnable file and open it from the Finder. It will take a few minutes to unpack and run the application.\n  These one-click installers are experimental. Report any issues with steps to reproduce at https://github.com/h2oai/h2ogpt/issues.\n\n  **Note:** The app bundle is unsigned. If you experience any issues with running the app, run the following commands:\n  ```bash\n  $ xattr -dr com.apple.quarantine {file-path}/h2ogpt-osx-m1-gpu\n  $ chmod +x {file-path}/h2ogpt-osx-m1-gpu\n  ```\n* [macOS Manual Install and Run Docs](docs/README_MACOS.md)\n\n---\n\n",
        "token": [
            "macOS",
            "(",
            "CPU/M1/M2",
            ")",
            "with",
            "full",
            "document",
            "Q/A",
            "capability",
            "*",
            "One-click",
            "Installers",
            "(",
            "Experimental",
            "and",
            "subject",
            "to",
            "changes",
            ")",
            "Nov",
            "08",
            ",",
            "2023",
            "-",
            "[",
            "h2ogpt-osx-m1-cpu",
            "]",
            "(",
            "https",
            ":",
            "//h2o-release.s3.amazonaws.com/h2ogpt/Nov2023/h2ogpt-osx-m1-cpu",
            ")",
            "-",
            "[",
            "h2ogpt-osx-m1-gpu",
            "]",
            "(",
            "https",
            ":",
            "//h2o-release.s3.amazonaws.com/h2ogpt/Nov2023/h2ogpt-osx-m1-gpu",
            ")",
            "Download",
            "the",
            "runnable",
            "file",
            "and",
            "open",
            "it",
            "from",
            "the",
            "Finder",
            ".",
            "It",
            "will",
            "take",
            "a",
            "few",
            "minutes",
            "to",
            "unpack",
            "and",
            "run",
            "the",
            "application",
            ".",
            "These",
            "one-click",
            "installers",
            "are",
            "experimental",
            ".",
            "Report",
            "any",
            "issues",
            "with",
            "steps",
            "to",
            "reproduce",
            "at",
            "https",
            ":",
            "//github.com/h2oai/h2ogpt/issues",
            ".",
            "*",
            "*",
            "Note",
            ":",
            "*",
            "*",
            "The",
            "app",
            "bundle",
            "is",
            "unsigned",
            ".",
            "If",
            "you",
            "experience",
            "any",
            "issues",
            "with",
            "running",
            "the",
            "app",
            ",",
            "run",
            "the",
            "following",
            "commands",
            ":",
            "``",
            "`",
            "bash",
            "$",
            "xattr",
            "-dr",
            "com.apple.quarantine",
            "{",
            "file-path",
            "}",
            "/h2ogpt-osx-m1-gpu",
            "$",
            "chmod",
            "+x",
            "{",
            "file-path",
            "}",
            "/h2ogpt-osx-m1-gpu",
            "``",
            "`",
            "*",
            "[",
            "macOS",
            "Manual",
            "Install",
            "and",
            "Run",
            "Docs",
            "]",
            "(",
            "docs/README_MACOS.md",
            ")",
            "--",
            "-"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/h2oai/h2ogpt",
        "readme_url": "https://raw.githubusercontent.com/h2oai/h2ogpt/main/README.md",
        "topic": [
            "ai",
            "chatgpt",
            "embeddings",
            "generative",
            "gpt",
            "gpt4all",
            "llama2",
            "llm",
            "mixtral",
            "pdf",
            "private",
            "privategpt",
            "vectorstore"
        ],
        "text": "Docs Guide\n<!--  cat README.md | ./gh-md-toc  -  But Help is heavily processed -->\n* [Get Started](#get-started)\n   * [Linux (CPU or CUDA)](docs/README_LINUX.md)\n   * [macOS (CPU or M1/M2)](docs/README_MACOS.md)\n   * [Windows 10/11 (CPU or CUDA)](docs/README_WINDOWS.md)\n   * [GPU (CUDA, AutoGPTQ, exllama) Running Details](docs/README_GPU.md)\n   * [CPU Running Details](docs/README_CPU.md)\n   * [CLI chat](docs/README_CLI.md)\n   * [Gradio UI](docs/README_ui.md)\n   * [Client API (Gradio, OpenAI-Compliant)](docs/README_CLIENT.md)\n   * [Inference Servers (HF TGI server, vLLM, Gradio, ExLLaMa, Replicate, OpenAI, Azure OpenAI)](docs/README_InferenceServers.md)\n   * [Python Wheel](docs/README_WHEEL.md)\n   * [Offline Installation](docs/README_offline.md)\n   * [Low Memory](docs/FAQ.md#low-memory-mode)\n   * [Docker](docs/README_DOCKER.md)\n* [LangChain Document Support](docs/README_LangChain.md)\n* [Compare to PrivateGPT et al.](docs/README_LangChain.md#what-is-h2ogpts-langchain-integration-like)\n* [Roadmap](#roadmap)\n* [Development](#development)\n* [Help](#help)\n   * [LangChain file types supported](docs/README_LangChain.md#supported-datatypes)\n   * [CLI Database control](docs/README_LangChain.md#database-creation)\n   * [FAQ](docs/FAQ.md)\n     * [Model Usage Notes](docs/FAQ.md#model-usage-notes)\n     * [Adding LLM Models (including using GGUF and Attention Sinks)](docs/FAQ.md#adding-models)\n     * [Adding Embedding Models](docs/FAQ.md#add-new-embedding-model)\n     * [Adding Prompts](docs/FAQ.md#adding-prompt-templates)\n     * [In-Context Learning](docs/FAQ.md#in-context-learning-via-prompt-engineering)\n     * [Multiple GPUs](docs/FAQ.md#multiple-gpus)\n     * [Low-Memory Usage](docs/FAQ.md#low-memory-mode)\n     * [Environment Variables](docs/FAQ.md#what-envs-can-i-pass-to-control-h2ogpt)\n     * [HTTPS access for server and client](docs/FAQ.md#https-access-for-server-and-client)\n   * [Useful Links](docs/LINKS.md)\n   * [Fine-Tuning](docs/FINETUNE.md)\n   * [Triton](docs/TRITON.md)\n   * [Commercial viability](docs/FAQ.md#commercial-viability)\n* [Acknowledgements](#acknowledgements)\n* [Why H2O.ai?](#why-h2oai)\n* [Disclaimer](#disclaimer)\n\n",
        "token": [
            "Docs",
            "Guide",
            "<",
            "!",
            "--",
            "cat",
            "README.md",
            "|",
            "./gh-md-toc",
            "-",
            "But",
            "Help",
            "is",
            "heavily",
            "processed",
            "--",
            ">",
            "*",
            "[",
            "Get",
            "Started",
            "]",
            "(",
            "#",
            "get-started",
            ")",
            "*",
            "[",
            "Linux",
            "(",
            "CPU",
            "or",
            "CUDA",
            ")",
            "]",
            "(",
            "docs/README_LINUX.md",
            ")",
            "*",
            "[",
            "macOS",
            "(",
            "CPU",
            "or",
            "M1/M2",
            ")",
            "]",
            "(",
            "docs/README_MACOS.md",
            ")",
            "*",
            "[",
            "Windows",
            "10/11",
            "(",
            "CPU",
            "or",
            "CUDA",
            ")",
            "]",
            "(",
            "docs/README_WINDOWS.md",
            ")",
            "*",
            "[",
            "GPU",
            "(",
            "CUDA",
            ",",
            "AutoGPTQ",
            ",",
            "exllama",
            ")",
            "Running",
            "Details",
            "]",
            "(",
            "docs/README_GPU.md",
            ")",
            "*",
            "[",
            "CPU",
            "Running",
            "Details",
            "]",
            "(",
            "docs/README_CPU.md",
            ")",
            "*",
            "[",
            "CLI",
            "chat",
            "]",
            "(",
            "docs/README_CLI.md",
            ")",
            "*",
            "[",
            "Gradio",
            "UI",
            "]",
            "(",
            "docs/README_ui.md",
            ")",
            "*",
            "[",
            "Client",
            "API",
            "(",
            "Gradio",
            ",",
            "OpenAI-Compliant",
            ")",
            "]",
            "(",
            "docs/README_CLIENT.md",
            ")",
            "*",
            "[",
            "Inference",
            "Servers",
            "(",
            "HF",
            "TGI",
            "server",
            ",",
            "vLLM",
            ",",
            "Gradio",
            ",",
            "ExLLaMa",
            ",",
            "Replicate",
            ",",
            "OpenAI",
            ",",
            "Azure",
            "OpenAI",
            ")",
            "]",
            "(",
            "docs/README_InferenceServers.md",
            ")",
            "*",
            "[",
            "Python",
            "Wheel",
            "]",
            "(",
            "docs/README_WHEEL.md",
            ")",
            "*",
            "[",
            "Offline",
            "Installation",
            "]",
            "(",
            "docs/README_offline.md",
            ")",
            "*",
            "[",
            "Low",
            "Memory",
            "]",
            "(",
            "docs/FAQ.md",
            "#",
            "low-memory-mode",
            ")",
            "*",
            "[",
            "Docker",
            "]",
            "(",
            "docs/README_DOCKER.md",
            ")",
            "*",
            "[",
            "LangChain",
            "Document",
            "Support",
            "]",
            "(",
            "docs/README_LangChain.md",
            ")",
            "*",
            "[",
            "Compare",
            "to",
            "PrivateGPT",
            "et",
            "al",
            ".",
            "]",
            "(",
            "docs/README_LangChain.md",
            "#",
            "what-is-h2ogpts-langchain-integration-like",
            ")",
            "*",
            "[",
            "Roadmap",
            "]",
            "(",
            "#",
            "roadmap",
            ")",
            "*",
            "[",
            "Development",
            "]",
            "(",
            "#",
            "development",
            ")",
            "*",
            "[",
            "Help",
            "]",
            "(",
            "#",
            "help",
            ")",
            "*",
            "[",
            "LangChain",
            "file",
            "types",
            "supported",
            "]",
            "(",
            "docs/README_LangChain.md",
            "#",
            "supported-datatypes",
            ")",
            "*",
            "[",
            "CLI",
            "Database",
            "control",
            "]",
            "(",
            "docs/README_LangChain.md",
            "#",
            "database-creation",
            ")",
            "*",
            "[",
            "FAQ",
            "]",
            "(",
            "docs/FAQ.md",
            ")",
            "*",
            "[",
            "Model",
            "Usage",
            "Notes",
            "]",
            "(",
            "docs/FAQ.md",
            "#",
            "model-usage-notes",
            ")",
            "*",
            "[",
            "Adding",
            "LLM",
            "Models",
            "(",
            "including",
            "using",
            "GGUF",
            "and",
            "Attention",
            "Sinks",
            ")",
            "]",
            "(",
            "docs/FAQ.md",
            "#",
            "adding-models",
            ")",
            "*",
            "[",
            "Adding",
            "Embedding",
            "Models",
            "]",
            "(",
            "docs/FAQ.md",
            "#",
            "add-new-embedding-model",
            ")",
            "*",
            "[",
            "Adding",
            "Prompts",
            "]",
            "(",
            "docs/FAQ.md",
            "#",
            "adding-prompt-templates",
            ")",
            "*",
            "[",
            "In-Context",
            "Learning",
            "]",
            "(",
            "docs/FAQ.md",
            "#",
            "in-context-learning-via-prompt-engineering",
            ")",
            "*",
            "[",
            "Multiple",
            "GPUs",
            "]",
            "(",
            "docs/FAQ.md",
            "#",
            "multiple-gpus",
            ")",
            "*",
            "[",
            "Low-Memory",
            "Usage",
            "]",
            "(",
            "docs/FAQ.md",
            "#",
            "low-memory-mode",
            ")",
            "*",
            "[",
            "Environment",
            "Variables",
            "]",
            "(",
            "docs/FAQ.md",
            "#",
            "what-envs-can-i-pass-to-control-h2ogpt",
            ")",
            "*",
            "[",
            "HTTPS",
            "access",
            "for",
            "server",
            "and",
            "client",
            "]",
            "(",
            "docs/FAQ.md",
            "#",
            "https-access-for-server-and-client",
            ")",
            "*",
            "[",
            "Useful",
            "Links",
            "]",
            "(",
            "docs/LINKS.md",
            ")",
            "*",
            "[",
            "Fine-Tuning",
            "]",
            "(",
            "docs/FINETUNE.md",
            ")",
            "*",
            "[",
            "Triton",
            "]",
            "(",
            "docs/TRITON.md",
            ")",
            "*",
            "[",
            "Commercial",
            "viability",
            "]",
            "(",
            "docs/FAQ.md",
            "#",
            "commercial-viability",
            ")",
            "*",
            "[",
            "Acknowledgements",
            "]",
            "(",
            "#",
            "acknowledgements",
            ")",
            "*",
            "[",
            "Why",
            "H2O.ai",
            "?",
            "]",
            "(",
            "#",
            "why-h2oai",
            ")",
            "*",
            "[",
            "Disclaimer",
            "]",
            "(",
            "#",
            "disclaimer",
            ")"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/h2oai/h2ogpt",
        "readme_url": "https://raw.githubusercontent.com/h2oai/h2ogpt/main/README.md",
        "topic": [
            "ai",
            "chatgpt",
            "embeddings",
            "generative",
            "gpt",
            "gpt4all",
            "llama2",
            "llm",
            "mixtral",
            "pdf",
            "private",
            "privategpt",
            "vectorstore"
        ],
        "text": "Experimental features\n\nThese are not part of normal installation instructions and are experimental.\n\n* [Agents](docs/README_Agents.md) -- in Alpha testing.  Optimal for OpenAI, but that also fails sometimes.\n\n",
        "token": [
            "Experimental",
            "features",
            "These",
            "are",
            "not",
            "part",
            "of",
            "normal",
            "installation",
            "instructions",
            "and",
            "are",
            "experimental",
            ".",
            "*",
            "[",
            "Agents",
            "]",
            "(",
            "docs/README_Agents.md",
            ")",
            "--",
            "in",
            "Alpha",
            "testing",
            ".",
            "Optimal",
            "for",
            "OpenAI",
            ",",
            "but",
            "that",
            "also",
            "fails",
            "sometimes",
            "."
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/h2oai/h2ogpt",
        "readme_url": "https://raw.githubusercontent.com/h2oai/h2ogpt/main/README.md",
        "topic": [
            "ai",
            "chatgpt",
            "embeddings",
            "generative",
            "gpt",
            "gpt4all",
            "llama2",
            "llm",
            "mixtral",
            "pdf",
            "private",
            "privategpt",
            "vectorstore"
        ],
        "text": "Development\n\n- To create a development environment for training and generation, follow the [installation instructions](docs/INSTALL.md).\n- To fine-tune any LLM models on your data, follow the [fine-tuning instructions](docs/FINETUNE.md).\n- To run h2oGPT tests:\n    ```bash\n    pip install requirements-parser pytest-instafail pytest-random-order\n    pip install playsound==1.3.0\n    pytest --instafail -s -v tests\n    ",
        "token": [
            "Development",
            "-",
            "To",
            "create",
            "a",
            "development",
            "environment",
            "for",
            "training",
            "and",
            "generation",
            ",",
            "follow",
            "the",
            "[",
            "installation",
            "instructions",
            "]",
            "(",
            "docs/INSTALL.md",
            ")",
            ".",
            "-",
            "To",
            "fine-tune",
            "any",
            "LLM",
            "models",
            "on",
            "your",
            "data",
            ",",
            "follow",
            "the",
            "[",
            "fine-tuning",
            "instructions",
            "]",
            "(",
            "docs/FINETUNE.md",
            ")",
            ".",
            "-",
            "To",
            "run",
            "h2oGPT",
            "tests",
            ":",
            "``",
            "`",
            "bash",
            "pip",
            "install",
            "requirements-parser",
            "pytest-instafail",
            "pytest-random-order",
            "pip",
            "install",
            "playsound==1.3.0",
            "pytest",
            "--",
            "instafail",
            "-s",
            "-v",
            "tests"
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/h2oai/h2ogpt",
        "readme_url": "https://raw.githubusercontent.com/h2oai/h2ogpt/main/README.md",
        "topic": [
            "ai",
            "chatgpt",
            "embeddings",
            "generative",
            "gpt",
            "gpt4all",
            "llama2",
            "llm",
            "mixtral",
            "pdf",
            "private",
            "privategpt",
            "vectorstore"
        ],
        "text": "for client tests\n    make -C client setup\n    make -C client build\n    pytest --instafail -s -v client/tests\n    ",
        "token": [
            "for",
            "client",
            "tests",
            "make",
            "-C",
            "client",
            "setup",
            "make",
            "-C",
            "client",
            "build",
            "pytest",
            "--",
            "instafail",
            "-s",
            "-v",
            "client/tests"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/ShishirPatil/gorilla",
        "readme_url": "https://raw.githubusercontent.com/ShishirPatil/gorilla/main/README.md",
        "topic": [
            "api",
            "api-documentation",
            "chatgpt",
            "claude-api",
            "gpt-4-api",
            "llm",
            "openai-api",
            "openai-functions"
        ],
        "text": "Gorilla: Large Language Model Connected with Massive APIs [[Project Website](https://shishirpatil.github.io/gorilla/)]\n\n\n<img src=\"https://github.com/ShishirPatil/gorilla/blob/gh-pages/assets/img/logo.png\" width=50% height=50%>\n\n**:fire: Gorilla OpenFunctions** is a drop-in alternative for function calling! [Release Blog](https://gorilla.cs.berkeley.edu/blogs/4_open_functions.html)\n\n**\ud83d\udfe2 Gorilla is Apache 2.0** With Gorilla being fine-tuned on MPT, and Falcon, you can use Gorilla commercially with no obligations! :golf:  \n\n**:rocket: Try Gorilla in 60s** [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1DEBPsccVLF_aUnmD0FwPeHFrtdC0QIUP?usp=sharing) \n\n:computer: Use [Gorilla in your CLI](https://github.com/gorilla-llm/gorilla-cli) with `pip install gorilla-cli`\n\n**:newspaper_roll: Checkout our paper!** [![arXiv](https://img.shields.io/badge/arXiv-2305.15334-<COLOR>.svg?style=flat-square)](https://arxiv.org/abs/2305.15334)\n\n**:wave: Join our Discord!** [![Discord](https://img.shields.io/discord/1111172801899012102?label=Discord&logo=discord&logoColor=green&style=flat-square)](https://discord.gg/SwTyuTAxX3)\n\n\n`Gorilla` enables LLMs to use tools by invoking APIs. Given a natural language query, Gorilla comes up with the semantically- and syntactically- correct API to invoke. With Gorilla, we are the first to demonstrate how to use LLMs to invoke 1,600+ (and growing) API calls accurately while reducing hallucination. We also release APIBench, the largest collection of APIs, curated and easy to be trained on! Join us, as we try to expand the largest API store and teach LLMs how to write them! Hop on our Discord, or open a PR, or email us if you would like to have your API incorporated as well.\n\n",
        "token": [
            "Gorilla",
            ":",
            "Large",
            "Language",
            "Model",
            "Connected",
            "with",
            "Massive",
            "APIs",
            "[",
            "[",
            "Project",
            "Website",
            "]",
            "(",
            "https",
            ":",
            "//shishirpatil.github.io/gorilla/",
            ")",
            "]",
            "<",
            "img",
            "src=",
            "''",
            "https",
            ":",
            "//github.com/ShishirPatil/gorilla/blob/gh-pages/assets/img/logo.png",
            "''",
            "width=50",
            "%",
            "height=50",
            "%",
            ">",
            "*",
            "*",
            ":",
            "fire",
            ":",
            "Gorilla",
            "OpenFunctions",
            "*",
            "*",
            "is",
            "a",
            "drop-in",
            "alternative",
            "for",
            "function",
            "calling",
            "!",
            "[",
            "Release",
            "Blog",
            "]",
            "(",
            "https",
            ":",
            "//gorilla.cs.berkeley.edu/blogs/4_open_functions.html",
            ")",
            "*",
            "*",
            "\ud83d\udfe2",
            "Gorilla",
            "is",
            "Apache",
            "2.0",
            "*",
            "*",
            "With",
            "Gorilla",
            "being",
            "fine-tuned",
            "on",
            "MPT",
            ",",
            "and",
            "Falcon",
            ",",
            "you",
            "can",
            "use",
            "Gorilla",
            "commercially",
            "with",
            "no",
            "obligations",
            "!",
            ":",
            "golf",
            ":",
            "*",
            "*",
            ":",
            "rocket",
            ":",
            "Try",
            "Gorilla",
            "in",
            "60s",
            "*",
            "*",
            "[",
            "!",
            "[",
            "Colab",
            "]",
            "(",
            "https",
            ":",
            "//colab.research.google.com/assets/colab-badge.svg",
            ")",
            "]",
            "(",
            "https",
            ":",
            "//colab.research.google.com/drive/1DEBPsccVLF_aUnmD0FwPeHFrtdC0QIUP",
            "?",
            "usp=sharing",
            ")",
            ":",
            "computer",
            ":",
            "Use",
            "[",
            "Gorilla",
            "in",
            "your",
            "CLI",
            "]",
            "(",
            "https",
            ":",
            "//github.com/gorilla-llm/gorilla-cli",
            ")",
            "with",
            "`",
            "pip",
            "install",
            "gorilla-cli",
            "`",
            "*",
            "*",
            ":",
            "newspaper_roll",
            ":",
            "Checkout",
            "our",
            "paper",
            "!",
            "*",
            "*",
            "[",
            "!",
            "[",
            "arXiv",
            "]",
            "(",
            "https",
            ":",
            "//img.shields.io/badge/arXiv-2305.15334-",
            "<",
            "COLOR",
            ">",
            ".svg",
            "?",
            "style=flat-square",
            ")",
            "]",
            "(",
            "https",
            ":",
            "//arxiv.org/abs/2305.15334",
            ")",
            "*",
            "*",
            ":",
            "wave",
            ":",
            "Join",
            "our",
            "Discord",
            "!",
            "*",
            "*",
            "[",
            "!",
            "[",
            "Discord",
            "]",
            "(",
            "https",
            ":",
            "//img.shields.io/discord/1111172801899012102",
            "?",
            "label=Discord",
            "&",
            "logo=discord",
            "&",
            "logoColor=green",
            "&",
            "style=flat-square",
            ")",
            "]",
            "(",
            "https",
            ":",
            "//discord.gg/SwTyuTAxX3",
            ")",
            "`",
            "Gorilla",
            "`",
            "enables",
            "LLMs",
            "to",
            "use",
            "tools",
            "by",
            "invoking",
            "APIs",
            ".",
            "Given",
            "a",
            "natural",
            "language",
            "query",
            ",",
            "Gorilla",
            "comes",
            "up",
            "with",
            "the",
            "semantically-",
            "and",
            "syntactically-",
            "correct",
            "API",
            "to",
            "invoke",
            ".",
            "With",
            "Gorilla",
            ",",
            "we",
            "are",
            "the",
            "first",
            "to",
            "demonstrate",
            "how",
            "to",
            "use",
            "LLMs",
            "to",
            "invoke",
            "1,600+",
            "(",
            "and",
            "growing",
            ")",
            "API",
            "calls",
            "accurately",
            "while",
            "reducing",
            "hallucination",
            ".",
            "We",
            "also",
            "release",
            "APIBench",
            ",",
            "the",
            "largest",
            "collection",
            "of",
            "APIs",
            ",",
            "curated",
            "and",
            "easy",
            "to",
            "be",
            "trained",
            "on",
            "!",
            "Join",
            "us",
            ",",
            "as",
            "we",
            "try",
            "to",
            "expand",
            "the",
            "largest",
            "API",
            "store",
            "and",
            "teach",
            "LLMs",
            "how",
            "to",
            "write",
            "them",
            "!",
            "Hop",
            "on",
            "our",
            "Discord",
            ",",
            "or",
            "open",
            "a",
            "PR",
            ",",
            "or",
            "email",
            "us",
            "if",
            "you",
            "would",
            "like",
            "to",
            "have",
            "your",
            "API",
            "incorporated",
            "as",
            "well",
            "."
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/ShishirPatil/gorilla",
        "readme_url": "https://raw.githubusercontent.com/ShishirPatil/gorilla/main/README.md",
        "topic": [
            "api",
            "api-documentation",
            "chatgpt",
            "claude-api",
            "gpt-4-api",
            "llm",
            "openai-api",
            "openai-functions"
        ],
        "text": "Repository Organization\n\nOur repository organization is shown below. \n\n  - The `data` folder contains all the evaluation APIs `(APIBench)` and the community contributed APIs.\n  - The `eval` folder contains all our evaluation code as well as the Gorilla outputs.\n  - The `inference` folder contains all the inference code for running Gorilla locally.\n  - <span style=\"color:hr\">[Coming Soon!]</span>  The `train` folder contains all the training code associated with Gorilla finetuning.\n\n\nFor our dataset collections, all the 1640 API documentation is in `data/api`. We also include the `APIBench` dataset created by self-instruct in `data/apibench`. For evaluation, we convert this into a LLM-friendly chat format, and the questions are in `eval/eval-data/questions`, and the corresponding responses are in `eval/eval-data/responses`.  We have also included the evaluation scripts are in `eval/eval-scripts`. This would be entirely sufficient to train Gorilla yourself, and reproduce our results. Please see [evaluation](https://github.com/ShishirPatil/gorilla/tree/main/eval) for the details on how to use our evaluation pipeline.\n\nAdditionally, we have released all the model weights. `gorilla-7b-hf-v0` lets you invoke over 925 Hugging Face APIs. Similarly, `gorilla-7b-tf-v0` and `gorilla-7b-th-v0` have 626 (exhaustive) Tensorflow v2, and 94 (exhaustive) Torch Hub APIs. `gorilla-mpt-7b-hf-v0` and `gorilla-falcon-7b-hf-v0` are Apache 2.0 licensed models (commercially usable) fine-tuned on MPT-7B and Falcon-7B respectively. We will release a model with all three combined with generic chat capability and community contributed APIs as soon as we can scale our serving infrastructure. You can run Gorilla locally from instructions in the `inference/` sub-directory, or we also provide a hosted Gorilla chat completion API (see Colab)! If you have any suggestions, or if you run into any issues please feel free to reach out to us either through Discord or email or raise a Github issue.\n\n```\ngorilla\n\u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 api (TF/HF/TH APIs used in generating apibench)\n\u2502   \u2502   \u251c\u2500\u2500 {api_name}_api.jsonl\n\u2502   \u251c\u2500\u2500 apibench (Evaluating LLM models) v-1.0\n\u2502   \u2502   \u251c\u2500\u2500 {api_name}_train.jsonl, {api_name}_eval.jsonl\n|   |\u2500\u2500 apizoo (Contributed by the community - evolving)\n\u2502   |   \u251c\u2500\u2500 username1.json\n\u2502   \u2502   \u251c\u2500\u2500 username2.json\n\u2502   \u2502   \u251c\u2500\u2500 ...\n\u251c\u2500\u2500 eval\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u251c\u2500\u2500 get_llm_responses.py\n\u2502   \u251c\u2500\u2500 eval-scripts\n\u2502   \u2502   \u251c\u2500\u2500 ast_eval_{api_name}.py\n\u2502   \u251c\u2500\u2500 eval-data\n\u2502   \u2502   \u251c\u2500\u2500 questions\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 API name\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 questions_{api_name}_{eval_metric}.jsonl\n\u2502   \u2502   \u251c\u2500\u2500 responses\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 API name\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 responses_{api_name}_Gorilla_FT_{eval_metric}.jsonl\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 responses_{api_name}_Gorilla_RT_{eval_metric}.jsonl\n\u251c\u2500\u2500 inference\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u251c\u2500\u2500 serve\n\u2502   \u2502   \u251c\u2500\u2500 gorilla_cli.py\n\u2502   \u2502   \u251c\u2500\u2500 conv_template.py\n\u251c\u2500\u2500 train (Coming Soon!)\n\n```\n\n",
        "token": [
            "Repository",
            "Organization",
            "Our",
            "repository",
            "organization",
            "is",
            "shown",
            "below",
            ".",
            "-",
            "The",
            "`",
            "data",
            "`",
            "folder",
            "contains",
            "all",
            "the",
            "evaluation",
            "APIs",
            "`",
            "(",
            "APIBench",
            ")",
            "`",
            "and",
            "the",
            "community",
            "contributed",
            "APIs",
            ".",
            "-",
            "The",
            "`",
            "eval",
            "`",
            "folder",
            "contains",
            "all",
            "our",
            "evaluation",
            "code",
            "as",
            "well",
            "as",
            "the",
            "Gorilla",
            "outputs",
            ".",
            "-",
            "The",
            "`",
            "inference",
            "`",
            "folder",
            "contains",
            "all",
            "the",
            "inference",
            "code",
            "for",
            "running",
            "Gorilla",
            "locally",
            ".",
            "-",
            "<",
            "span",
            "style=",
            "''",
            "color",
            ":",
            "hr",
            "''",
            ">",
            "[",
            "Coming",
            "Soon",
            "!",
            "]",
            "<",
            "/span",
            ">",
            "The",
            "`",
            "train",
            "`",
            "folder",
            "contains",
            "all",
            "the",
            "training",
            "code",
            "associated",
            "with",
            "Gorilla",
            "finetuning",
            ".",
            "For",
            "our",
            "dataset",
            "collections",
            ",",
            "all",
            "the",
            "1640",
            "API",
            "documentation",
            "is",
            "in",
            "`",
            "data/api",
            "`",
            ".",
            "We",
            "also",
            "include",
            "the",
            "`",
            "APIBench",
            "`",
            "dataset",
            "created",
            "by",
            "self-instruct",
            "in",
            "`",
            "data/apibench",
            "`",
            ".",
            "For",
            "evaluation",
            ",",
            "we",
            "convert",
            "this",
            "into",
            "a",
            "LLM-friendly",
            "chat",
            "format",
            ",",
            "and",
            "the",
            "questions",
            "are",
            "in",
            "`",
            "eval/eval-data/questions",
            "`",
            ",",
            "and",
            "the",
            "corresponding",
            "responses",
            "are",
            "in",
            "`",
            "eval/eval-data/responses",
            "`",
            ".",
            "We",
            "have",
            "also",
            "included",
            "the",
            "evaluation",
            "scripts",
            "are",
            "in",
            "`",
            "eval/eval-scripts",
            "`",
            ".",
            "This",
            "would",
            "be",
            "entirely",
            "sufficient",
            "to",
            "train",
            "Gorilla",
            "yourself",
            ",",
            "and",
            "reproduce",
            "our",
            "results",
            ".",
            "Please",
            "see",
            "[",
            "evaluation",
            "]",
            "(",
            "https",
            ":",
            "//github.com/ShishirPatil/gorilla/tree/main/eval",
            ")",
            "for",
            "the",
            "details",
            "on",
            "how",
            "to",
            "use",
            "our",
            "evaluation",
            "pipeline",
            ".",
            "Additionally",
            ",",
            "we",
            "have",
            "released",
            "all",
            "the",
            "model",
            "weights",
            ".",
            "`",
            "gorilla-7b-hf-v0",
            "`",
            "lets",
            "you",
            "invoke",
            "over",
            "925",
            "Hugging",
            "Face",
            "APIs",
            ".",
            "Similarly",
            ",",
            "`",
            "gorilla-7b-tf-v0",
            "`",
            "and",
            "`",
            "gorilla-7b-th-v0",
            "`",
            "have",
            "626",
            "(",
            "exhaustive",
            ")",
            "Tensorflow",
            "v2",
            ",",
            "and",
            "94",
            "(",
            "exhaustive",
            ")",
            "Torch",
            "Hub",
            "APIs",
            ".",
            "`",
            "gorilla-mpt-7b-hf-v0",
            "`",
            "and",
            "`",
            "gorilla-falcon-7b-hf-v0",
            "`",
            "are",
            "Apache",
            "2.0",
            "licensed",
            "models",
            "(",
            "commercially",
            "usable",
            ")",
            "fine-tuned",
            "on",
            "MPT-7B",
            "and",
            "Falcon-7B",
            "respectively",
            ".",
            "We",
            "will",
            "release",
            "a",
            "model",
            "with",
            "all",
            "three",
            "combined",
            "with",
            "generic",
            "chat",
            "capability",
            "and",
            "community",
            "contributed",
            "APIs",
            "as",
            "soon",
            "as",
            "we",
            "can",
            "scale",
            "our",
            "serving",
            "infrastructure",
            ".",
            "You",
            "can",
            "run",
            "Gorilla",
            "locally",
            "from",
            "instructions",
            "in",
            "the",
            "`",
            "inference/",
            "`",
            "sub-directory",
            ",",
            "or",
            "we",
            "also",
            "provide",
            "a",
            "hosted",
            "Gorilla",
            "chat",
            "completion",
            "API",
            "(",
            "see",
            "Colab",
            ")",
            "!",
            "If",
            "you",
            "have",
            "any",
            "suggestions",
            ",",
            "or",
            "if",
            "you",
            "run",
            "into",
            "any",
            "issues",
            "please",
            "feel",
            "free",
            "to",
            "reach",
            "out",
            "to",
            "us",
            "either",
            "through",
            "Discord",
            "or",
            "email",
            "or",
            "raise",
            "a",
            "Github",
            "issue",
            ".",
            "``",
            "`",
            "gorilla",
            "\u251c\u2500\u2500",
            "data",
            "\u2502",
            "\u251c\u2500\u2500",
            "api",
            "(",
            "TF/HF/TH",
            "APIs",
            "used",
            "in",
            "generating",
            "apibench",
            ")",
            "\u2502",
            "\u2502",
            "\u251c\u2500\u2500",
            "{",
            "api_name",
            "}",
            "_api.jsonl",
            "\u2502",
            "\u251c\u2500\u2500",
            "apibench",
            "(",
            "Evaluating",
            "LLM",
            "models",
            ")",
            "v-1.0",
            "\u2502",
            "\u2502",
            "\u251c\u2500\u2500",
            "{",
            "api_name",
            "}",
            "_train.jsonl",
            ",",
            "{",
            "api_name",
            "}",
            "_eval.jsonl",
            "|",
            "|\u2500\u2500",
            "apizoo",
            "(",
            "Contributed",
            "by",
            "the",
            "community",
            "-",
            "evolving",
            ")",
            "\u2502",
            "|",
            "\u251c\u2500\u2500",
            "username1.json",
            "\u2502",
            "\u2502",
            "\u251c\u2500\u2500",
            "username2.json",
            "\u2502",
            "\u2502",
            "\u251c\u2500\u2500",
            "...",
            "\u251c\u2500\u2500",
            "eval",
            "\u2502",
            "\u251c\u2500\u2500",
            "README.md",
            "\u2502",
            "\u251c\u2500\u2500",
            "get_llm_responses.py",
            "\u2502",
            "\u251c\u2500\u2500",
            "eval-scripts",
            "\u2502",
            "\u2502",
            "\u251c\u2500\u2500",
            "ast_eval_",
            "{",
            "api_name",
            "}",
            ".py",
            "\u2502",
            "\u251c\u2500\u2500",
            "eval-data",
            "\u2502",
            "\u2502",
            "\u251c\u2500\u2500",
            "questions",
            "\u2502",
            "\u2502",
            "\u2502",
            "\u251c\u2500\u2500",
            "API",
            "name",
            "\u2502",
            "\u2502",
            "\u2502",
            "\u2502",
            "\u251c\u2500\u2500",
            "questions_",
            "{",
            "api_name",
            "}",
            "_",
            "{",
            "eval_metric",
            "}",
            ".jsonl",
            "\u2502",
            "\u2502",
            "\u251c\u2500\u2500",
            "responses",
            "\u2502",
            "\u2502",
            "\u2502",
            "\u251c\u2500\u2500",
            "API",
            "name",
            "\u2502",
            "\u2502",
            "\u2502",
            "\u2502",
            "\u251c\u2500\u2500",
            "responses_",
            "{",
            "api_name",
            "}",
            "_Gorilla_FT_",
            "{",
            "eval_metric",
            "}",
            ".jsonl",
            "\u2502",
            "\u2502",
            "\u2502",
            "\u2502",
            "\u251c\u2500\u2500",
            "responses_",
            "{",
            "api_name",
            "}",
            "_Gorilla_RT_",
            "{",
            "eval_metric",
            "}",
            ".jsonl",
            "\u251c\u2500\u2500",
            "inference",
            "\u2502",
            "\u251c\u2500\u2500",
            "README.md",
            "\u2502",
            "\u251c\u2500\u2500",
            "serve",
            "\u2502",
            "\u2502",
            "\u251c\u2500\u2500",
            "gorilla_cli.py",
            "\u2502",
            "\u2502",
            "\u251c\u2500\u2500",
            "conv_template.py",
            "\u251c\u2500\u2500",
            "train",
            "(",
            "Coming",
            "Soon",
            "!",
            ")",
            "``",
            "`"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/ShishirPatil/gorilla",
        "readme_url": "https://raw.githubusercontent.com/ShishirPatil/gorilla/main/README.md",
        "topic": [
            "api",
            "api-documentation",
            "chatgpt",
            "claude-api",
            "gpt-4-api",
            "llm",
            "openai-api",
            "openai-functions"
        ],
        "text": "Contributing Your API\nWe aim to build an open-source, one-stop-shop for all APIs, LLMs can interact with! Any suggestions and contributions are welcome! Please see the details on [how to contribute](https://github.com/ShishirPatil/gorilla/tree/main/data/README.md). THIS WILL ALWAYS REMAIN OPEN SOURCE.\n\n\n",
        "token": [
            "Contributing",
            "Your",
            "API",
            "We",
            "aim",
            "to",
            "build",
            "an",
            "open-source",
            ",",
            "one-stop-shop",
            "for",
            "all",
            "APIs",
            ",",
            "LLMs",
            "can",
            "interact",
            "with",
            "!",
            "Any",
            "suggestions",
            "and",
            "contributions",
            "are",
            "welcome",
            "!",
            "Please",
            "see",
            "the",
            "details",
            "on",
            "[",
            "how",
            "to",
            "contribute",
            "]",
            "(",
            "https",
            ":",
            "//github.com/ShishirPatil/gorilla/tree/main/data/README.md",
            ")",
            ".",
            "THIS",
            "WILL",
            "ALWAYS",
            "REMAIN",
            "OPEN",
            "SOURCE",
            "."
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/ShishirPatil/gorilla",
        "readme_url": "https://raw.githubusercontent.com/ShishirPatil/gorilla/main/README.md",
        "topic": [
            "api",
            "api-documentation",
            "chatgpt",
            "claude-api",
            "gpt-4-api",
            "llm",
            "openai-api",
            "openai-functions"
        ],
        "text": "FAQ(s)\n\n1. I would like to use Gorilla commercially. Is there going to be a Apache 2.0 licensed version?\n\nYes! We now have models that you can use commercially without any obligations.\n\n\n2. Can we use Gorilla with Langchain, Toolformer, AutoGPT etc?\n\nAbsolutely! You've highlighted a great aspect of our tools. Gorilla is  an  end-to-end model, specifically tailored to serve correct API calls without requiring any additional coding. It's designed to work as part of a wider ecosystem and can be flexibly integrated with other tools.\n\nLangchain, is a versatile developer tool. Its \"agents\" can efficiently swap in any LLM, Gorilla included, making it a highly adaptable solution for various needs.\n\nAutoGPT, on the other hand, concentrates on the art of prompting GPT series models. It's worth noting that Gorilla, as a fully fine-tuned model, consistently shows remarkable accuracy, and lowers hallucination, outperforming GPT-4 in making specific API calls.\n\nNow, when it comes to ToolFormer, Toolformer zeroes in on a select set of tools, providing specialized functionalities. Gorilla, in contrast, has the capacity to manage thousands of API calls, offering a broader coverage over a more extensive range of tools.\n\nThe beauty of these tools truly shines when they collaborate, complementing each other's strengths and capabilities to create an even more powerful and comprehensive solution. This is where your contribution can make a difference. We enthusiastically welcome any inputs to further refine and enhance these tools. \n\n3. How to train your own Gorilla models? \n\nWe will release the training code as soon as we can get GPUs to test and finalize the pipeline. Given the demand for our hosted end-points, we have dedicated all of our GPUs to serve the models. If you would like to help with resources get in touch!\n\n\n",
        "token": [
            "FAQ",
            "(",
            "s",
            ")",
            "1",
            ".",
            "I",
            "would",
            "like",
            "to",
            "use",
            "Gorilla",
            "commercially",
            ".",
            "Is",
            "there",
            "going",
            "to",
            "be",
            "a",
            "Apache",
            "2.0",
            "licensed",
            "version",
            "?",
            "Yes",
            "!",
            "We",
            "now",
            "have",
            "models",
            "that",
            "you",
            "can",
            "use",
            "commercially",
            "without",
            "any",
            "obligations",
            ".",
            "2",
            ".",
            "Can",
            "we",
            "use",
            "Gorilla",
            "with",
            "Langchain",
            ",",
            "Toolformer",
            ",",
            "AutoGPT",
            "etc",
            "?",
            "Absolutely",
            "!",
            "You",
            "'ve",
            "highlighted",
            "a",
            "great",
            "aspect",
            "of",
            "our",
            "tools",
            ".",
            "Gorilla",
            "is",
            "an",
            "end-to-end",
            "model",
            ",",
            "specifically",
            "tailored",
            "to",
            "serve",
            "correct",
            "API",
            "calls",
            "without",
            "requiring",
            "any",
            "additional",
            "coding",
            ".",
            "It",
            "'s",
            "designed",
            "to",
            "work",
            "as",
            "part",
            "of",
            "a",
            "wider",
            "ecosystem",
            "and",
            "can",
            "be",
            "flexibly",
            "integrated",
            "with",
            "other",
            "tools",
            ".",
            "Langchain",
            ",",
            "is",
            "a",
            "versatile",
            "developer",
            "tool",
            ".",
            "Its",
            "``",
            "agents",
            "''",
            "can",
            "efficiently",
            "swap",
            "in",
            "any",
            "LLM",
            ",",
            "Gorilla",
            "included",
            ",",
            "making",
            "it",
            "a",
            "highly",
            "adaptable",
            "solution",
            "for",
            "various",
            "needs",
            ".",
            "AutoGPT",
            ",",
            "on",
            "the",
            "other",
            "hand",
            ",",
            "concentrates",
            "on",
            "the",
            "art",
            "of",
            "prompting",
            "GPT",
            "series",
            "models",
            ".",
            "It",
            "'s",
            "worth",
            "noting",
            "that",
            "Gorilla",
            ",",
            "as",
            "a",
            "fully",
            "fine-tuned",
            "model",
            ",",
            "consistently",
            "shows",
            "remarkable",
            "accuracy",
            ",",
            "and",
            "lowers",
            "hallucination",
            ",",
            "outperforming",
            "GPT-4",
            "in",
            "making",
            "specific",
            "API",
            "calls",
            ".",
            "Now",
            ",",
            "when",
            "it",
            "comes",
            "to",
            "ToolFormer",
            ",",
            "Toolformer",
            "zeroes",
            "in",
            "on",
            "a",
            "select",
            "set",
            "of",
            "tools",
            ",",
            "providing",
            "specialized",
            "functionalities",
            ".",
            "Gorilla",
            ",",
            "in",
            "contrast",
            ",",
            "has",
            "the",
            "capacity",
            "to",
            "manage",
            "thousands",
            "of",
            "API",
            "calls",
            ",",
            "offering",
            "a",
            "broader",
            "coverage",
            "over",
            "a",
            "more",
            "extensive",
            "range",
            "of",
            "tools",
            ".",
            "The",
            "beauty",
            "of",
            "these",
            "tools",
            "truly",
            "shines",
            "when",
            "they",
            "collaborate",
            ",",
            "complementing",
            "each",
            "other",
            "'s",
            "strengths",
            "and",
            "capabilities",
            "to",
            "create",
            "an",
            "even",
            "more",
            "powerful",
            "and",
            "comprehensive",
            "solution",
            ".",
            "This",
            "is",
            "where",
            "your",
            "contribution",
            "can",
            "make",
            "a",
            "difference",
            ".",
            "We",
            "enthusiastically",
            "welcome",
            "any",
            "inputs",
            "to",
            "further",
            "refine",
            "and",
            "enhance",
            "these",
            "tools",
            ".",
            "3",
            ".",
            "How",
            "to",
            "train",
            "your",
            "own",
            "Gorilla",
            "models",
            "?",
            "We",
            "will",
            "release",
            "the",
            "training",
            "code",
            "as",
            "soon",
            "as",
            "we",
            "can",
            "get",
            "GPUs",
            "to",
            "test",
            "and",
            "finalize",
            "the",
            "pipeline",
            ".",
            "Given",
            "the",
            "demand",
            "for",
            "our",
            "hosted",
            "end-points",
            ",",
            "we",
            "have",
            "dedicated",
            "all",
            "of",
            "our",
            "GPUs",
            "to",
            "serve",
            "the",
            "models",
            ".",
            "If",
            "you",
            "would",
            "like",
            "to",
            "help",
            "with",
            "resources",
            "get",
            "in",
            "touch",
            "!"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/QwenLM/Qwen",
        "readme_url": "https://raw.githubusercontent.com/QwenLM/Qwen/main/README.md",
        "topic": [
            "chinese",
            "flash-attention",
            "large-language-models",
            "llm",
            "natural-language-processing",
            "pretrained-models"
        ],
        "text": "Quickstart\n\nBelow, we provide simple examples to show how to use Qwen-Chat with \ud83e\udd16 ModelScope and \ud83e\udd17 Transformers.\n\nYou can use our pre-built docker images to skip most of the environment setup steps, see Section [\"Using Pre-built Docker Images\"](#-docker) for more details. \n\nIf not using docker, please make sure you have setup the environment and installed the required packages. Make sure you meet the above requirements, and then install the dependent libraries.\n\n```bash\npip install -r requirements.txt\n```\n\nIf your device supports fp16 or bf16, we recommend installing [flash-attention](https://github.com/Dao-AILab/flash-attention) (**we support flash attention 2 now.**) for higher efficiency and lower memory usage. (**flash-attention is optional and the project can run normally without installing it**)\n\n```bash\ngit clone https://github.com/Dao-AILab/flash-attention\ncd flash-attention && pip install .\n",
        "token": [
            "Quickstart",
            "Below",
            ",",
            "we",
            "provide",
            "simple",
            "examples",
            "to",
            "show",
            "how",
            "to",
            "use",
            "Qwen-Chat",
            "with",
            "\ud83e\udd16",
            "ModelScope",
            "and",
            "\ud83e\udd17",
            "Transformers",
            ".",
            "You",
            "can",
            "use",
            "our",
            "pre-built",
            "docker",
            "images",
            "to",
            "skip",
            "most",
            "of",
            "the",
            "environment",
            "setup",
            "steps",
            ",",
            "see",
            "Section",
            "[",
            "``",
            "Using",
            "Pre-built",
            "Docker",
            "Images",
            "''",
            "]",
            "(",
            "#",
            "-docker",
            ")",
            "for",
            "more",
            "details",
            ".",
            "If",
            "not",
            "using",
            "docker",
            ",",
            "please",
            "make",
            "sure",
            "you",
            "have",
            "setup",
            "the",
            "environment",
            "and",
            "installed",
            "the",
            "required",
            "packages",
            ".",
            "Make",
            "sure",
            "you",
            "meet",
            "the",
            "above",
            "requirements",
            ",",
            "and",
            "then",
            "install",
            "the",
            "dependent",
            "libraries",
            ".",
            "``",
            "`",
            "bash",
            "pip",
            "install",
            "-r",
            "requirements.txt",
            "``",
            "`",
            "If",
            "your",
            "device",
            "supports",
            "fp16",
            "or",
            "bf16",
            ",",
            "we",
            "recommend",
            "installing",
            "[",
            "flash-attention",
            "]",
            "(",
            "https",
            ":",
            "//github.com/Dao-AILab/flash-attention",
            ")",
            "(",
            "*",
            "*",
            "we",
            "support",
            "flash",
            "attention",
            "2",
            "now",
            ".",
            "*",
            "*",
            ")",
            "for",
            "higher",
            "efficiency",
            "and",
            "lower",
            "memory",
            "usage",
            ".",
            "(",
            "*",
            "*",
            "flash-attention",
            "is",
            "optional",
            "and",
            "the",
            "project",
            "can",
            "run",
            "normally",
            "without",
            "installing",
            "it",
            "*",
            "*",
            ")",
            "``",
            "`",
            "bash",
            "git",
            "clone",
            "https",
            ":",
            "//github.com/Dao-AILab/flash-attention",
            "cd",
            "flash-attention",
            "&",
            "&",
            "pip",
            "install",
            "."
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/QwenLM/Qwen",
        "readme_url": "https://raw.githubusercontent.com/QwenLM/Qwen/main/README.md",
        "topic": [
            "chinese",
            "flash-attention",
            "large-language-models",
            "llm",
            "natural-language-processing",
            "pretrained-models"
        ],
        "text": "Below are optional. Installing them might be slow.\n",
        "token": [
            "Below",
            "are",
            "optional",
            ".",
            "Installing",
            "them",
            "might",
            "be",
            "slow",
            "."
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/QwenLM/Qwen",
        "readme_url": "https://raw.githubusercontent.com/QwenLM/Qwen/main/README.md",
        "topic": [
            "chinese",
            "flash-attention",
            "large-language-models",
            "llm",
            "natural-language-processing",
            "pretrained-models"
        ],
        "text": "pip install csrc/layer_norm\n",
        "token": [
            "pip",
            "install",
            "csrc/layer_norm"
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/QwenLM/Qwen",
        "readme_url": "https://raw.githubusercontent.com/QwenLM/Qwen/main/README.md",
        "topic": [
            "chinese",
            "flash-attention",
            "large-language-models",
            "llm",
            "natural-language-processing",
            "pretrained-models"
        ],
        "text": "pip install csrc/rotary\n```\n\nNow you can start with ModelScope or Transformers.\n\n",
        "token": [
            "pip",
            "install",
            "csrc/rotary",
            "``",
            "`",
            "Now",
            "you",
            "can",
            "start",
            "with",
            "ModelScope",
            "or",
            "Transformers",
            "."
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/QwenLM/Qwen",
        "readme_url": "https://raw.githubusercontent.com/QwenLM/Qwen/main/README.md",
        "topic": [
            "chinese",
            "flash-attention",
            "large-language-models",
            "llm",
            "natural-language-processing",
            "pretrained-models"
        ],
        "text": "DashScope\nThe most simple way to use Qwen through APIs is DashScope API service through Alibaba Cloud. We give an introduction to the usage. Additionally, we provide a script for you to deploy an OpenAI-style API on your own servers.\n\nDashScope is the large language model API service provided by Alibaba Cloud, which now supports Qwen. Note that the models behind DashScope are in-house versions temporarily without details provided. The services include `qwen-turbo` and `qwen-plus`, where the former one runs faster and the latter achieves better performance. For more information, visit the documentation [here](https://dashscope.aliyun.com).\n\nPlease head to the official website [link](https://help.aliyun.com/zh/dashscope/developer-reference/activate-dashscope-and-create-an-api-key?spm=a2c4g.11186623.0.0.6c2774fahtfXdn) to create a DashScope account and obtain the API key (AK). We recommend setting the AK with an environment variable:\n```bash\nexport DASHSCOPE_API_KEY=\"YOUR_DASHSCOPE_API_KEY\"\n```\nThen please install the packages and click [here](https://help.aliyun.com/zh/dashscope/developer-reference/install-dashscope-sdk) for the documentation. If you use Python, you can install DashScope with pip:\n```bash\npip install dashscope\n```\nIf you use JAVA SDK, you can install it in this way:\n```xml\n<!-- https://mvnrepository.com/artifact/com.alibaba/dashscope-sdk-java -->\n<dependency>\n    <groupId>com.alibaba</groupId>\n    <artifactId>dashscope-sdk-java</artifactId>\n    <version>the-latest-version</version>\n</dependency>\n```\nThe simplest way to use DashScope is the usage with messages, which is similar to OpenAI API. The example is demonstrated below:\n```python\nimport random\nfrom http import HTTPStatus\nfrom dashscope import Generation\n\n\ndef call_with_messages():\n    messages = [{'role': 'system', 'content': 'You are a helpful assistant.'},\n                {'role': 'user', 'content': '\u5982\u4f55\u505a\u897f\u7ea2\u67ff\u9e21\u86cb\uff1f'}]\n    gen = Generation()\n    response = gen.call(\n        Generation.Models.qwen_turbo,\n        messages=messages,\n        seed=random.randint(1, 10000),  ",
        "token": [
            "DashScope",
            "The",
            "most",
            "simple",
            "way",
            "to",
            "use",
            "Qwen",
            "through",
            "APIs",
            "is",
            "DashScope",
            "API",
            "service",
            "through",
            "Alibaba",
            "Cloud",
            ".",
            "We",
            "give",
            "an",
            "introduction",
            "to",
            "the",
            "usage",
            ".",
            "Additionally",
            ",",
            "we",
            "provide",
            "a",
            "script",
            "for",
            "you",
            "to",
            "deploy",
            "an",
            "OpenAI-style",
            "API",
            "on",
            "your",
            "own",
            "servers",
            ".",
            "DashScope",
            "is",
            "the",
            "large",
            "language",
            "model",
            "API",
            "service",
            "provided",
            "by",
            "Alibaba",
            "Cloud",
            ",",
            "which",
            "now",
            "supports",
            "Qwen",
            ".",
            "Note",
            "that",
            "the",
            "models",
            "behind",
            "DashScope",
            "are",
            "in-house",
            "versions",
            "temporarily",
            "without",
            "details",
            "provided",
            ".",
            "The",
            "services",
            "include",
            "`",
            "qwen-turbo",
            "`",
            "and",
            "`",
            "qwen-plus",
            "`",
            ",",
            "where",
            "the",
            "former",
            "one",
            "runs",
            "faster",
            "and",
            "the",
            "latter",
            "achieves",
            "better",
            "performance",
            ".",
            "For",
            "more",
            "information",
            ",",
            "visit",
            "the",
            "documentation",
            "[",
            "here",
            "]",
            "(",
            "https",
            ":",
            "//dashscope.aliyun.com",
            ")",
            ".",
            "Please",
            "head",
            "to",
            "the",
            "official",
            "website",
            "[",
            "link",
            "]",
            "(",
            "https",
            ":",
            "//help.aliyun.com/zh/dashscope/developer-reference/activate-dashscope-and-create-an-api-key",
            "?",
            "spm=a2c4g.11186623.0.0.6c2774fahtfXdn",
            ")",
            "to",
            "create",
            "a",
            "DashScope",
            "account",
            "and",
            "obtain",
            "the",
            "API",
            "key",
            "(",
            "AK",
            ")",
            ".",
            "We",
            "recommend",
            "setting",
            "the",
            "AK",
            "with",
            "an",
            "environment",
            "variable",
            ":",
            "``",
            "`",
            "bash",
            "export",
            "DASHSCOPE_API_KEY=",
            "''",
            "YOUR_DASHSCOPE_API_KEY",
            "''",
            "``",
            "`",
            "Then",
            "please",
            "install",
            "the",
            "packages",
            "and",
            "click",
            "[",
            "here",
            "]",
            "(",
            "https",
            ":",
            "//help.aliyun.com/zh/dashscope/developer-reference/install-dashscope-sdk",
            ")",
            "for",
            "the",
            "documentation",
            ".",
            "If",
            "you",
            "use",
            "Python",
            ",",
            "you",
            "can",
            "install",
            "DashScope",
            "with",
            "pip",
            ":",
            "``",
            "`",
            "bash",
            "pip",
            "install",
            "dashscope",
            "``",
            "`",
            "If",
            "you",
            "use",
            "JAVA",
            "SDK",
            ",",
            "you",
            "can",
            "install",
            "it",
            "in",
            "this",
            "way",
            ":",
            "``",
            "`",
            "xml",
            "<",
            "!",
            "--",
            "https",
            ":",
            "//mvnrepository.com/artifact/com.alibaba/dashscope-sdk-java",
            "--",
            ">",
            "<",
            "dependency",
            ">",
            "<",
            "groupId",
            ">",
            "com.alibaba",
            "<",
            "/groupId",
            ">",
            "<",
            "artifactId",
            ">",
            "dashscope-sdk-java",
            "<",
            "/artifactId",
            ">",
            "<",
            "version",
            ">",
            "the-latest-version",
            "<",
            "/version",
            ">",
            "<",
            "/dependency",
            ">",
            "``",
            "`",
            "The",
            "simplest",
            "way",
            "to",
            "use",
            "DashScope",
            "is",
            "the",
            "usage",
            "with",
            "messages",
            ",",
            "which",
            "is",
            "similar",
            "to",
            "OpenAI",
            "API",
            ".",
            "The",
            "example",
            "is",
            "demonstrated",
            "below",
            ":",
            "``",
            "`",
            "python",
            "import",
            "random",
            "from",
            "http",
            "import",
            "HTTPStatus",
            "from",
            "dashscope",
            "import",
            "Generation",
            "def",
            "call_with_messages",
            "(",
            ")",
            ":",
            "messages",
            "=",
            "[",
            "{",
            "'role",
            "'",
            ":",
            "'system",
            "'",
            ",",
            "'content",
            "'",
            ":",
            "'You",
            "are",
            "a",
            "helpful",
            "assistant",
            ".",
            "'",
            "}",
            ",",
            "{",
            "'role",
            "'",
            ":",
            "'user",
            "'",
            ",",
            "'content",
            "'",
            ":",
            "'\u5982\u4f55\u505a\u897f\u7ea2\u67ff\u9e21\u86cb\uff1f",
            "'",
            "}",
            "]",
            "gen",
            "=",
            "Generation",
            "(",
            ")",
            "response",
            "=",
            "gen.call",
            "(",
            "Generation.Models.qwen_turbo",
            ",",
            "messages=messages",
            ",",
            "seed=random.randint",
            "(",
            "1",
            ",",
            "10000",
            ")",
            ","
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/QwenLM/Qwen",
        "readme_url": "https://raw.githubusercontent.com/QwenLM/Qwen/main/README.md",
        "topic": [
            "chinese",
            "flash-attention",
            "large-language-models",
            "llm",
            "natural-language-processing",
            "pretrained-models"
        ],
        "text": "GPTQ\n\nWe provide a solution based on [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ), and release the Int4 and Int8 quantized models, which achieve nearly lossless model effects but improved performance on both memory costs and inference speed.\n\nHere we demonstrate how to use our provided quantized models for inference. Before you start, make sure you meet the requirements of auto-gptq (e.g., torch 2.0 and above, transformers 4.32.0 and above, etc.) and install the required packages:\n\n```bash\npip install auto-gptq optimum\n```\n\nIf you meet problems installing `auto-gptq`, we advise you to check out the official [repo](https://github.com/PanQiWei/AutoGPTQ) to find a wheel.\n\n> Note: The pre-compiled `auto-gptq` packages strongly depend on the version of `torch` and its CUDA version. Moreover, due to recent update, \n> you may also encounter unsupported version errors from `transformers`, `optimum`, or `peft`.\n> We recommend using the latest versions meeting the following requirements:\n> - torch==2.1 auto-gptq>=0.5.1 transformers>=4.35.0 optimum>=1.14.0 peft>=0.6.1\n> - torch>=2.0,<2.1 auto-gptq<0.5.0 transformers<4.35.0 optimum<1.14.0 peft>=0.5.0,<0.6.0\n\nThen you can load the quantized model easily and run inference as same as usual:\n\n```python\n",
        "token": [
            "GPTQ",
            "We",
            "provide",
            "a",
            "solution",
            "based",
            "on",
            "[",
            "AutoGPTQ",
            "]",
            "(",
            "https",
            ":",
            "//github.com/PanQiWei/AutoGPTQ",
            ")",
            ",",
            "and",
            "release",
            "the",
            "Int4",
            "and",
            "Int8",
            "quantized",
            "models",
            ",",
            "which",
            "achieve",
            "nearly",
            "lossless",
            "model",
            "effects",
            "but",
            "improved",
            "performance",
            "on",
            "both",
            "memory",
            "costs",
            "and",
            "inference",
            "speed",
            ".",
            "Here",
            "we",
            "demonstrate",
            "how",
            "to",
            "use",
            "our",
            "provided",
            "quantized",
            "models",
            "for",
            "inference",
            ".",
            "Before",
            "you",
            "start",
            ",",
            "make",
            "sure",
            "you",
            "meet",
            "the",
            "requirements",
            "of",
            "auto-gptq",
            "(",
            "e.g.",
            ",",
            "torch",
            "2.0",
            "and",
            "above",
            ",",
            "transformers",
            "4.32.0",
            "and",
            "above",
            ",",
            "etc",
            ".",
            ")",
            "and",
            "install",
            "the",
            "required",
            "packages",
            ":",
            "``",
            "`",
            "bash",
            "pip",
            "install",
            "auto-gptq",
            "optimum",
            "``",
            "`",
            "If",
            "you",
            "meet",
            "problems",
            "installing",
            "`",
            "auto-gptq",
            "`",
            ",",
            "we",
            "advise",
            "you",
            "to",
            "check",
            "out",
            "the",
            "official",
            "[",
            "repo",
            "]",
            "(",
            "https",
            ":",
            "//github.com/PanQiWei/AutoGPTQ",
            ")",
            "to",
            "find",
            "a",
            "wheel",
            ".",
            ">",
            "Note",
            ":",
            "The",
            "pre-compiled",
            "`",
            "auto-gptq",
            "`",
            "packages",
            "strongly",
            "depend",
            "on",
            "the",
            "version",
            "of",
            "`",
            "torch",
            "`",
            "and",
            "its",
            "CUDA",
            "version",
            ".",
            "Moreover",
            ",",
            "due",
            "to",
            "recent",
            "update",
            ",",
            ">",
            "you",
            "may",
            "also",
            "encounter",
            "unsupported",
            "version",
            "errors",
            "from",
            "`",
            "transformers",
            "`",
            ",",
            "`",
            "optimum",
            "`",
            ",",
            "or",
            "`",
            "peft",
            "`",
            ".",
            ">",
            "We",
            "recommend",
            "using",
            "the",
            "latest",
            "versions",
            "meeting",
            "the",
            "following",
            "requirements",
            ":",
            ">",
            "-",
            "torch==2.1",
            "auto-gptq",
            ">",
            "=0.5.1",
            "transformers",
            ">",
            "=4.35.0",
            "optimum",
            ">",
            "=1.14.0",
            "peft",
            ">",
            "=0.6.1",
            ">",
            "-",
            "torch",
            ">",
            "=2.0",
            ",",
            "<",
            "2.1",
            "auto-gptq",
            "<",
            "0.5.0",
            "transformers",
            "<",
            "4.35.0",
            "optimum",
            "<",
            "1.14.0",
            "peft",
            ">",
            "=0.5.0",
            ",",
            "<",
            "0.6.0",
            "Then",
            "you",
            "can",
            "load",
            "the",
            "quantized",
            "model",
            "easily",
            "and",
            "run",
            "inference",
            "as",
            "same",
            "as",
            "usual",
            ":",
            "``",
            "`",
            "python"
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/QwenLM/Qwen",
        "readme_url": "https://raw.githubusercontent.com/QwenLM/Qwen/main/README.md",
        "topic": [
            "chinese",
            "flash-attention",
            "large-language-models",
            "llm",
            "natural-language-processing",
            "pretrained-models"
        ],
        "text": "Usage\nNow we provide the official training script, `finetune.py`, for users to finetune the pretrained model for downstream applications in a simple fashion. Additionally, we provide shell scripts to launch finetuning with no worries. This script supports the training with [DeepSpeed](https://github.com/microsoft/DeepSpeed) and [FSDP](https://engineering.fb.com/2021/07/15/open-source/fsdp/). The shell scripts that we provide use DeepSpeed (Note: this may have conflicts with the latest version of pydantic and you should use make sure `pydantic<2.0`) and Peft. You can install them by:\n```bash\npip install peft deepspeed\n```\n\nTo prepare your training data, you need to put all the samples into a list and save it to a json file. Each sample is a dictionary consisting of an id and a list for conversation. Below is a simple example list with 1 sample:\n```json\n[\n  {\n    \"id\": \"identity_0\",\n    \"conversations\": [\n      {\n        \"from\": \"user\",\n        \"value\": \"\u4f60\u597d\"\n      },\n      {\n        \"from\": \"assistant\",\n        \"value\": \"\u6211\u662f\u4e00\u4e2a\u8bed\u8a00\u6a21\u578b\uff0c\u6211\u53eb\u901a\u4e49\u5343\u95ee\u3002\"\n      }\n    ]\n  }\n]\n```\n\nAfter data preparation, you can use the provided shell scripts to run finetuning. Remember to specify the path to the data file, `$DATA`.\n\nThe finetuning scripts allow you to perform:\n- Full-parameter finetuning\n- LoRA\n- Q-LoRA\n\nFull-parameter finetuning requires updating all parameters in the whole training process. To launch your training, run the following script:\n\n```bash\n",
        "token": [
            "Usage",
            "Now",
            "we",
            "provide",
            "the",
            "official",
            "training",
            "script",
            ",",
            "`",
            "finetune.py",
            "`",
            ",",
            "for",
            "users",
            "to",
            "finetune",
            "the",
            "pretrained",
            "model",
            "for",
            "downstream",
            "applications",
            "in",
            "a",
            "simple",
            "fashion",
            ".",
            "Additionally",
            ",",
            "we",
            "provide",
            "shell",
            "scripts",
            "to",
            "launch",
            "finetuning",
            "with",
            "no",
            "worries",
            ".",
            "This",
            "script",
            "supports",
            "the",
            "training",
            "with",
            "[",
            "DeepSpeed",
            "]",
            "(",
            "https",
            ":",
            "//github.com/microsoft/DeepSpeed",
            ")",
            "and",
            "[",
            "FSDP",
            "]",
            "(",
            "https",
            ":",
            "//engineering.fb.com/2021/07/15/open-source/fsdp/",
            ")",
            ".",
            "The",
            "shell",
            "scripts",
            "that",
            "we",
            "provide",
            "use",
            "DeepSpeed",
            "(",
            "Note",
            ":",
            "this",
            "may",
            "have",
            "conflicts",
            "with",
            "the",
            "latest",
            "version",
            "of",
            "pydantic",
            "and",
            "you",
            "should",
            "use",
            "make",
            "sure",
            "`",
            "pydantic",
            "<",
            "2.0",
            "`",
            ")",
            "and",
            "Peft",
            ".",
            "You",
            "can",
            "install",
            "them",
            "by",
            ":",
            "``",
            "`",
            "bash",
            "pip",
            "install",
            "peft",
            "deepspeed",
            "``",
            "`",
            "To",
            "prepare",
            "your",
            "training",
            "data",
            ",",
            "you",
            "need",
            "to",
            "put",
            "all",
            "the",
            "samples",
            "into",
            "a",
            "list",
            "and",
            "save",
            "it",
            "to",
            "a",
            "json",
            "file",
            ".",
            "Each",
            "sample",
            "is",
            "a",
            "dictionary",
            "consisting",
            "of",
            "an",
            "id",
            "and",
            "a",
            "list",
            "for",
            "conversation",
            ".",
            "Below",
            "is",
            "a",
            "simple",
            "example",
            "list",
            "with",
            "1",
            "sample",
            ":",
            "``",
            "`",
            "json",
            "[",
            "{",
            "``",
            "id",
            "''",
            ":",
            "``",
            "identity_0",
            "''",
            ",",
            "``",
            "conversations",
            "''",
            ":",
            "[",
            "{",
            "``",
            "from",
            "''",
            ":",
            "``",
            "user",
            "''",
            ",",
            "``",
            "value",
            "''",
            ":",
            "``",
            "\u4f60\u597d",
            "''",
            "}",
            ",",
            "{",
            "``",
            "from",
            "''",
            ":",
            "``",
            "assistant",
            "''",
            ",",
            "``",
            "value",
            "''",
            ":",
            "``",
            "\u6211\u662f\u4e00\u4e2a\u8bed\u8a00\u6a21\u578b\uff0c\u6211\u53eb\u901a\u4e49\u5343\u95ee\u3002",
            "''",
            "}",
            "]",
            "}",
            "]",
            "``",
            "`",
            "After",
            "data",
            "preparation",
            ",",
            "you",
            "can",
            "use",
            "the",
            "provided",
            "shell",
            "scripts",
            "to",
            "run",
            "finetuning",
            ".",
            "Remember",
            "to",
            "specify",
            "the",
            "path",
            "to",
            "the",
            "data",
            "file",
            ",",
            "`",
            "$",
            "DATA",
            "`",
            ".",
            "The",
            "finetuning",
            "scripts",
            "allow",
            "you",
            "to",
            "perform",
            ":",
            "-",
            "Full-parameter",
            "finetuning",
            "-",
            "LoRA",
            "-",
            "Q-LoRA",
            "Full-parameter",
            "finetuning",
            "requires",
            "updating",
            "all",
            "parameters",
            "in",
            "the",
            "whole",
            "training",
            "process",
            ".",
            "To",
            "launch",
            "your",
            "training",
            ",",
            "run",
            "the",
            "following",
            "script",
            ":",
            "``",
            "`",
            "bash"
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/QwenLM/Qwen",
        "readme_url": "https://raw.githubusercontent.com/QwenLM/Qwen/main/README.md",
        "topic": [
            "chinese",
            "flash-attention",
            "large-language-models",
            "llm",
            "natural-language-processing",
            "pretrained-models"
        ],
        "text": "Distributed training. We do not provide single-GPU training script as the insufficient GPU memory will break down the training.\nbash finetune/finetune_ds.sh\n```\n\nRemember to specify the correct model name or path, the data path, as well as the output directory in the shell scripts. Another thing to notice is that we use DeepSpeed ZeRO 3 in this script. If you want to make changes, just remove the argument `--deepspeed` or make changes in the DeepSpeed configuration json file based on your requirements. Additionally, this script supports mixed-precision training, and thus you can use `--bf16 True` or `--fp16 True`. Remember to use DeepSpeed when you use fp16 due to mixed precision training. Empirically we advise you to use bf16 to make your training consistent with our pretraining and alignment if your machine supports bf16, and thus we use it by default.\n\nSimilarly, to run LoRA, use another script to run as shown below. Before you start, make sure that you have installed `peft`. Also, you need to specify your paths to your model, data, and output. We advise you to use absolute path for your pretrained model. This is because LoRA only saves the adapter and the absolute path in the adapter configuration json file is used for finding out the pretrained model to load. Also, this script support both bf16 and fp16.\n\n```bash\n",
        "token": [
            "Distributed",
            "training",
            ".",
            "We",
            "do",
            "not",
            "provide",
            "single-GPU",
            "training",
            "script",
            "as",
            "the",
            "insufficient",
            "GPU",
            "memory",
            "will",
            "break",
            "down",
            "the",
            "training",
            ".",
            "bash",
            "finetune/finetune_ds.sh",
            "``",
            "`",
            "Remember",
            "to",
            "specify",
            "the",
            "correct",
            "model",
            "name",
            "or",
            "path",
            ",",
            "the",
            "data",
            "path",
            ",",
            "as",
            "well",
            "as",
            "the",
            "output",
            "directory",
            "in",
            "the",
            "shell",
            "scripts",
            ".",
            "Another",
            "thing",
            "to",
            "notice",
            "is",
            "that",
            "we",
            "use",
            "DeepSpeed",
            "ZeRO",
            "3",
            "in",
            "this",
            "script",
            ".",
            "If",
            "you",
            "want",
            "to",
            "make",
            "changes",
            ",",
            "just",
            "remove",
            "the",
            "argument",
            "`",
            "--",
            "deepspeed",
            "`",
            "or",
            "make",
            "changes",
            "in",
            "the",
            "DeepSpeed",
            "configuration",
            "json",
            "file",
            "based",
            "on",
            "your",
            "requirements",
            ".",
            "Additionally",
            ",",
            "this",
            "script",
            "supports",
            "mixed-precision",
            "training",
            ",",
            "and",
            "thus",
            "you",
            "can",
            "use",
            "`",
            "--",
            "bf16",
            "True",
            "`",
            "or",
            "`",
            "--",
            "fp16",
            "True",
            "`",
            ".",
            "Remember",
            "to",
            "use",
            "DeepSpeed",
            "when",
            "you",
            "use",
            "fp16",
            "due",
            "to",
            "mixed",
            "precision",
            "training",
            ".",
            "Empirically",
            "we",
            "advise",
            "you",
            "to",
            "use",
            "bf16",
            "to",
            "make",
            "your",
            "training",
            "consistent",
            "with",
            "our",
            "pretraining",
            "and",
            "alignment",
            "if",
            "your",
            "machine",
            "supports",
            "bf16",
            ",",
            "and",
            "thus",
            "we",
            "use",
            "it",
            "by",
            "default",
            ".",
            "Similarly",
            ",",
            "to",
            "run",
            "LoRA",
            ",",
            "use",
            "another",
            "script",
            "to",
            "run",
            "as",
            "shown",
            "below",
            ".",
            "Before",
            "you",
            "start",
            ",",
            "make",
            "sure",
            "that",
            "you",
            "have",
            "installed",
            "`",
            "peft",
            "`",
            ".",
            "Also",
            ",",
            "you",
            "need",
            "to",
            "specify",
            "your",
            "paths",
            "to",
            "your",
            "model",
            ",",
            "data",
            ",",
            "and",
            "output",
            ".",
            "We",
            "advise",
            "you",
            "to",
            "use",
            "absolute",
            "path",
            "for",
            "your",
            "pretrained",
            "model",
            ".",
            "This",
            "is",
            "because",
            "LoRA",
            "only",
            "saves",
            "the",
            "adapter",
            "and",
            "the",
            "absolute",
            "path",
            "in",
            "the",
            "adapter",
            "configuration",
            "json",
            "file",
            "is",
            "used",
            "for",
            "finding",
            "out",
            "the",
            "pretrained",
            "model",
            "to",
            "load",
            ".",
            "Also",
            ",",
            "this",
            "script",
            "support",
            "both",
            "bf16",
            "and",
            "fp16",
            ".",
            "``",
            "`",
            "bash"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/QwenLM/Qwen",
        "readme_url": "https://raw.githubusercontent.com/QwenLM/Qwen/main/README.md",
        "topic": [
            "chinese",
            "flash-attention",
            "large-language-models",
            "llm",
            "natural-language-processing",
            "pretrained-models"
        ],
        "text": "Distributed training\nbash finetune/finetune_lora_ds.sh\n```\n\nIn comparison with full-parameter finetuning, LoRA ([paper](https://arxiv.org/abs/2106.09685)) only updates the parameters of adapter layers but keeps the original large language model layers frozen. This allows much fewer memory costs and thus fewer computation costs. \n\nNote that if you use LoRA to finetune the base language model, e.g., Qwen-7B, instead of chat models, e.g., Qwen-7B-Chat, the script automatically switches the embedding and output layer as trainable parameters. This is because the base language model has no knowledge of special tokens brought by ChatML format. Thus these layers should be updated for the model to understand and predict the tokens. Or in another word, if your training brings in special tokens in LoRA, you should set the layers to trainable parameters by setting `modules_to_save` inside the code. Also, if we have these parameters trainable, it is not available to use ZeRO 3, and this is why we use ZeRO 2 in the script by default. If you do not have new trainable parameters, you can switch to ZeRO 3 by changing the DeepSpeed configuration file. Additionally, we find that there is a significant gap between the memory footprint of LoRA with and without these trainable parameters. Therefore, if you have trouble with memory, we advise you to LoRA finetune the chat models. Check the profile below for more information. \n\nIf you still suffer from insufficient memory, you can consider Q-LoRA ([paper](https://arxiv.org/abs/2305.14314)), which uses the quantized large language model and other techniques such as paged attention to allow even fewer memory costs. \n\nNote: to run single-GPU Q-LoRA training, you may need to install `mpi4py` through `pip` or `conda`.\n\nTo run Q-LoRA, directly run the following script:\n\n```bash\n",
        "token": [
            "Distributed",
            "training",
            "bash",
            "finetune/finetune_lora_ds.sh",
            "``",
            "`",
            "In",
            "comparison",
            "with",
            "full-parameter",
            "finetuning",
            ",",
            "LoRA",
            "(",
            "[",
            "paper",
            "]",
            "(",
            "https",
            ":",
            "//arxiv.org/abs/2106.09685",
            ")",
            ")",
            "only",
            "updates",
            "the",
            "parameters",
            "of",
            "adapter",
            "layers",
            "but",
            "keeps",
            "the",
            "original",
            "large",
            "language",
            "model",
            "layers",
            "frozen",
            ".",
            "This",
            "allows",
            "much",
            "fewer",
            "memory",
            "costs",
            "and",
            "thus",
            "fewer",
            "computation",
            "costs",
            ".",
            "Note",
            "that",
            "if",
            "you",
            "use",
            "LoRA",
            "to",
            "finetune",
            "the",
            "base",
            "language",
            "model",
            ",",
            "e.g.",
            ",",
            "Qwen-7B",
            ",",
            "instead",
            "of",
            "chat",
            "models",
            ",",
            "e.g.",
            ",",
            "Qwen-7B-Chat",
            ",",
            "the",
            "script",
            "automatically",
            "switches",
            "the",
            "embedding",
            "and",
            "output",
            "layer",
            "as",
            "trainable",
            "parameters",
            ".",
            "This",
            "is",
            "because",
            "the",
            "base",
            "language",
            "model",
            "has",
            "no",
            "knowledge",
            "of",
            "special",
            "tokens",
            "brought",
            "by",
            "ChatML",
            "format",
            ".",
            "Thus",
            "these",
            "layers",
            "should",
            "be",
            "updated",
            "for",
            "the",
            "model",
            "to",
            "understand",
            "and",
            "predict",
            "the",
            "tokens",
            ".",
            "Or",
            "in",
            "another",
            "word",
            ",",
            "if",
            "your",
            "training",
            "brings",
            "in",
            "special",
            "tokens",
            "in",
            "LoRA",
            ",",
            "you",
            "should",
            "set",
            "the",
            "layers",
            "to",
            "trainable",
            "parameters",
            "by",
            "setting",
            "`",
            "modules_to_save",
            "`",
            "inside",
            "the",
            "code",
            ".",
            "Also",
            ",",
            "if",
            "we",
            "have",
            "these",
            "parameters",
            "trainable",
            ",",
            "it",
            "is",
            "not",
            "available",
            "to",
            "use",
            "ZeRO",
            "3",
            ",",
            "and",
            "this",
            "is",
            "why",
            "we",
            "use",
            "ZeRO",
            "2",
            "in",
            "the",
            "script",
            "by",
            "default",
            ".",
            "If",
            "you",
            "do",
            "not",
            "have",
            "new",
            "trainable",
            "parameters",
            ",",
            "you",
            "can",
            "switch",
            "to",
            "ZeRO",
            "3",
            "by",
            "changing",
            "the",
            "DeepSpeed",
            "configuration",
            "file",
            ".",
            "Additionally",
            ",",
            "we",
            "find",
            "that",
            "there",
            "is",
            "a",
            "significant",
            "gap",
            "between",
            "the",
            "memory",
            "footprint",
            "of",
            "LoRA",
            "with",
            "and",
            "without",
            "these",
            "trainable",
            "parameters",
            ".",
            "Therefore",
            ",",
            "if",
            "you",
            "have",
            "trouble",
            "with",
            "memory",
            ",",
            "we",
            "advise",
            "you",
            "to",
            "LoRA",
            "finetune",
            "the",
            "chat",
            "models",
            ".",
            "Check",
            "the",
            "profile",
            "below",
            "for",
            "more",
            "information",
            ".",
            "If",
            "you",
            "still",
            "suffer",
            "from",
            "insufficient",
            "memory",
            ",",
            "you",
            "can",
            "consider",
            "Q-LoRA",
            "(",
            "[",
            "paper",
            "]",
            "(",
            "https",
            ":",
            "//arxiv.org/abs/2305.14314",
            ")",
            ")",
            ",",
            "which",
            "uses",
            "the",
            "quantized",
            "large",
            "language",
            "model",
            "and",
            "other",
            "techniques",
            "such",
            "as",
            "paged",
            "attention",
            "to",
            "allow",
            "even",
            "fewer",
            "memory",
            "costs",
            ".",
            "Note",
            ":",
            "to",
            "run",
            "single-GPU",
            "Q-LoRA",
            "training",
            ",",
            "you",
            "may",
            "need",
            "to",
            "install",
            "`",
            "mpi4py",
            "`",
            "through",
            "`",
            "pip",
            "`",
            "or",
            "`",
            "conda",
            "`",
            ".",
            "To",
            "run",
            "Q-LoRA",
            ",",
            "directly",
            "run",
            "the",
            "following",
            "script",
            ":",
            "``",
            "`",
            "bash"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/QwenLM/Qwen",
        "readme_url": "https://raw.githubusercontent.com/QwenLM/Qwen/main/README.md",
        "topic": [
            "chinese",
            "flash-attention",
            "large-language-models",
            "llm",
            "natural-language-processing",
            "pretrained-models"
        ],
        "text": "Quantize Fine-tuned Models\n\nThis section applies to full-parameter/LoRA fine-tuned models. (Note: You do not need to quantize the Q-LoRA fine-tuned model because it is already quantized.)\nIf you use LoRA, please follow the above instructions to merge your model before quantization. \n\nWe recommend using [auto_gptq](https://github.com/PanQiWei/AutoGPTQ) to quantize the finetuned model. \n\n```bash\npip install auto-gptq optimum\n```\n\nNote: Currently AutoGPTQ has a bug referred in [this issue](https://github.com/PanQiWei/AutoGPTQ/issues/370). Here is a [workaround PR](https://github.com/PanQiWei/AutoGPTQ/pull/495), and you can pull this branch and install from the source.\n\nFirst, prepare the calibration data. You can reuse the fine-tuning data, or use other data following the same format.\n\nSecond, run the following script:\n\n```bash\npython run_gptq.py \\\n    --model_name_or_path $YOUR_LORA_MODEL_PATH \\\n    --data_path $DATA \\\n    --out_path $OUTPUT_PATH \\\n    --bits 4 ",
        "token": [
            "Quantize",
            "Fine-tuned",
            "Models",
            "This",
            "section",
            "applies",
            "to",
            "full-parameter/LoRA",
            "fine-tuned",
            "models",
            ".",
            "(",
            "Note",
            ":",
            "You",
            "do",
            "not",
            "need",
            "to",
            "quantize",
            "the",
            "Q-LoRA",
            "fine-tuned",
            "model",
            "because",
            "it",
            "is",
            "already",
            "quantized",
            ".",
            ")",
            "If",
            "you",
            "use",
            "LoRA",
            ",",
            "please",
            "follow",
            "the",
            "above",
            "instructions",
            "to",
            "merge",
            "your",
            "model",
            "before",
            "quantization",
            ".",
            "We",
            "recommend",
            "using",
            "[",
            "auto_gptq",
            "]",
            "(",
            "https",
            ":",
            "//github.com/PanQiWei/AutoGPTQ",
            ")",
            "to",
            "quantize",
            "the",
            "finetuned",
            "model",
            ".",
            "``",
            "`",
            "bash",
            "pip",
            "install",
            "auto-gptq",
            "optimum",
            "``",
            "`",
            "Note",
            ":",
            "Currently",
            "AutoGPTQ",
            "has",
            "a",
            "bug",
            "referred",
            "in",
            "[",
            "this",
            "issue",
            "]",
            "(",
            "https",
            ":",
            "//github.com/PanQiWei/AutoGPTQ/issues/370",
            ")",
            ".",
            "Here",
            "is",
            "a",
            "[",
            "workaround",
            "PR",
            "]",
            "(",
            "https",
            ":",
            "//github.com/PanQiWei/AutoGPTQ/pull/495",
            ")",
            ",",
            "and",
            "you",
            "can",
            "pull",
            "this",
            "branch",
            "and",
            "install",
            "from",
            "the",
            "source",
            ".",
            "First",
            ",",
            "prepare",
            "the",
            "calibration",
            "data",
            ".",
            "You",
            "can",
            "reuse",
            "the",
            "fine-tuning",
            "data",
            ",",
            "or",
            "use",
            "other",
            "data",
            "following",
            "the",
            "same",
            "format",
            ".",
            "Second",
            ",",
            "run",
            "the",
            "following",
            "script",
            ":",
            "``",
            "`",
            "bash",
            "python",
            "run_gptq.py",
            "\\",
            "--",
            "model_name_or_path",
            "$",
            "YOUR_LORA_MODEL_PATH",
            "\\",
            "--",
            "data_path",
            "$",
            "DATA",
            "\\",
            "--",
            "out_path",
            "$",
            "OUTPUT_PATH",
            "\\",
            "--",
            "bits",
            "4"
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/QwenLM/Qwen",
        "readme_url": "https://raw.githubusercontent.com/QwenLM/Qwen/main/README.md",
        "topic": [
            "chinese",
            "flash-attention",
            "large-language-models",
            "llm",
            "natural-language-processing",
            "pretrained-models"
        ],
        "text": "Profiling of Memory and Speed\nWe profile the GPU memory and training speed of both LoRA (LoRA (emb) refers to training the embedding and output layer, while LoRA has no trainable embedding and output layer) and Q-LoRA in the setup of single-GPU training. In this test, we experiment on a single A100-SXM4-80G GPU, and we use CUDA 11.8 and Pytorch 2.0. Flash attention 2 is applied. We uniformly use a batch size of 1 and gradient accumulation of 8. We profile the memory (GB) and speed (s/iter) of inputs of different lengths, namely 256, 512, 1024, 2048, 4096, and 8192. We also report the statistics of full-parameter finetuning with Qwen-7B on 2 A100 GPUs. We only report the statistics of 256, 512, and 1024 tokens due to the limitation of GPU memory. \n\nFor Qwen-7B, we also test the performance of multinode finetuning. We experiment using two servers, each containing two A100-SXM4-80G GPUs, and the rest of configurations are the same as other Qwen-7B experiments. The results of multinode finetuning are marked as LoRA (multinode) in the table.\n\nFor Qwen-72B, we experiment in two ways: 1) Lora fintuning + DeepSpeed ZeRO 3 on 4 A100-SXM4-80G GPUs and 2) QLora (int4) fine-tuning on a single A100-SXM4-80G GPU. Note that OOM occurs on 4 A100-SXM4-80G GPUs both with LoRA (emb) fine-tuning and LoRA fine-tuning without Deepspeed ZeRO 3 (you can pass `--deepspeed finetune/ds_config_zero3.json` to [`finetune/finetune_lora_ds.sh`](finetune/finetune_lora_ds.sh) to enable DeepSpeed ZeRO 3).\n\nThe statistics are listed below:\n\n<table>\n    <tr>\n      <th rowspan=\"2\">Model Size</th><th rowspan=\"2\">Method</th><th rowspan=\"2\">#Nodes</th><th rowspan=\"2\">#GPUs per node</th><th colspan=\"6\" align=\"center\">Sequence Length</th>\n    </tr>\n    <tr>\n        <th align=\"center\">256</th><th align=\"center\">512</th><th align=\"center\">1024</th><th align=\"center\">2048</th><th align=\"center\">4096</th><th align=\"center\">8192</th>\n    </tr>\n    <tr>\n        <th rowspan=\"4\">1.8B</th><td>LoRA</td>\n        <td>1</td><td>1</td>\n        <td align=\"center\">6.7G / 1.0s/it</td><td align=\"center\">7.4G / 1.0s/it</td><td align=\"center\">8.4G / 1.1s/it</td><td align=\"center\">11.0G / 1.7s/it</td><td align=\"center\">16.2G / 3.3s/it</td><td align=\"center\">21.8G / 6.8s/it</td>\n    </tr>\n    <tr>\n        <td>LoRA (emb)</td>\n        <td>1</td><td>1</td>\n        <td align=\"center\">13.7G / 1.0s/it</td><td align=\"center\">14.0G / 1.0s/it</td><td align=\"center\">14.0G / 1.1s/it</td><td align=\"center\">15.1G / 1.8s/it</td><td align=\"center\">19.7G / 3.4s/it</td><td align=\"center\">27.7G / 7.0s/it</td>\n    </tr>\n    <tr>\n        <td>Q-LoRA</td>\n        <td>1</td><td>1</td>\n        <td align=\"center\">5.8G / 1.4s/it</td><td align=\"center\">6.0G / 1.4s/it</td><td align=\"center\">6.6G / 1.4s/it</td><td align=\"center\">7.8G / 2.0s/it</td><td align=\"center\">10.2G / 3.4s/it</td><td align=\"center\">15.8G / 6.5s/it</td>\n    </tr>\n    <tr>\n        <td>Full-parameter</td>\n        <td>1</td><td>1</td>\n        <td align=\"center\">43.5G / 2.1s/it</td><td align=\"center\">43.5G / 2.2s/it</td><td align=\"center\">43.5G / 2.2s/it</td><td align=\"center\">43.5G / 2.3s/it</td><td align=\"center\">47.1G / 2.8s/it</td><td align=\"center\">48.3G / 5.6s/it</td>\n    </tr>\n    <tr>\n        <th rowspan=\"5\">7B</th>\n        <td>LoRA</td>\n        <td>1</td><td>1</td>\n        <td align=\"center\">20.1G / 1.2s/it</td><td align=\"center\">20.4G / 1.5s/it</td><td align=\"center\">21.5G / 2.8s/it</td><td align=\"center\">23.8G / 5.2s/it</td><td align=\"center\">29.7G / 10.1s/it</td><td align=\"center\">36.6G / 21.3s/it</td>\n    </tr>\n    <tr>\n        <td>LoRA (emb)</td>\n        <td>1</td><td>1</td>\n        <td align=\"center\">33.7G / 1.4s/it</td><td align=\"center\">34.1G / 1.6s/it</td><td align=\"center\">35.2G / 2.9s/it</td><td align=\"center\">35.1G / 5.3s/it</td><td align=\"center\">39.2G / 10.3s/it</td><td align=\"center\">48.5G / 21.7s/it</td>\n    </tr>\n    <tr>\n        <td>Q-LoRA</td>\n        <td>1</td><td>1</td>\n        <td align=\"center\">11.5G / 3.0s/it</td><td align=\"center\">11.5G / 3.0s/it</td><td align=\"center\">12.3G / 3.5s/it</td><td align=\"center\">13.9G / 7.0s/it</td><td align=\"center\">16.9G / 11.6s/it</td><td align=\"center\">23.5G / 22.3s/it</td>\n    </tr>\n    <tr>\n        <td>Full-parameter</td>\n<td>1</td><td>2</td>\n<td align=\"center\">139.2G / 4.0s/it</td><td align=\"center\">148.0G / 4.0s/it</td><td align=\"center\">162.0G / 4.5s/it</td><td align=\"center\">-</td><td align=\"center\">-</td><td align=\"center\">-</td>\n    </tr>\n    <tr>\n        <td>LoRA (multinode)</td>\n        <td>2</td><td>2</td>\n        <td align=\"center\">74.7G / 2.09s/it</td><td align=\"center\">77.6G / 3.16s/it</td><td align=\"center\">84.9G / 5.17s/it</td><td align=\"center\">95.1G / 9.25s/it</td><td align=\"center\">121.1G / 18.1s/it</td><td align=\"center\">155.5G / 37.4s/it</td>\n    </tr>\n    <tr>\n        <th rowspan=\"3\">14B</th>\n        <td>LoRA</td>\n        <td>1</td><td>1</td>\n        <td align=\"center\">34.6G / 1.6s/it</td><td align=\"center\">35.1G / 2.4s/it</td><td align=\"center\">35.3G / 4.4s/it</td><td align=\"center\">37.4G / 8.4s/it</td><td align=\"center\">42.5G / 17.0s/it</td><td align=\"center\">55.2G / 36.0s/it</td>\n    </tr>\n    <tr>\n        <td>LoRA (emb)</td>\n        <td>1</td><td>1</td>\n        <td align=\"center\">51.2 / 1.7s/it</td><td align=\"center\">51.1G / 2.6s/it</td><td align=\"center\">51.5G / 4.6s/it</td><td align=\"center\">54.1G / 8.6s/it</td><td align=\"center\">56.8G / 17.2s/it</td><td align=\"center\">67.7G / 36.3s/it</td>\n    </tr>\n    <tr>\n        <td>Q-LoRA</td>\n        <td>1</td><td>1</td>\n        <td align=\"center\">18.7G / 5.3s/it</td><td align=\"center\">18.4G / 6.3s/it</td><td align=\"center\">18.9G / 8.2s/it</td><td align=\"center\">19.9G / 11.8s/it</td><td align=\"center\">23.0G / 20.1s/it</td><td align=\"center\">27.9G / 38.3s/it</td>\n    </tr>\n    <tr>\n        <th rowspan=\"2\">72B</th>\n        <td>LoRA + Deepspeed Zero3</td>\n        <td>1</td><td>4</td>\n        <td align=\"center\">215.4G / 17.6s/it</td><td align=\"center\">217.7G / 20.5s/it</td><td align=\"center\">222.6G / 29.4s/it</td><td align=\"center\">228.8G / 45.7s/it</td><td align=\"center\">249.0G / 83.4s/it</td><td align=\"center\">289.2G / 161.5s/it</td>\n    </tr>\n    <tr>\n        <td>Q-LoRA</td>\n        <td>1</td><td>1</td>\n        <td align=\"center\">61.4G / 27.4s/it</td><td align=\"center\">61.4G / 31.5s/it</td><td align=\"center\">62.9G / 41.4s/it</td><td align=\"center\">64.1G / 59.5s/it</td><td align=\"center\">68.0G / 97.7s/it</td><td align=\"center\">75.6G / 179.8s/it</td>\n    </tr>\n</table>\n\n<br>\n\n",
        "token": [
            "Profiling",
            "of",
            "Memory",
            "and",
            "Speed",
            "We",
            "profile",
            "the",
            "GPU",
            "memory",
            "and",
            "training",
            "speed",
            "of",
            "both",
            "LoRA",
            "(",
            "LoRA",
            "(",
            "emb",
            ")",
            "refers",
            "to",
            "training",
            "the",
            "embedding",
            "and",
            "output",
            "layer",
            ",",
            "while",
            "LoRA",
            "has",
            "no",
            "trainable",
            "embedding",
            "and",
            "output",
            "layer",
            ")",
            "and",
            "Q-LoRA",
            "in",
            "the",
            "setup",
            "of",
            "single-GPU",
            "training",
            ".",
            "In",
            "this",
            "test",
            ",",
            "we",
            "experiment",
            "on",
            "a",
            "single",
            "A100-SXM4-80G",
            "GPU",
            ",",
            "and",
            "we",
            "use",
            "CUDA",
            "11.8",
            "and",
            "Pytorch",
            "2.0",
            ".",
            "Flash",
            "attention",
            "2",
            "is",
            "applied",
            ".",
            "We",
            "uniformly",
            "use",
            "a",
            "batch",
            "size",
            "of",
            "1",
            "and",
            "gradient",
            "accumulation",
            "of",
            "8",
            ".",
            "We",
            "profile",
            "the",
            "memory",
            "(",
            "GB",
            ")",
            "and",
            "speed",
            "(",
            "s/iter",
            ")",
            "of",
            "inputs",
            "of",
            "different",
            "lengths",
            ",",
            "namely",
            "256",
            ",",
            "512",
            ",",
            "1024",
            ",",
            "2048",
            ",",
            "4096",
            ",",
            "and",
            "8192",
            ".",
            "We",
            "also",
            "report",
            "the",
            "statistics",
            "of",
            "full-parameter",
            "finetuning",
            "with",
            "Qwen-7B",
            "on",
            "2",
            "A100",
            "GPUs",
            ".",
            "We",
            "only",
            "report",
            "the",
            "statistics",
            "of",
            "256",
            ",",
            "512",
            ",",
            "and",
            "1024",
            "tokens",
            "due",
            "to",
            "the",
            "limitation",
            "of",
            "GPU",
            "memory",
            ".",
            "For",
            "Qwen-7B",
            ",",
            "we",
            "also",
            "test",
            "the",
            "performance",
            "of",
            "multinode",
            "finetuning",
            ".",
            "We",
            "experiment",
            "using",
            "two",
            "servers",
            ",",
            "each",
            "containing",
            "two",
            "A100-SXM4-80G",
            "GPUs",
            ",",
            "and",
            "the",
            "rest",
            "of",
            "configurations",
            "are",
            "the",
            "same",
            "as",
            "other",
            "Qwen-7B",
            "experiments",
            ".",
            "The",
            "results",
            "of",
            "multinode",
            "finetuning",
            "are",
            "marked",
            "as",
            "LoRA",
            "(",
            "multinode",
            ")",
            "in",
            "the",
            "table",
            ".",
            "For",
            "Qwen-72B",
            ",",
            "we",
            "experiment",
            "in",
            "two",
            "ways",
            ":",
            "1",
            ")",
            "Lora",
            "fintuning",
            "+",
            "DeepSpeed",
            "ZeRO",
            "3",
            "on",
            "4",
            "A100-SXM4-80G",
            "GPUs",
            "and",
            "2",
            ")",
            "QLora",
            "(",
            "int4",
            ")",
            "fine-tuning",
            "on",
            "a",
            "single",
            "A100-SXM4-80G",
            "GPU",
            ".",
            "Note",
            "that",
            "OOM",
            "occurs",
            "on",
            "4",
            "A100-SXM4-80G",
            "GPUs",
            "both",
            "with",
            "LoRA",
            "(",
            "emb",
            ")",
            "fine-tuning",
            "and",
            "LoRA",
            "fine-tuning",
            "without",
            "Deepspeed",
            "ZeRO",
            "3",
            "(",
            "you",
            "can",
            "pass",
            "`",
            "--",
            "deepspeed",
            "finetune/ds_config_zero3.json",
            "`",
            "to",
            "[",
            "`",
            "finetune/finetune_lora_ds.sh",
            "`",
            "]",
            "(",
            "finetune/finetune_lora_ds.sh",
            ")",
            "to",
            "enable",
            "DeepSpeed",
            "ZeRO",
            "3",
            ")",
            ".",
            "The",
            "statistics",
            "are",
            "listed",
            "below",
            ":",
            "<",
            "table",
            ">",
            "<",
            "tr",
            ">",
            "<",
            "th",
            "rowspan=",
            "''",
            "2",
            "''",
            ">",
            "Model",
            "Size",
            "<",
            "/th",
            ">",
            "<",
            "th",
            "rowspan=",
            "''",
            "2",
            "''",
            ">",
            "Method",
            "<",
            "/th",
            ">",
            "<",
            "th",
            "rowspan=",
            "''",
            "2",
            "''",
            ">",
            "#",
            "Nodes",
            "<",
            "/th",
            ">",
            "<",
            "th",
            "rowspan=",
            "''",
            "2",
            "''",
            ">",
            "#",
            "GPUs",
            "per",
            "node",
            "<",
            "/th",
            ">",
            "<",
            "th",
            "colspan=",
            "''",
            "6",
            "''",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "Sequence",
            "Length",
            "<",
            "/th",
            ">",
            "<",
            "/tr",
            ">",
            "<",
            "tr",
            ">",
            "<",
            "th",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "256",
            "<",
            "/th",
            ">",
            "<",
            "th",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "512",
            "<",
            "/th",
            ">",
            "<",
            "th",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "1024",
            "<",
            "/th",
            ">",
            "<",
            "th",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "2048",
            "<",
            "/th",
            ">",
            "<",
            "th",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "4096",
            "<",
            "/th",
            ">",
            "<",
            "th",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "8192",
            "<",
            "/th",
            ">",
            "<",
            "/tr",
            ">",
            "<",
            "tr",
            ">",
            "<",
            "th",
            "rowspan=",
            "''",
            "4",
            "''",
            ">",
            "1.8B",
            "<",
            "/th",
            ">",
            "<",
            "td",
            ">",
            "LoRA",
            "<",
            "/td",
            ">",
            "<",
            "td",
            ">",
            "1",
            "<",
            "/td",
            ">",
            "<",
            "td",
            ">",
            "1",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "6.7G",
            "/",
            "1.0s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "7.4G",
            "/",
            "1.0s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "8.4G",
            "/",
            "1.1s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "11.0G",
            "/",
            "1.7s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "16.2G",
            "/",
            "3.3s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "21.8G",
            "/",
            "6.8s/it",
            "<",
            "/td",
            ">",
            "<",
            "/tr",
            ">",
            "<",
            "tr",
            ">",
            "<",
            "td",
            ">",
            "LoRA",
            "(",
            "emb",
            ")",
            "<",
            "/td",
            ">",
            "<",
            "td",
            ">",
            "1",
            "<",
            "/td",
            ">",
            "<",
            "td",
            ">",
            "1",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "13.7G",
            "/",
            "1.0s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "14.0G",
            "/",
            "1.0s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "14.0G",
            "/",
            "1.1s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "15.1G",
            "/",
            "1.8s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "19.7G",
            "/",
            "3.4s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "27.7G",
            "/",
            "7.0s/it",
            "<",
            "/td",
            ">",
            "<",
            "/tr",
            ">",
            "<",
            "tr",
            ">",
            "<",
            "td",
            ">",
            "Q-LoRA",
            "<",
            "/td",
            ">",
            "<",
            "td",
            ">",
            "1",
            "<",
            "/td",
            ">",
            "<",
            "td",
            ">",
            "1",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "5.8G",
            "/",
            "1.4s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "6.0G",
            "/",
            "1.4s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "6.6G",
            "/",
            "1.4s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "7.8G",
            "/",
            "2.0s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "10.2G",
            "/",
            "3.4s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "15.8G",
            "/",
            "6.5s/it",
            "<",
            "/td",
            ">",
            "<",
            "/tr",
            ">",
            "<",
            "tr",
            ">",
            "<",
            "td",
            ">",
            "Full-parameter",
            "<",
            "/td",
            ">",
            "<",
            "td",
            ">",
            "1",
            "<",
            "/td",
            ">",
            "<",
            "td",
            ">",
            "1",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "43.5G",
            "/",
            "2.1s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "43.5G",
            "/",
            "2.2s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "43.5G",
            "/",
            "2.2s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "43.5G",
            "/",
            "2.3s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "47.1G",
            "/",
            "2.8s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "48.3G",
            "/",
            "5.6s/it",
            "<",
            "/td",
            ">",
            "<",
            "/tr",
            ">",
            "<",
            "tr",
            ">",
            "<",
            "th",
            "rowspan=",
            "''",
            "5",
            "''",
            ">",
            "7B",
            "<",
            "/th",
            ">",
            "<",
            "td",
            ">",
            "LoRA",
            "<",
            "/td",
            ">",
            "<",
            "td",
            ">",
            "1",
            "<",
            "/td",
            ">",
            "<",
            "td",
            ">",
            "1",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "20.1G",
            "/",
            "1.2s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "20.4G",
            "/",
            "1.5s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "21.5G",
            "/",
            "2.8s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "23.8G",
            "/",
            "5.2s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "29.7G",
            "/",
            "10.1s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "36.6G",
            "/",
            "21.3s/it",
            "<",
            "/td",
            ">",
            "<",
            "/tr",
            ">",
            "<",
            "tr",
            ">",
            "<",
            "td",
            ">",
            "LoRA",
            "(",
            "emb",
            ")",
            "<",
            "/td",
            ">",
            "<",
            "td",
            ">",
            "1",
            "<",
            "/td",
            ">",
            "<",
            "td",
            ">",
            "1",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "33.7G",
            "/",
            "1.4s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "34.1G",
            "/",
            "1.6s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "35.2G",
            "/",
            "2.9s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "35.1G",
            "/",
            "5.3s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "39.2G",
            "/",
            "10.3s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "48.5G",
            "/",
            "21.7s/it",
            "<",
            "/td",
            ">",
            "<",
            "/tr",
            ">",
            "<",
            "tr",
            ">",
            "<",
            "td",
            ">",
            "Q-LoRA",
            "<",
            "/td",
            ">",
            "<",
            "td",
            ">",
            "1",
            "<",
            "/td",
            ">",
            "<",
            "td",
            ">",
            "1",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "11.5G",
            "/",
            "3.0s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "11.5G",
            "/",
            "3.0s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "12.3G",
            "/",
            "3.5s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "13.9G",
            "/",
            "7.0s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "16.9G",
            "/",
            "11.6s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "23.5G",
            "/",
            "22.3s/it",
            "<",
            "/td",
            ">",
            "<",
            "/tr",
            ">",
            "<",
            "tr",
            ">",
            "<",
            "td",
            ">",
            "Full-parameter",
            "<",
            "/td",
            ">",
            "<",
            "td",
            ">",
            "1",
            "<",
            "/td",
            ">",
            "<",
            "td",
            ">",
            "2",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "139.2G",
            "/",
            "4.0s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "148.0G",
            "/",
            "4.0s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "162.0G",
            "/",
            "4.5s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "-",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "-",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "-",
            "<",
            "/td",
            ">",
            "<",
            "/tr",
            ">",
            "<",
            "tr",
            ">",
            "<",
            "td",
            ">",
            "LoRA",
            "(",
            "multinode",
            ")",
            "<",
            "/td",
            ">",
            "<",
            "td",
            ">",
            "2",
            "<",
            "/td",
            ">",
            "<",
            "td",
            ">",
            "2",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "74.7G",
            "/",
            "2.09s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "77.6G",
            "/",
            "3.16s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "84.9G",
            "/",
            "5.17s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "95.1G",
            "/",
            "9.25s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "121.1G",
            "/",
            "18.1s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "155.5G",
            "/",
            "37.4s/it",
            "<",
            "/td",
            ">",
            "<",
            "/tr",
            ">",
            "<",
            "tr",
            ">",
            "<",
            "th",
            "rowspan=",
            "''",
            "3",
            "''",
            ">",
            "14B",
            "<",
            "/th",
            ">",
            "<",
            "td",
            ">",
            "LoRA",
            "<",
            "/td",
            ">",
            "<",
            "td",
            ">",
            "1",
            "<",
            "/td",
            ">",
            "<",
            "td",
            ">",
            "1",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "34.6G",
            "/",
            "1.6s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "35.1G",
            "/",
            "2.4s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "35.3G",
            "/",
            "4.4s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "37.4G",
            "/",
            "8.4s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "42.5G",
            "/",
            "17.0s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "55.2G",
            "/",
            "36.0s/it",
            "<",
            "/td",
            ">",
            "<",
            "/tr",
            ">",
            "<",
            "tr",
            ">",
            "<",
            "td",
            ">",
            "LoRA",
            "(",
            "emb",
            ")",
            "<",
            "/td",
            ">",
            "<",
            "td",
            ">",
            "1",
            "<",
            "/td",
            ">",
            "<",
            "td",
            ">",
            "1",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "51.2",
            "/",
            "1.7s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "51.1G",
            "/",
            "2.6s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "51.5G",
            "/",
            "4.6s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "54.1G",
            "/",
            "8.6s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "56.8G",
            "/",
            "17.2s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "67.7G",
            "/",
            "36.3s/it",
            "<",
            "/td",
            ">",
            "<",
            "/tr",
            ">",
            "<",
            "tr",
            ">",
            "<",
            "td",
            ">",
            "Q-LoRA",
            "<",
            "/td",
            ">",
            "<",
            "td",
            ">",
            "1",
            "<",
            "/td",
            ">",
            "<",
            "td",
            ">",
            "1",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "18.7G",
            "/",
            "5.3s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "18.4G",
            "/",
            "6.3s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "18.9G",
            "/",
            "8.2s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "19.9G",
            "/",
            "11.8s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "23.0G",
            "/",
            "20.1s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "27.9G",
            "/",
            "38.3s/it",
            "<",
            "/td",
            ">",
            "<",
            "/tr",
            ">",
            "<",
            "tr",
            ">",
            "<",
            "th",
            "rowspan=",
            "''",
            "2",
            "''",
            ">",
            "72B",
            "<",
            "/th",
            ">",
            "<",
            "td",
            ">",
            "LoRA",
            "+",
            "Deepspeed",
            "Zero3",
            "<",
            "/td",
            ">",
            "<",
            "td",
            ">",
            "1",
            "<",
            "/td",
            ">",
            "<",
            "td",
            ">",
            "4",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "215.4G",
            "/",
            "17.6s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "217.7G",
            "/",
            "20.5s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "222.6G",
            "/",
            "29.4s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "228.8G",
            "/",
            "45.7s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "249.0G",
            "/",
            "83.4s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "289.2G",
            "/",
            "161.5s/it",
            "<",
            "/td",
            ">",
            "<",
            "/tr",
            ">",
            "<",
            "tr",
            ">",
            "<",
            "td",
            ">",
            "Q-LoRA",
            "<",
            "/td",
            ">",
            "<",
            "td",
            ">",
            "1",
            "<",
            "/td",
            ">",
            "<",
            "td",
            ">",
            "1",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "61.4G",
            "/",
            "27.4s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "61.4G",
            "/",
            "31.5s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "62.9G",
            "/",
            "41.4s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "64.1G",
            "/",
            "59.5s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "68.0G",
            "/",
            "97.7s/it",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "75.6G",
            "/",
            "179.8s/it",
            "<",
            "/td",
            ">",
            "<",
            "/tr",
            ">",
            "<",
            "/table",
            ">",
            "<",
            "br",
            ">"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/QwenLM/Qwen",
        "readme_url": "https://raw.githubusercontent.com/QwenLM/Qwen/main/README.md",
        "topic": [
            "chinese",
            "flash-attention",
            "large-language-models",
            "llm",
            "natural-language-processing",
            "pretrained-models"
        ],
        "text": "vLLM \n\nFor deployment and fast inference, we suggest using vLLM. \n\nIf you use cuda 12.1 and pytorch 2.1, you can directly use the following command to install vLLM.\n\n```bash\n",
        "token": [
            "vLLM",
            "For",
            "deployment",
            "and",
            "fast",
            "inference",
            ",",
            "we",
            "suggest",
            "using",
            "vLLM",
            ".",
            "If",
            "you",
            "use",
            "cuda",
            "12.1",
            "and",
            "pytorch",
            "2.1",
            ",",
            "you",
            "can",
            "directly",
            "use",
            "the",
            "following",
            "command",
            "to",
            "install",
            "vLLM",
            ".",
            "``",
            "`",
            "bash"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/QwenLM/Qwen",
        "readme_url": "https://raw.githubusercontent.com/QwenLM/Qwen/main/README.md",
        "topic": [
            "chinese",
            "flash-attention",
            "large-language-models",
            "llm",
            "natural-language-processing",
            "pretrained-models"
        ],
        "text": "pip install vllm  ",
        "token": [
            "pip",
            "install",
            "vllm"
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/QwenLM/Qwen",
        "readme_url": "https://raw.githubusercontent.com/QwenLM/Qwen/main/README.md",
        "topic": [
            "chinese",
            "flash-attention",
            "large-language-models",
            "llm",
            "natural-language-processing",
            "pretrained-models"
        ],
        "text": "The below lines support int4 quantization (int8 will be supported soon). The installation are slower (~10 minutes).\ngit clone https://github.com/QwenLM/vllm-gptq\ncd vllm-gptq\npip install -e .\n```\n\nOtherwise, please refer to the official vLLM [Installation Instructions](https://docs.vllm.ai/en/latest/getting_started/installation.html), or our [vLLM repo for GPTQ quantization](https://github.com/QwenLM/vllm-gptq).\n\n",
        "token": [
            "The",
            "below",
            "lines",
            "support",
            "int4",
            "quantization",
            "(",
            "int8",
            "will",
            "be",
            "supported",
            "soon",
            ")",
            ".",
            "The",
            "installation",
            "are",
            "slower",
            "(",
            "~10",
            "minutes",
            ")",
            ".",
            "git",
            "clone",
            "https",
            ":",
            "//github.com/QwenLM/vllm-gptq",
            "cd",
            "vllm-gptq",
            "pip",
            "install",
            "-e",
            ".",
            "``",
            "`",
            "Otherwise",
            ",",
            "please",
            "refer",
            "to",
            "the",
            "official",
            "vLLM",
            "[",
            "Installation",
            "Instructions",
            "]",
            "(",
            "https",
            ":",
            "//docs.vllm.ai/en/latest/getting_started/installation.html",
            ")",
            ",",
            "or",
            "our",
            "[",
            "vLLM",
            "repo",
            "for",
            "GPTQ",
            "quantization",
            "]",
            "(",
            "https",
            ":",
            "//github.com/QwenLM/vllm-gptq",
            ")",
            "."
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/QwenLM/Qwen",
        "readme_url": "https://raw.githubusercontent.com/QwenLM/Qwen/main/README.md",
        "topic": [
            "chinese",
            "flash-attention",
            "large-language-models",
            "llm",
            "natural-language-processing",
            "pretrained-models"
        ],
        "text": "vLLM + Web Demo / OpenAI-like API\n\nYou can use FastChat to lauch a web demo or an OpenAI API server. First, install FastChat:\n\n```bash\npip install \"fschat[model_worker,webui]\"\n```\n\nTo run Qwen with vLLM and FastChat, you need launch a controller by:\n```bash\npython -m fastchat.serve.controller\n```\n\nThen you can launch the model worker, which means loading your model for inference. For single GPU inference, you can directly run:\n```bash\npython -m fastchat.serve.vllm_worker --model-path $model_path --trust-remote-code --dtype bfloat16\n",
        "token": [
            "vLLM",
            "+",
            "Web",
            "Demo",
            "/",
            "OpenAI-like",
            "API",
            "You",
            "can",
            "use",
            "FastChat",
            "to",
            "lauch",
            "a",
            "web",
            "demo",
            "or",
            "an",
            "OpenAI",
            "API",
            "server",
            ".",
            "First",
            ",",
            "install",
            "FastChat",
            ":",
            "``",
            "`",
            "bash",
            "pip",
            "install",
            "``",
            "fschat",
            "[",
            "model_worker",
            ",",
            "webui",
            "]",
            "''",
            "``",
            "`",
            "To",
            "run",
            "Qwen",
            "with",
            "vLLM",
            "and",
            "FastChat",
            ",",
            "you",
            "need",
            "launch",
            "a",
            "controller",
            "by",
            ":",
            "``",
            "`",
            "bash",
            "python",
            "-m",
            "fastchat.serve.controller",
            "``",
            "`",
            "Then",
            "you",
            "can",
            "launch",
            "the",
            "model",
            "worker",
            ",",
            "which",
            "means",
            "loading",
            "your",
            "model",
            "for",
            "inference",
            ".",
            "For",
            "single",
            "GPU",
            "inference",
            ",",
            "you",
            "can",
            "directly",
            "run",
            ":",
            "``",
            "`",
            "bash",
            "python",
            "-m",
            "fastchat.serve.vllm_worker",
            "--",
            "model-path",
            "$",
            "model_path",
            "--",
            "trust-remote-code",
            "--",
            "dtype",
            "bfloat16"
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/QwenLM/Qwen",
        "readme_url": "https://raw.githubusercontent.com/QwenLM/Qwen/main/README.md",
        "topic": [
            "chinese",
            "flash-attention",
            "large-language-models",
            "llm",
            "natural-language-processing",
            "pretrained-models"
        ],
        "text": "Web UI\n\nWe provide code for users to build a web UI demo (thanks to @wysaid). Before you start, make sure you install the following packages:\n\n```\npip install -r requirements_web_demo.txt\n```\n\nThen run the command below and click on the generated link:\n\n```bash\npython web_demo.py\n```\n\n<p align=\"center\">\n    <br>\n    <img src=\"assets/web_demo.gif\" width=\"600\" />\n    <br>\n<p>\n\n",
        "token": [
            "Web",
            "UI",
            "We",
            "provide",
            "code",
            "for",
            "users",
            "to",
            "build",
            "a",
            "web",
            "UI",
            "demo",
            "(",
            "thanks",
            "to",
            "@",
            "wysaid",
            ")",
            ".",
            "Before",
            "you",
            "start",
            ",",
            "make",
            "sure",
            "you",
            "install",
            "the",
            "following",
            "packages",
            ":",
            "``",
            "`",
            "pip",
            "install",
            "-r",
            "requirements_web_demo.txt",
            "``",
            "`",
            "Then",
            "run",
            "the",
            "command",
            "below",
            "and",
            "click",
            "on",
            "the",
            "generated",
            "link",
            ":",
            "``",
            "`",
            "bash",
            "python",
            "web_demo.py",
            "``",
            "`",
            "<",
            "p",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "<",
            "br",
            ">",
            "<",
            "img",
            "src=",
            "''",
            "assets/web_demo.gif",
            "''",
            "width=",
            "''",
            "600",
            "''",
            "/",
            ">",
            "<",
            "br",
            ">",
            "<",
            "p",
            ">"
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/QwenLM/Qwen",
        "readme_url": "https://raw.githubusercontent.com/QwenLM/Qwen/main/README.md",
        "topic": [
            "chinese",
            "flash-attention",
            "large-language-models",
            "llm",
            "natural-language-processing",
            "pretrained-models"
        ],
        "text": "API\n\nWe provide methods to deploy local API based on OpenAI API (thanks to @hanpenggit). Before you start, install the required packages:\n\n```bash\npip install fastapi uvicorn \"openai<1.0\" pydantic sse_starlette\n```\n\nThen run the command to deploy your API:\n\n```bash\npython openai_api.py\n```\n\nYou can change your arguments, e.g., `-c` for checkpoint name or path, `--cpu-only` for CPU deployment, etc. If you meet problems launching your API deployment, updating the packages to the latest version can probably solve them.\n\nUsing the API is also simple. See the example below:\n\n```python\nimport openai\nopenai.api_base = \"http://localhost:8000/v1\"\nopenai.api_key = \"none\"\n\n",
        "token": [
            "API",
            "We",
            "provide",
            "methods",
            "to",
            "deploy",
            "local",
            "API",
            "based",
            "on",
            "OpenAI",
            "API",
            "(",
            "thanks",
            "to",
            "@",
            "hanpenggit",
            ")",
            ".",
            "Before",
            "you",
            "start",
            ",",
            "install",
            "the",
            "required",
            "packages",
            ":",
            "``",
            "`",
            "bash",
            "pip",
            "install",
            "fastapi",
            "uvicorn",
            "``",
            "openai",
            "<",
            "1.0",
            "''",
            "pydantic",
            "sse_starlette",
            "``",
            "`",
            "Then",
            "run",
            "the",
            "command",
            "to",
            "deploy",
            "your",
            "API",
            ":",
            "``",
            "`",
            "bash",
            "python",
            "openai_api.py",
            "``",
            "`",
            "You",
            "can",
            "change",
            "your",
            "arguments",
            ",",
            "e.g.",
            ",",
            "`",
            "-c",
            "`",
            "for",
            "checkpoint",
            "name",
            "or",
            "path",
            ",",
            "`",
            "--",
            "cpu-only",
            "`",
            "for",
            "CPU",
            "deployment",
            ",",
            "etc",
            ".",
            "If",
            "you",
            "meet",
            "problems",
            "launching",
            "your",
            "API",
            "deployment",
            ",",
            "updating",
            "the",
            "packages",
            "to",
            "the",
            "latest",
            "version",
            "can",
            "probably",
            "solve",
            "them",
            ".",
            "Using",
            "the",
            "API",
            "is",
            "also",
            "simple",
            ".",
            "See",
            "the",
            "example",
            "below",
            ":",
            "``",
            "`",
            "python",
            "import",
            "openai",
            "openai.api_base",
            "=",
            "``",
            "http",
            ":",
            "//localhost:8000/v1",
            "''",
            "openai.api_key",
            "=",
            "``",
            "none",
            "''"
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/QwenLM/Qwen",
        "readme_url": "https://raw.githubusercontent.com/QwenLM/Qwen/main/README.md",
        "topic": [
            "chinese",
            "flash-attention",
            "large-language-models",
            "llm",
            "natural-language-processing",
            "pretrained-models"
        ],
        "text": "\ud83d\udc33 Docker\n\nTo simplify the deployment process, we provide docker images with pre-built environments: [qwenllm/qwen](https://hub.docker.com/r/qwenllm/qwen). You only need to install the driver and download model files to launch demos, deploy OpenAI API, and finetune the model.\n\n",
        "token": [
            "\ud83d\udc33",
            "Docker",
            "To",
            "simplify",
            "the",
            "deployment",
            "process",
            ",",
            "we",
            "provide",
            "docker",
            "images",
            "with",
            "pre-built",
            "environments",
            ":",
            "[",
            "qwenllm/qwen",
            "]",
            "(",
            "https",
            ":",
            "//hub.docker.com/r/qwenllm/qwen",
            ")",
            ".",
            "You",
            "only",
            "need",
            "to",
            "install",
            "the",
            "driver",
            "and",
            "download",
            "model",
            "files",
            "to",
            "launch",
            "demos",
            ",",
            "deploy",
            "OpenAI",
            "API",
            ",",
            "and",
            "finetune",
            "the",
            "model",
            "."
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/QwenLM/Qwen",
        "readme_url": "https://raw.githubusercontent.com/QwenLM/Qwen/main/README.md",
        "topic": [
            "chinese",
            "flash-attention",
            "large-language-models",
            "llm",
            "natural-language-processing",
            "pretrained-models"
        ],
        "text": "Preparation\n\n1. Install the correct version of Nvidia driver depending on the image to use:\n  - `qwenllm/qwen:cu117` (**recommend**): `>= 515.48.07`\n  - `qwenllm/qwen:cu114` (w/o flash-attention): `>= 470.82.01`\n  - `qwenllm/qwen:cu121`: `>= 530.30.02`\n  - `qwenllm/qwen:latest`: same as `qwenllm/qwen:cu117`\n\n2. Install and configure [docker](https://docs.docker.com/engine/install/) and [nvidia-container-toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html):\n\n```bash\n",
        "token": [
            "Preparation",
            "1",
            ".",
            "Install",
            "the",
            "correct",
            "version",
            "of",
            "Nvidia",
            "driver",
            "depending",
            "on",
            "the",
            "image",
            "to",
            "use",
            ":",
            "-",
            "`",
            "qwenllm/qwen",
            ":",
            "cu117",
            "`",
            "(",
            "*",
            "*",
            "recommend",
            "*",
            "*",
            ")",
            ":",
            "`",
            ">",
            "=",
            "515.48.07",
            "`",
            "-",
            "`",
            "qwenllm/qwen",
            ":",
            "cu114",
            "`",
            "(",
            "w/o",
            "flash-attention",
            ")",
            ":",
            "`",
            ">",
            "=",
            "470.82.01",
            "`",
            "-",
            "`",
            "qwenllm/qwen",
            ":",
            "cu121",
            "`",
            ":",
            "`",
            ">",
            "=",
            "530.30.02",
            "`",
            "-",
            "`",
            "qwenllm/qwen",
            ":",
            "latest",
            "`",
            ":",
            "same",
            "as",
            "`",
            "qwenllm/qwen",
            ":",
            "cu117",
            "`",
            "2",
            ".",
            "Install",
            "and",
            "configure",
            "[",
            "docker",
            "]",
            "(",
            "https",
            ":",
            "//docs.docker.com/engine/install/",
            ")",
            "and",
            "[",
            "nvidia-container-toolkit",
            "]",
            "(",
            "https",
            ":",
            "//docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html",
            ")",
            ":",
            "``",
            "`",
            "bash"
        ],
        "level of complexity": 1
    },
    {
        "url": "https://github.com/QwenLM/Qwen",
        "readme_url": "https://raw.githubusercontent.com/QwenLM/Qwen/main/README.md",
        "topic": [
            "chinese",
            "flash-attention",
            "large-language-models",
            "llm",
            "natural-language-processing",
            "pretrained-models"
        ],
        "text": "test if docker is correctly installed\nsudo docker run hello-world\n\n",
        "token": [
            "test",
            "if",
            "docker",
            "is",
            "correctly",
            "installed",
            "sudo",
            "docker",
            "run",
            "hello-world"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/QwenLM/Qwen",
        "readme_url": "https://raw.githubusercontent.com/QwenLM/Qwen/main/README.md",
        "topic": [
            "chinese",
            "flash-attention",
            "large-language-models",
            "llm",
            "natural-language-processing",
            "pretrained-models"
        ],
        "text": "test if nvidia-container-toolkit is correctly installed\nsudo docker run --rm --runtime=nvidia --gpus all ubuntu nvidia-smi\n```\n\n3. Download model checkpoints and codes to your environment (see [here](#DownloadModel)).\n\n",
        "token": [
            "test",
            "if",
            "nvidia-container-toolkit",
            "is",
            "correctly",
            "installed",
            "sudo",
            "docker",
            "run",
            "--",
            "rm",
            "--",
            "runtime=nvidia",
            "--",
            "gpus",
            "all",
            "ubuntu",
            "nvidia-smi",
            "``",
            "`",
            "3",
            ".",
            "Download",
            "model",
            "checkpoints",
            "and",
            "codes",
            "to",
            "your",
            "environment",
            "(",
            "see",
            "[",
            "here",
            "]",
            "(",
            "#",
            "DownloadModel",
            ")",
            ")",
            "."
        ],
        "level of complexity": 1
    },
    {
        "url": "https://github.com/QwenLM/Qwen",
        "readme_url": "https://raw.githubusercontent.com/QwenLM/Qwen/main/README.md",
        "topic": [
            "chinese",
            "flash-attention",
            "large-language-models",
            "llm",
            "natural-language-processing",
            "pretrained-models"
        ],
        "text": "Deployment\n\nHere we use Qwen-7B-Chat as an example. Before launching a web demo or API, you can setup the configuration as shown below:\n\n```bash\nIMAGE_NAME=qwenllm/qwen:cu117\nPORT=8901\nCHECKPOINT_PATH=/path/to/Qwen-7B-Chat   ",
        "token": [
            "Deployment",
            "Here",
            "we",
            "use",
            "Qwen-7B-Chat",
            "as",
            "an",
            "example",
            ".",
            "Before",
            "launching",
            "a",
            "web",
            "demo",
            "or",
            "API",
            ",",
            "you",
            "can",
            "setup",
            "the",
            "configuration",
            "as",
            "shown",
            "below",
            ":",
            "``",
            "`",
            "bash",
            "IMAGE_NAME=qwenllm/qwen",
            ":",
            "cu117",
            "PORT=8901",
            "CHECKPOINT_PATH=/path/to/Qwen-7B-Chat"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/QwenLM/Qwen",
        "readme_url": "https://raw.githubusercontent.com/QwenLM/Qwen/main/README.md",
        "topic": [
            "chinese",
            "flash-attention",
            "large-language-models",
            "llm",
            "natural-language-processing",
            "pretrained-models"
        ],
        "text": "Finetuning\n\nThe method of finetuning using the pre-built Docker image is basically the same as [the above chapter](#Finetuning) (we have already installed dependencies in the image):\n\nThe following is an example of single-GPU LoRA:\n```bash\nIMAGE_NAME=qwenllm/qwen:cu117\nCHECKPOINT_PATH=/path/to/Qwen-7B                ",
        "token": [
            "Finetuning",
            "The",
            "method",
            "of",
            "finetuning",
            "using",
            "the",
            "pre-built",
            "Docker",
            "image",
            "is",
            "basically",
            "the",
            "same",
            "as",
            "[",
            "the",
            "above",
            "chapter",
            "]",
            "(",
            "#",
            "Finetuning",
            ")",
            "(",
            "we",
            "have",
            "already",
            "installed",
            "dependencies",
            "in",
            "the",
            "image",
            ")",
            ":",
            "The",
            "following",
            "is",
            "an",
            "example",
            "of",
            "single-GPU",
            "LoRA",
            ":",
            "``",
            "`",
            "bash",
            "IMAGE_NAME=qwenllm/qwen",
            ":",
            "cu117",
            "CHECKPOINT_PATH=/path/to/Qwen-7B"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/QwenLM/Qwen",
        "readme_url": "https://raw.githubusercontent.com/QwenLM/Qwen/main/README.md",
        "topic": [
            "chinese",
            "flash-attention",
            "large-language-models",
            "llm",
            "natural-language-processing",
            "pretrained-models"
        ],
        "text": "Tool Usage\n\nQwen-Chat has been optimized for tool usage and function calling capabilities. Users can develop agents, LangChain applications, and even augment Qwen with a Python Code Interpreter.\n\nWe provide documentation on how to implement tool calls based on the principle of ReAct Prompting, please refer to [the ReAct example](examples/react_prompt.md). Based on this principle, we provide support for function calling in [openai_api.py](openai_api.py).\n\nWe have tested the model's tool calling capabilities on our open-source Chinese evaluation benchmark and found that Qwen-Chat consistently performs well:\n\n<table>\n    <tr>\n        <th colspan=\"4\" align=\"center\">Chinese Tool-Use Benchmark (Version 20231206)</th>\n    </tr>\n    <tr>\n        <th align=\"center\">Model</th><th align=\"center\">Tool Selection (Acc.\u2191)</th><th align=\"center\">Tool Input (Rouge-L\u2191)</th><th align=\"center\">False Positive Error\u2193</th>\n    </tr>\n    <tr>\n        <td>GPT-4</td><td align=\"center\">98.0%</td><td align=\"center\">0.953</td><td align=\"center\">23.9%</td>\n    </tr>\n    <tr>\n        <td>GPT-3.5</td><td align=\"center\">74.5%</td><td align=\"center\">0.807</td><td align=\"center\">80.6%</td>\n    </tr>\n    <tr>\n        <td>Qwen-1_8B-Chat</td><td align=\"center\">85.0%</td><td align=\"center\">0.839</td><td align=\"center\">27.6%</td>\n    </tr>\n    <tr>\n        <td>Qwen-7B-Chat</td><td align=\"center\">95.5%</td><td align=\"center\">0.900</td><td align=\"center\">11.6%</td>\n    </tr>\n    <tr>\n        <td>Qwen-14B-Chat</td><td align=\"center\">96.9%</td><td align=\"center\">0.917</td><td align=\"center\">5.6%</td>\n    </tr>\n    <tr>\n        <td>Qwen-72B-Chat</td><td align=\"center\">98.2%</td><td align=\"center\">0.927</td><td align=\"center\">1.1%</td>\n    </tr>\n</table>\n\nTo assess Qwen's ability to use the Python Code Interpreter for tasks such as mathematical problem solving, data visualization, and other general-purpose tasks such as file handling and web scraping, we have created and open-sourced a benchmark specifically designed for evaluating these capabilities. You can find the benchmark at this [link](https://github.com/QwenLM/Qwen-Agent/tree/main/benchmark).\n\nWe have observed that Qwen performs well in terms of code executability and result accuracy when generating code:\n\n<table>\n    <tr>\n        <th colspan=\"5\" align=\"center\">Code Interpreter Benchmark (Version 20231206)</th>\n    </tr>\n    <tr>\n        <th rowspan=\"2\" align=\"center\">Model</th>\n        <th colspan=\"3\" align=\"center\">Accuracy of Code Execution Results (%)</th>\n        <th colspan=\"1\" align=\"center\">Executable Rate of Code (%)</th>\n    </tr>\n    <tr>\n        <th align=\"center\">Math\u2191</th><th align=\"center\">Visualization-Hard\u2191</th><th align=\"center\">Visualization-Easy\u2191</th><th align=\"center\">General\u2191</th>\n    </tr>\n    <tr>\n        <td>GPT-4</td>\n        <td align=\"center\">82.8</td>\n        <td align=\"center\">66.7</td>\n        <td align=\"center\">60.8</td>\n        <td align=\"center\">82.8</td>\n    </tr>\n    <tr>\n        <td>GPT-3.5</td>\n        <td align=\"center\">47.3</td>\n        <td align=\"center\">33.3</td>\n        <td align=\"center\">55.7</td>\n        <td align=\"center\">74.1</td>\n    </tr>\n    <tr>\n        <td>LLaMA2-13B-Chat</td>\n        <td align=\"center\">8.3</td>\n        <td align=\"center\">1.2</td>\n        <td align=\"center\">15.2</td>\n        <td align=\"center\">48.3</td>\n    </tr>\n    <tr>\n        <td>CodeLLaMA-13B-Instruct</td>\n        <td align=\"center\">28.2</td>\n        <td align=\"center\">15.5</td>\n        <td align=\"center\">21.5</td>\n        <td align=\"center\">74.1</td>\n    </tr>\n    <tr>\n        <td>InternLM-20B-Chat</td>\n        <td align=\"center\">34.6</td>\n        <td align=\"center\">10.7</td>\n        <td align=\"center\">25.1</td>\n        <td align=\"center\">65.5</td>\n    </tr>\n    <tr>\n        <td>ChatGLM3-6B</td>\n        <td align=\"center\">54.2</td>\n        <td align=\"center\">4.8</td>\n        <td align=\"center\">15.2</td>\n        <td align=\"center\">67.1</td>\n    </tr>\n    <tr>\n        <td>Qwen-1.8B-Chat</td>\n        <td align=\"center\">25.6</td>\n        <td align=\"center\">21.4</td>\n        <td align=\"center\">22.8</td>\n        <td align=\"center\">65.5</td>\n    </tr>\n    <tr>\n        <td>Qwen-7B-Chat</td>\n        <td align=\"center\">41.9</td>\n        <td align=\"center\">23.8</td>\n        <td align=\"center\">38.0</td>\n        <td align=\"center\">67.2</td>\n    </tr>\n    <tr>\n        <td>Qwen-14B-Chat</td>\n        <td align=\"center\">58.4</td>\n        <td align=\"center\">31.0</td>\n        <td align=\"center\">45.6</td>\n        <td align=\"center\">65.5</td>\n    </tr>\n    <tr>\n        <td>Qwen-72B-Chat</td>\n        <td align=\"center\">72.7</td>\n        <td align=\"center\">41.7</td>\n        <td align=\"center\">43.0</td>\n        <td align=\"center\">82.8</td>\n    </tr>\n</table>\n\n<p align=\"center\">\n    <br>\n    <img src=\"assets/code_interpreter_showcase_001.jpg\" />\n    <br>\n<p>\n\n<br>\n\n",
        "token": [
            "Tool",
            "Usage",
            "Qwen-Chat",
            "has",
            "been",
            "optimized",
            "for",
            "tool",
            "usage",
            "and",
            "function",
            "calling",
            "capabilities",
            ".",
            "Users",
            "can",
            "develop",
            "agents",
            ",",
            "LangChain",
            "applications",
            ",",
            "and",
            "even",
            "augment",
            "Qwen",
            "with",
            "a",
            "Python",
            "Code",
            "Interpreter",
            ".",
            "We",
            "provide",
            "documentation",
            "on",
            "how",
            "to",
            "implement",
            "tool",
            "calls",
            "based",
            "on",
            "the",
            "principle",
            "of",
            "ReAct",
            "Prompting",
            ",",
            "please",
            "refer",
            "to",
            "[",
            "the",
            "ReAct",
            "example",
            "]",
            "(",
            "examples/react_prompt.md",
            ")",
            ".",
            "Based",
            "on",
            "this",
            "principle",
            ",",
            "we",
            "provide",
            "support",
            "for",
            "function",
            "calling",
            "in",
            "[",
            "openai_api.py",
            "]",
            "(",
            "openai_api.py",
            ")",
            ".",
            "We",
            "have",
            "tested",
            "the",
            "model",
            "'s",
            "tool",
            "calling",
            "capabilities",
            "on",
            "our",
            "open-source",
            "Chinese",
            "evaluation",
            "benchmark",
            "and",
            "found",
            "that",
            "Qwen-Chat",
            "consistently",
            "performs",
            "well",
            ":",
            "<",
            "table",
            ">",
            "<",
            "tr",
            ">",
            "<",
            "th",
            "colspan=",
            "''",
            "4",
            "''",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "Chinese",
            "Tool-Use",
            "Benchmark",
            "(",
            "Version",
            "20231206",
            ")",
            "<",
            "/th",
            ">",
            "<",
            "/tr",
            ">",
            "<",
            "tr",
            ">",
            "<",
            "th",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "Model",
            "<",
            "/th",
            ">",
            "<",
            "th",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "Tool",
            "Selection",
            "(",
            "Acc.\u2191",
            ")",
            "<",
            "/th",
            ">",
            "<",
            "th",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "Tool",
            "Input",
            "(",
            "Rouge-L\u2191",
            ")",
            "<",
            "/th",
            ">",
            "<",
            "th",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "False",
            "Positive",
            "Error\u2193",
            "<",
            "/th",
            ">",
            "<",
            "/tr",
            ">",
            "<",
            "tr",
            ">",
            "<",
            "td",
            ">",
            "GPT-4",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "98.0",
            "%",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "0.953",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "23.9",
            "%",
            "<",
            "/td",
            ">",
            "<",
            "/tr",
            ">",
            "<",
            "tr",
            ">",
            "<",
            "td",
            ">",
            "GPT-3.5",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "74.5",
            "%",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "0.807",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "80.6",
            "%",
            "<",
            "/td",
            ">",
            "<",
            "/tr",
            ">",
            "<",
            "tr",
            ">",
            "<",
            "td",
            ">",
            "Qwen-1_8B-Chat",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "85.0",
            "%",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "0.839",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "27.6",
            "%",
            "<",
            "/td",
            ">",
            "<",
            "/tr",
            ">",
            "<",
            "tr",
            ">",
            "<",
            "td",
            ">",
            "Qwen-7B-Chat",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "95.5",
            "%",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "0.900",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "11.6",
            "%",
            "<",
            "/td",
            ">",
            "<",
            "/tr",
            ">",
            "<",
            "tr",
            ">",
            "<",
            "td",
            ">",
            "Qwen-14B-Chat",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "96.9",
            "%",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "0.917",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "5.6",
            "%",
            "<",
            "/td",
            ">",
            "<",
            "/tr",
            ">",
            "<",
            "tr",
            ">",
            "<",
            "td",
            ">",
            "Qwen-72B-Chat",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "98.2",
            "%",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "0.927",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "1.1",
            "%",
            "<",
            "/td",
            ">",
            "<",
            "/tr",
            ">",
            "<",
            "/table",
            ">",
            "To",
            "assess",
            "Qwen",
            "'s",
            "ability",
            "to",
            "use",
            "the",
            "Python",
            "Code",
            "Interpreter",
            "for",
            "tasks",
            "such",
            "as",
            "mathematical",
            "problem",
            "solving",
            ",",
            "data",
            "visualization",
            ",",
            "and",
            "other",
            "general-purpose",
            "tasks",
            "such",
            "as",
            "file",
            "handling",
            "and",
            "web",
            "scraping",
            ",",
            "we",
            "have",
            "created",
            "and",
            "open-sourced",
            "a",
            "benchmark",
            "specifically",
            "designed",
            "for",
            "evaluating",
            "these",
            "capabilities",
            ".",
            "You",
            "can",
            "find",
            "the",
            "benchmark",
            "at",
            "this",
            "[",
            "link",
            "]",
            "(",
            "https",
            ":",
            "//github.com/QwenLM/Qwen-Agent/tree/main/benchmark",
            ")",
            ".",
            "We",
            "have",
            "observed",
            "that",
            "Qwen",
            "performs",
            "well",
            "in",
            "terms",
            "of",
            "code",
            "executability",
            "and",
            "result",
            "accuracy",
            "when",
            "generating",
            "code",
            ":",
            "<",
            "table",
            ">",
            "<",
            "tr",
            ">",
            "<",
            "th",
            "colspan=",
            "''",
            "5",
            "''",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "Code",
            "Interpreter",
            "Benchmark",
            "(",
            "Version",
            "20231206",
            ")",
            "<",
            "/th",
            ">",
            "<",
            "/tr",
            ">",
            "<",
            "tr",
            ">",
            "<",
            "th",
            "rowspan=",
            "''",
            "2",
            "''",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "Model",
            "<",
            "/th",
            ">",
            "<",
            "th",
            "colspan=",
            "''",
            "3",
            "''",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "Accuracy",
            "of",
            "Code",
            "Execution",
            "Results",
            "(",
            "%",
            ")",
            "<",
            "/th",
            ">",
            "<",
            "th",
            "colspan=",
            "''",
            "1",
            "''",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "Executable",
            "Rate",
            "of",
            "Code",
            "(",
            "%",
            ")",
            "<",
            "/th",
            ">",
            "<",
            "/tr",
            ">",
            "<",
            "tr",
            ">",
            "<",
            "th",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "Math\u2191",
            "<",
            "/th",
            ">",
            "<",
            "th",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "Visualization-Hard\u2191",
            "<",
            "/th",
            ">",
            "<",
            "th",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "Visualization-Easy\u2191",
            "<",
            "/th",
            ">",
            "<",
            "th",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "General\u2191",
            "<",
            "/th",
            ">",
            "<",
            "/tr",
            ">",
            "<",
            "tr",
            ">",
            "<",
            "td",
            ">",
            "GPT-4",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "82.8",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "66.7",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "60.8",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "82.8",
            "<",
            "/td",
            ">",
            "<",
            "/tr",
            ">",
            "<",
            "tr",
            ">",
            "<",
            "td",
            ">",
            "GPT-3.5",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "47.3",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "33.3",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "55.7",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "74.1",
            "<",
            "/td",
            ">",
            "<",
            "/tr",
            ">",
            "<",
            "tr",
            ">",
            "<",
            "td",
            ">",
            "LLaMA2-13B-Chat",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "8.3",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "1.2",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "15.2",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "48.3",
            "<",
            "/td",
            ">",
            "<",
            "/tr",
            ">",
            "<",
            "tr",
            ">",
            "<",
            "td",
            ">",
            "CodeLLaMA-13B-Instruct",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "28.2",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "15.5",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "21.5",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "74.1",
            "<",
            "/td",
            ">",
            "<",
            "/tr",
            ">",
            "<",
            "tr",
            ">",
            "<",
            "td",
            ">",
            "InternLM-20B-Chat",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "34.6",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "10.7",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "25.1",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "65.5",
            "<",
            "/td",
            ">",
            "<",
            "/tr",
            ">",
            "<",
            "tr",
            ">",
            "<",
            "td",
            ">",
            "ChatGLM3-6B",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "54.2",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "4.8",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "15.2",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "67.1",
            "<",
            "/td",
            ">",
            "<",
            "/tr",
            ">",
            "<",
            "tr",
            ">",
            "<",
            "td",
            ">",
            "Qwen-1.8B-Chat",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "25.6",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "21.4",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "22.8",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "65.5",
            "<",
            "/td",
            ">",
            "<",
            "/tr",
            ">",
            "<",
            "tr",
            ">",
            "<",
            "td",
            ">",
            "Qwen-7B-Chat",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "41.9",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "23.8",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "38.0",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "67.2",
            "<",
            "/td",
            ">",
            "<",
            "/tr",
            ">",
            "<",
            "tr",
            ">",
            "<",
            "td",
            ">",
            "Qwen-14B-Chat",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "58.4",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "31.0",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "45.6",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "65.5",
            "<",
            "/td",
            ">",
            "<",
            "/tr",
            ">",
            "<",
            "tr",
            ">",
            "<",
            "td",
            ">",
            "Qwen-72B-Chat",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "72.7",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "41.7",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "43.0",
            "<",
            "/td",
            ">",
            "<",
            "td",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "82.8",
            "<",
            "/td",
            ">",
            "<",
            "/tr",
            ">",
            "<",
            "/table",
            ">",
            "<",
            "p",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "<",
            "br",
            ">",
            "<",
            "img",
            "src=",
            "''",
            "assets/code_interpreter_showcase_001.jpg",
            "''",
            "/",
            ">",
            "<",
            "br",
            ">",
            "<",
            "p",
            ">",
            "<",
            "br",
            ">"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/mlc-ai/web-llm",
        "readme_url": "https://raw.githubusercontent.com/mlc-ai/web-llm/main/README.md",
        "topic": [
            "chatgpt",
            "deep-learning",
            "language-model",
            "llm",
            "tvm",
            "webgpu",
            "webml"
        ],
        "text": "Customized Model Weights\n\nWebLLM works as a companion project of [MLC LLM](https://github.com/mlc-ai/mlc-llm).\nIt reuses the model artifact and builds flow of MLC LLM, please check out\n[MLC LLM document](https://llm.mlc.ai/docs/deploy/javascript.html)\non how to add new model weights and libraries to WebLLM.\n\nHere, we go over the high-level idea. There are two elements of the WebLLM package that enables new models and weight variants.\n\n- model_url: Contains a URL to model artifacts, such as weights and meta-data.\n- model_lib_url: A URL to the web assembly library (i.e. wasm file) that contains the executables to accelerate the model computations.\n\nBoth are customizable in the WebLLM.\n\n```typescript\nasync main() {\n  const myLlamaUrl = \"/url/to/my/llama\";\n  const appConfig = {\n    \"model_list\": [\n      {\n        \"model_url\": myLlamaUrl,\n        \"local_id\": \"MyLlama-3b-v1-q4f32_0\"\n        \"model_lib_url\": \"/url/to/myllama3b.wasm\",\n      }\n    ],\n  };\n  // override default\n  const chatOpts = {\n    \"repetition_penalty\": 1.01\n  };\n\n  const chat = new ChatModule();\n  // load a prebuilt model\n  // with a chat option override and app config\n  // under the hood, it will load the model from myLlamaUrl\n  // and cache it in the browser cache\n  // The chat will also load the model library from \"/url/to/myllama3b.wasm\",\n  // assuming that it is compatible to the model in myLlamaUrl.\n  await chat.reload(\"MyLlama-3b-v1-q4f32_0\", chatOpts, appConfig);\n}\n```\n\nIn many cases, we only want to supply the model weight variant, but\nnot necessarily a new model (e.g. `NeuralHermes-Mistral` can reuse `Mistral`'s\nmodel library; `WizardMath` can reuse `Llama-2`'s model library). For\nan example of how a model library is shared by different model variants,\nsee `examples/simple-chat/src/gh-config.js`. We also provide\na plethora of prebuilt model libraries, including:\n\n- `Llama-2-7b-chat-hf-q4f32_1`: Llama-7b models.\n- `RedPajama-INCITE-Chat-3B-v1-q4f32_1`: RedPajama-3B variants.\n- `Mistral-7B-Instruct-v0.1-q4f16_1`: Mistral-7B variants.\n- and many more at [binary-mlc-llm-libs](https://github.com/mlc-ai/binary-mlc-llm-libs).\n\n",
        "token": [
            "Customized",
            "Model",
            "Weights",
            "WebLLM",
            "works",
            "as",
            "a",
            "companion",
            "project",
            "of",
            "[",
            "MLC",
            "LLM",
            "]",
            "(",
            "https",
            ":",
            "//github.com/mlc-ai/mlc-llm",
            ")",
            ".",
            "It",
            "reuses",
            "the",
            "model",
            "artifact",
            "and",
            "builds",
            "flow",
            "of",
            "MLC",
            "LLM",
            ",",
            "please",
            "check",
            "out",
            "[",
            "MLC",
            "LLM",
            "document",
            "]",
            "(",
            "https",
            ":",
            "//llm.mlc.ai/docs/deploy/javascript.html",
            ")",
            "on",
            "how",
            "to",
            "add",
            "new",
            "model",
            "weights",
            "and",
            "libraries",
            "to",
            "WebLLM",
            ".",
            "Here",
            ",",
            "we",
            "go",
            "over",
            "the",
            "high-level",
            "idea",
            ".",
            "There",
            "are",
            "two",
            "elements",
            "of",
            "the",
            "WebLLM",
            "package",
            "that",
            "enables",
            "new",
            "models",
            "and",
            "weight",
            "variants",
            ".",
            "-",
            "model_url",
            ":",
            "Contains",
            "a",
            "URL",
            "to",
            "model",
            "artifacts",
            ",",
            "such",
            "as",
            "weights",
            "and",
            "meta-data",
            ".",
            "-",
            "model_lib_url",
            ":",
            "A",
            "URL",
            "to",
            "the",
            "web",
            "assembly",
            "library",
            "(",
            "i.e",
            ".",
            "wasm",
            "file",
            ")",
            "that",
            "contains",
            "the",
            "executables",
            "to",
            "accelerate",
            "the",
            "model",
            "computations",
            ".",
            "Both",
            "are",
            "customizable",
            "in",
            "the",
            "WebLLM",
            ".",
            "``",
            "`",
            "typescript",
            "async",
            "main",
            "(",
            ")",
            "{",
            "const",
            "myLlamaUrl",
            "=",
            "``",
            "/url/to/my/llama",
            "''",
            ";",
            "const",
            "appConfig",
            "=",
            "{",
            "``",
            "model_list",
            "''",
            ":",
            "[",
            "{",
            "``",
            "model_url",
            "''",
            ":",
            "myLlamaUrl",
            ",",
            "``",
            "local_id",
            "''",
            ":",
            "``",
            "MyLlama-3b-v1-q4f32_0",
            "''",
            "``",
            "model_lib_url",
            "''",
            ":",
            "``",
            "/url/to/myllama3b.wasm",
            "''",
            ",",
            "}",
            "]",
            ",",
            "}",
            ";",
            "//",
            "override",
            "default",
            "const",
            "chatOpts",
            "=",
            "{",
            "``",
            "repetition_penalty",
            "''",
            ":",
            "1.01",
            "}",
            ";",
            "const",
            "chat",
            "=",
            "new",
            "ChatModule",
            "(",
            ")",
            ";",
            "//",
            "load",
            "a",
            "prebuilt",
            "model",
            "//",
            "with",
            "a",
            "chat",
            "option",
            "override",
            "and",
            "app",
            "config",
            "//",
            "under",
            "the",
            "hood",
            ",",
            "it",
            "will",
            "load",
            "the",
            "model",
            "from",
            "myLlamaUrl",
            "//",
            "and",
            "cache",
            "it",
            "in",
            "the",
            "browser",
            "cache",
            "//",
            "The",
            "chat",
            "will",
            "also",
            "load",
            "the",
            "model",
            "library",
            "from",
            "``",
            "/url/to/myllama3b.wasm",
            "''",
            ",",
            "//",
            "assuming",
            "that",
            "it",
            "is",
            "compatible",
            "to",
            "the",
            "model",
            "in",
            "myLlamaUrl",
            ".",
            "await",
            "chat.reload",
            "(",
            "``",
            "MyLlama-3b-v1-q4f32_0",
            "''",
            ",",
            "chatOpts",
            ",",
            "appConfig",
            ")",
            ";",
            "}",
            "``",
            "`",
            "In",
            "many",
            "cases",
            ",",
            "we",
            "only",
            "want",
            "to",
            "supply",
            "the",
            "model",
            "weight",
            "variant",
            ",",
            "but",
            "not",
            "necessarily",
            "a",
            "new",
            "model",
            "(",
            "e.g",
            ".",
            "`",
            "NeuralHermes-Mistral",
            "`",
            "can",
            "reuse",
            "`",
            "Mistral",
            "`",
            "'s",
            "model",
            "library",
            ";",
            "`",
            "WizardMath",
            "`",
            "can",
            "reuse",
            "`",
            "Llama-2",
            "`",
            "'s",
            "model",
            "library",
            ")",
            ".",
            "For",
            "an",
            "example",
            "of",
            "how",
            "a",
            "model",
            "library",
            "is",
            "shared",
            "by",
            "different",
            "model",
            "variants",
            ",",
            "see",
            "`",
            "examples/simple-chat/src/gh-config.js",
            "`",
            ".",
            "We",
            "also",
            "provide",
            "a",
            "plethora",
            "of",
            "prebuilt",
            "model",
            "libraries",
            ",",
            "including",
            ":",
            "-",
            "`",
            "Llama-2-7b-chat-hf-q4f32_1",
            "`",
            ":",
            "Llama-7b",
            "models",
            ".",
            "-",
            "`",
            "RedPajama-INCITE-Chat-3B-v1-q4f32_1",
            "`",
            ":",
            "RedPajama-3B",
            "variants",
            ".",
            "-",
            "`",
            "Mistral-7B-Instruct-v0.1-q4f16_1",
            "`",
            ":",
            "Mistral-7B",
            "variants",
            ".",
            "-",
            "and",
            "many",
            "more",
            "at",
            "[",
            "binary-mlc-llm-libs",
            "]",
            "(",
            "https",
            ":",
            "//github.com/mlc-ai/binary-mlc-llm-libs",
            ")",
            "."
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/mlc-ai/web-llm",
        "readme_url": "https://raw.githubusercontent.com/mlc-ai/web-llm/main/README.md",
        "topic": [
            "chatgpt",
            "deep-learning",
            "language-model",
            "llm",
            "tvm",
            "webgpu",
            "webml"
        ],
        "text": "Build WebLLM Package From Source\n\nNOTE: you don't need to build by yourself unless you would\nlike to change the WebLLM package, follow [use WebLLM](#use-web-llm-package) instead.\n\nWebLLM package is a web runtime designed for [MLC LLM](https://github.com/mlc-ai/mlc-llm).\n\n1. Install all the prerequisites for compilation:\n    1. [emscripten](https://emscripten.org). It is an LLVM-based compiler that compiles C/C++ source code to WebAssembly.\n        - Follow the [installation instruction](https://emscripten.org/docs/getting_started/downloads.html#installation-instructions-using-the-emsdk-recommended) to install the latest emsdk.\n        - Source `emsdk_env.sh` by `source path/to/emsdk_env.sh`, so that `emcc` is reachable from PATH and the command `emcc` works.\n    4. Install jekyll by following the [official guides](https://jekyllrb.com/docs/installation/). It is the package we use for website. This is not needed if you're using nextjs (see next-simple-chat in the examples).\n    5. Install jekyll-remote-theme by command. Try [gem mirror](https://gems.ruby-china.com/) if install blocked.\n        ```shell\n        gem install jekyll-remote-theme\n        ```\n    We can verify the successful installation by trying out `emcc` and `jekyll` in terminal, respectively.\n\n2. Setup necessary environment\n\n    Prepare all the necessary dependencies for web build:\n    ```shell\n    ./scripts/prep_deps.sh\n    ```\n\n3. Buld WebLLM Package\n\n    ```shell\n    npm run build\n    ```\n\n4. Validate some of the sub-packages\n\n    You can then go to the subfolders in [examples](examples) to validate some of the sub-packages.\n    We use Parcelv2 for bundling. Although Parcel is not very good at tracking parent directory\n    changes sometimes. When you make a change in the WebLLM package, try to edit the `package.json`\n    of the subfolder and save it, which will trigger Parcel to rebuild.\n\n\n",
        "token": [
            "Build",
            "WebLLM",
            "Package",
            "From",
            "Source",
            "NOTE",
            ":",
            "you",
            "do",
            "n't",
            "need",
            "to",
            "build",
            "by",
            "yourself",
            "unless",
            "you",
            "would",
            "like",
            "to",
            "change",
            "the",
            "WebLLM",
            "package",
            ",",
            "follow",
            "[",
            "use",
            "WebLLM",
            "]",
            "(",
            "#",
            "use-web-llm-package",
            ")",
            "instead",
            ".",
            "WebLLM",
            "package",
            "is",
            "a",
            "web",
            "runtime",
            "designed",
            "for",
            "[",
            "MLC",
            "LLM",
            "]",
            "(",
            "https",
            ":",
            "//github.com/mlc-ai/mlc-llm",
            ")",
            ".",
            "1",
            ".",
            "Install",
            "all",
            "the",
            "prerequisites",
            "for",
            "compilation",
            ":",
            "1",
            ".",
            "[",
            "emscripten",
            "]",
            "(",
            "https",
            ":",
            "//emscripten.org",
            ")",
            ".",
            "It",
            "is",
            "an",
            "LLVM-based",
            "compiler",
            "that",
            "compiles",
            "C/C++",
            "source",
            "code",
            "to",
            "WebAssembly",
            ".",
            "-",
            "Follow",
            "the",
            "[",
            "installation",
            "instruction",
            "]",
            "(",
            "https",
            ":",
            "//emscripten.org/docs/getting_started/downloads.html",
            "#",
            "installation-instructions-using-the-emsdk-recommended",
            ")",
            "to",
            "install",
            "the",
            "latest",
            "emsdk",
            ".",
            "-",
            "Source",
            "`",
            "emsdk_env.sh",
            "`",
            "by",
            "`",
            "source",
            "path/to/emsdk_env.sh",
            "`",
            ",",
            "so",
            "that",
            "`",
            "emcc",
            "`",
            "is",
            "reachable",
            "from",
            "PATH",
            "and",
            "the",
            "command",
            "`",
            "emcc",
            "`",
            "works",
            ".",
            "4",
            ".",
            "Install",
            "jekyll",
            "by",
            "following",
            "the",
            "[",
            "official",
            "guides",
            "]",
            "(",
            "https",
            ":",
            "//jekyllrb.com/docs/installation/",
            ")",
            ".",
            "It",
            "is",
            "the",
            "package",
            "we",
            "use",
            "for",
            "website",
            ".",
            "This",
            "is",
            "not",
            "needed",
            "if",
            "you",
            "'re",
            "using",
            "nextjs",
            "(",
            "see",
            "next-simple-chat",
            "in",
            "the",
            "examples",
            ")",
            ".",
            "5",
            ".",
            "Install",
            "jekyll-remote-theme",
            "by",
            "command",
            ".",
            "Try",
            "[",
            "gem",
            "mirror",
            "]",
            "(",
            "https",
            ":",
            "//gems.ruby-china.com/",
            ")",
            "if",
            "install",
            "blocked",
            ".",
            "``",
            "`",
            "shell",
            "gem",
            "install",
            "jekyll-remote-theme",
            "``",
            "`",
            "We",
            "can",
            "verify",
            "the",
            "successful",
            "installation",
            "by",
            "trying",
            "out",
            "`",
            "emcc",
            "`",
            "and",
            "`",
            "jekyll",
            "`",
            "in",
            "terminal",
            ",",
            "respectively",
            ".",
            "2",
            ".",
            "Setup",
            "necessary",
            "environment",
            "Prepare",
            "all",
            "the",
            "necessary",
            "dependencies",
            "for",
            "web",
            "build",
            ":",
            "``",
            "`",
            "shell",
            "./scripts/prep_deps.sh",
            "``",
            "`",
            "3",
            ".",
            "Buld",
            "WebLLM",
            "Package",
            "``",
            "`",
            "shell",
            "npm",
            "run",
            "build",
            "``",
            "`",
            "4",
            ".",
            "Validate",
            "some",
            "of",
            "the",
            "sub-packages",
            "You",
            "can",
            "then",
            "go",
            "to",
            "the",
            "subfolders",
            "in",
            "[",
            "examples",
            "]",
            "(",
            "examples",
            ")",
            "to",
            "validate",
            "some",
            "of",
            "the",
            "sub-packages",
            ".",
            "We",
            "use",
            "Parcelv2",
            "for",
            "bundling",
            ".",
            "Although",
            "Parcel",
            "is",
            "not",
            "very",
            "good",
            "at",
            "tracking",
            "parent",
            "directory",
            "changes",
            "sometimes",
            ".",
            "When",
            "you",
            "make",
            "a",
            "change",
            "in",
            "the",
            "WebLLM",
            "package",
            ",",
            "try",
            "to",
            "edit",
            "the",
            "`",
            "package.json",
            "`",
            "of",
            "the",
            "subfolder",
            "and",
            "save",
            "it",
            ",",
            "which",
            "will",
            "trigger",
            "Parcel",
            "to",
            "rebuild",
            "."
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/nebuly-ai/nebuly",
        "readme_url": "https://raw.githubusercontent.com/nebuly-ai/nebuly/main/README.md",
        "topic": [
            "ai",
            "analytics",
            "artificial-intelligence",
            "deeplearning",
            "large-language-models",
            "llm"
        ],
        "text": "Installation\n\nThe easiest way to install Nebuly\u2019s SDK is via\u00a0`pip`:\n\n```\npip install nebuly\n```\n\nOnce installed, authenticate to Nebuly platform and start building.\n\n",
        "token": [
            "Installation",
            "The",
            "easiest",
            "way",
            "to",
            "install",
            "Nebuly",
            "\u2019",
            "s",
            "SDK",
            "is",
            "via",
            "`",
            "pip",
            "`",
            ":",
            "``",
            "`",
            "pip",
            "install",
            "nebuly",
            "``",
            "`",
            "Once",
            "installed",
            ",",
            "authenticate",
            "to",
            "Nebuly",
            "platform",
            "and",
            "start",
            "building",
            "."
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/LlamaFamily/Llama2-Chinese",
        "readme_url": "https://raw.githubusercontent.com/LlamaFamily/Llama2-Chinese/main/README.md",
        "topic": [
            "finetune",
            "llama",
            "llama2",
            "llm",
            "lora",
            "pretrain"
        ],
        "text": "\ud83c\udf44 \u6a21\u578b\u91cf\u5316\n\u6211\u4eec\u5bf9\u4e2d\u6587\u5fae\u8c03\u7684\u6a21\u578b\u53c2\u6570\u8fdb\u884c\u4e86\u91cf\u5316\uff0c\u65b9\u4fbf\u4ee5\u66f4\u5c11\u7684\u8ba1\u7b97\u8d44\u6e90\u8fd0\u884c\u3002\u76ee\u524d\u5df2\u7ecf\u5728[Hugging Face](https://huggingface.co/FlagAlpha)\u4e0a\u4f20\u4e8613B\u4e2d\u6587\u5fae\u8c03\u6a21\u578b[FlagAlpha/Llama2-Chinese-13b-Chat](https://huggingface.co/FlagAlpha/Llama2-Chinese-13b-Chat)\u76844bit\u538b\u7f29\u7248\u672c[FlagAlpha/Llama2-Chinese-13b-Chat-4bit](https://huggingface.co/FlagAlpha/Llama2-Chinese-13b-Chat-4bit)\uff0c\u5177\u4f53\u8c03\u7528\u65b9\u5f0f\u5982\u4e0b\uff1a\n\n\u73af\u5883\u51c6\u5907\uff1a\n```\npip install git+https://github.com/PanQiWei/AutoGPTQ.git\n```\n\n```python\nfrom transformers import AutoTokenizer\nfrom auto_gptq import AutoGPTQForCausalLM\nmodel = AutoGPTQForCausalLM.from_quantized('FlagAlpha/Llama2-Chinese-13b-Chat-4bit', device=\"cuda:0\")\ntokenizer = AutoTokenizer.from_pretrained('FlagAlpha/Llama2-Chinese-13b-Chat-4bit',use_fast=False)\ninput_ids = tokenizer(['<s>Human: \u600e\u4e48\u767b\u4e0a\u706b\u661f\\n</s><s>Assistant: '], return_tensors=\"pt\",add_special_tokens=False).input_ids.to('cuda')        \ngenerate_input = {\n    \"input_ids\":input_ids,\n    \"max_new_tokens\":512,\n    \"do_sample\":True,\n    \"top_k\":50,\n    \"top_p\":0.95,\n    \"temperature\":0.3,\n    \"repetition_penalty\":1.3,\n    \"eos_token_id\":tokenizer.eos_token_id,\n    \"bos_token_id\":tokenizer.bos_token_id,\n    \"pad_token_id\":tokenizer.pad_token_id\n}\ngenerate_ids  = model.generate(**generate_input)\ntext = tokenizer.decode(generate_ids[0])\nprint(text)\n```\n\n",
        "token": [
            "\ud83c\udf44",
            "\u6a21\u578b\u91cf\u5316",
            "\u6211\u4eec\u5bf9\u4e2d\u6587\u5fae\u8c03\u7684\u6a21\u578b\u53c2\u6570\u8fdb\u884c\u4e86\u91cf\u5316\uff0c\u65b9\u4fbf\u4ee5\u66f4\u5c11\u7684\u8ba1\u7b97\u8d44\u6e90\u8fd0\u884c\u3002\u76ee\u524d\u5df2\u7ecf\u5728",
            "[",
            "Hugging",
            "Face",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/FlagAlpha",
            ")",
            "\u4e0a\u4f20\u4e8613B\u4e2d\u6587\u5fae\u8c03\u6a21\u578b",
            "[",
            "FlagAlpha/Llama2-Chinese-13b-Chat",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/FlagAlpha/Llama2-Chinese-13b-Chat",
            ")",
            "\u76844bit\u538b\u7f29\u7248\u672c",
            "[",
            "FlagAlpha/Llama2-Chinese-13b-Chat-4bit",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/FlagAlpha/Llama2-Chinese-13b-Chat-4bit",
            ")",
            "\uff0c\u5177\u4f53\u8c03\u7528\u65b9\u5f0f\u5982\u4e0b\uff1a",
            "\u73af\u5883\u51c6\u5907\uff1a",
            "``",
            "`",
            "pip",
            "install",
            "git+https",
            ":",
            "//github.com/PanQiWei/AutoGPTQ.git",
            "``",
            "`",
            "``",
            "`",
            "python",
            "from",
            "transformers",
            "import",
            "AutoTokenizer",
            "from",
            "auto_gptq",
            "import",
            "AutoGPTQForCausalLM",
            "model",
            "=",
            "AutoGPTQForCausalLM.from_quantized",
            "(",
            "'FlagAlpha/Llama2-Chinese-13b-Chat-4bit",
            "'",
            ",",
            "device=",
            "''",
            "cuda:0",
            "''",
            ")",
            "tokenizer",
            "=",
            "AutoTokenizer.from_pretrained",
            "(",
            "'FlagAlpha/Llama2-Chinese-13b-Chat-4bit",
            "'",
            ",",
            "use_fast=False",
            ")",
            "input_ids",
            "=",
            "tokenizer",
            "(",
            "[",
            "'",
            "<",
            "s",
            ">",
            "Human",
            ":",
            "\u600e\u4e48\u767b\u4e0a\u706b\u661f\\n",
            "<",
            "/s",
            ">",
            "<",
            "s",
            ">",
            "Assistant",
            ":",
            "'",
            "]",
            ",",
            "return_tensors=",
            "''",
            "pt",
            "''",
            ",",
            "add_special_tokens=False",
            ")",
            ".input_ids.to",
            "(",
            "'cuda",
            "'",
            ")",
            "generate_input",
            "=",
            "{",
            "``",
            "input_ids",
            "''",
            ":",
            "input_ids",
            ",",
            "``",
            "max_new_tokens",
            "''",
            ":512",
            ",",
            "``",
            "do_sample",
            "''",
            ":",
            "True",
            ",",
            "``",
            "top_k",
            "''",
            ":50",
            ",",
            "``",
            "top_p",
            "''",
            ":0.95",
            ",",
            "``",
            "temperature",
            "''",
            ":0.3",
            ",",
            "``",
            "repetition_penalty",
            "''",
            ":1.3",
            ",",
            "``",
            "eos_token_id",
            "''",
            ":",
            "tokenizer.eos_token_id",
            ",",
            "``",
            "bos_token_id",
            "''",
            ":",
            "tokenizer.bos_token_id",
            ",",
            "``",
            "pad_token_id",
            "''",
            ":",
            "tokenizer.pad_token_id",
            "}",
            "generate_ids",
            "=",
            "model.generate",
            "(",
            "*",
            "*",
            "generate_input",
            ")",
            "text",
            "=",
            "tokenizer.decode",
            "(",
            "generate_ids",
            "[",
            "0",
            "]",
            ")",
            "print",
            "(",
            "text",
            ")",
            "``",
            "`"
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/bentoml/OpenLLM",
        "readme_url": "https://raw.githubusercontent.com/bentoml/OpenLLM/main/README.md",
        "topic": [
            "ai",
            "bentoml",
            "falcon",
            "fine-tuning",
            "llama",
            "llama2",
            "llm",
            "llm-inference",
            "llm-ops",
            "llm-serving",
            "llmops",
            "mistral",
            "ml",
            "mlops",
            "model-inference",
            "mpt",
            "open-source-llm",
            "openllm",
            "stablelm",
            "vicuna"
        ],
        "text": "Docker\n\nWe provide a docker container that helps you start running OpenLLM:\n\n```bash\ndocker run --rm -it -p 3000:3000 ghcr.io/bentoml/openllm start facebook/opt-1.3b --backend pt\n```\n\n> [!NOTE]\n> Given you have access to GPUs and have setup [nvidia-docker](https://github.com/NVIDIA/nvidia-container-toolkit),  you can additionally pass in `--gpus`\n> to use GPU for faster inference and optimization\n>```bash\n> docker run --rm --gpus all -p 3000:3000 -it ghcr.io/bentoml/openllm start HuggingFaceH4/zephyr-7b-beta --backend vllm\n> ```\n\n\n",
        "token": [
            "Docker",
            "We",
            "provide",
            "a",
            "docker",
            "container",
            "that",
            "helps",
            "you",
            "start",
            "running",
            "OpenLLM",
            ":",
            "``",
            "`",
            "bash",
            "docker",
            "run",
            "--",
            "rm",
            "-it",
            "-p",
            "3000:3000",
            "ghcr.io/bentoml/openllm",
            "start",
            "facebook/opt-1.3b",
            "--",
            "backend",
            "pt",
            "``",
            "`",
            ">",
            "[",
            "!",
            "NOTE",
            "]",
            ">",
            "Given",
            "you",
            "have",
            "access",
            "to",
            "GPUs",
            "and",
            "have",
            "setup",
            "[",
            "nvidia-docker",
            "]",
            "(",
            "https",
            ":",
            "//github.com/NVIDIA/nvidia-container-toolkit",
            ")",
            ",",
            "you",
            "can",
            "additionally",
            "pass",
            "in",
            "`",
            "--",
            "gpus",
            "`",
            ">",
            "to",
            "use",
            "GPU",
            "for",
            "faster",
            "inference",
            "and",
            "optimization",
            ">",
            "``",
            "`",
            "bash",
            ">",
            "docker",
            "run",
            "--",
            "rm",
            "--",
            "gpus",
            "all",
            "-p",
            "3000:3000",
            "-it",
            "ghcr.io/bentoml/openllm",
            "start",
            "HuggingFaceH4/zephyr-7b-beta",
            "--",
            "backend",
            "vllm",
            ">",
            "``",
            "`"
        ],
        "level of complexity": 1
    },
    {
        "url": "https://github.com/bentoml/OpenLLM",
        "readme_url": "https://raw.githubusercontent.com/bentoml/OpenLLM/main/README.md",
        "topic": [
            "ai",
            "bentoml",
            "falcon",
            "fine-tuning",
            "llama",
            "llama2",
            "llm",
            "llm-inference",
            "llm-ops",
            "llm-serving",
            "llmops",
            "mistral",
            "ml",
            "mlops",
            "model-inference",
            "mpt",
            "open-source-llm",
            "openllm",
            "stablelm",
            "vicuna"
        ],
        "text": "\ud83c\udfc3 Get started\n\nThe following provides instructions for how to get started with OpenLLM locally.\n",
        "token": [
            "\ud83c\udfc3",
            "Get",
            "started",
            "The",
            "following",
            "provides",
            "instructions",
            "for",
            "how",
            "to",
            "get",
            "started",
            "with",
            "OpenLLM",
            "locally",
            "."
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/bentoml/OpenLLM",
        "readme_url": "https://raw.githubusercontent.com/bentoml/OpenLLM/main/README.md",
        "topic": [
            "ai",
            "bentoml",
            "falcon",
            "fine-tuning",
            "llama",
            "llama2",
            "llm",
            "llm-inference",
            "llm-ops",
            "llm-serving",
            "llmops",
            "mistral",
            "ml",
            "mlops",
            "model-inference",
            "mpt",
            "open-source-llm",
            "openllm",
            "stablelm",
            "vicuna"
        ],
        "text": "Prerequisites\n\nYou have installed Python 3.8 (or later) and\u00a0`pip`. We highly recommend using a [Virtual Environment](https://docs.python.org/3/library/venv.html) to prevent package conflicts.\n\n",
        "token": [
            "Prerequisites",
            "You",
            "have",
            "installed",
            "Python",
            "3.8",
            "(",
            "or",
            "later",
            ")",
            "and",
            "`",
            "pip",
            "`",
            ".",
            "We",
            "highly",
            "recommend",
            "using",
            "a",
            "[",
            "Virtual",
            "Environment",
            "]",
            "(",
            "https",
            ":",
            "//docs.python.org/3/library/venv.html",
            ")",
            "to",
            "prevent",
            "package",
            "conflicts",
            "."
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/bentoml/OpenLLM",
        "readme_url": "https://raw.githubusercontent.com/bentoml/OpenLLM/main/README.md",
        "topic": [
            "ai",
            "bentoml",
            "falcon",
            "fine-tuning",
            "llama",
            "llama2",
            "llm",
            "llm-inference",
            "llm-ops",
            "llm-serving",
            "llmops",
            "mistral",
            "ml",
            "mlops",
            "model-inference",
            "mpt",
            "open-source-llm",
            "openllm",
            "stablelm",
            "vicuna"
        ],
        "text": "Install OpenLLM\n\nInstall OpenLLM by using `pip` as follows:\n\n```bash\npip install openllm\n```\n\nTo verify the installation, run:\n\n```bash\n$ openllm -h\n\nUsage: openllm [OPTIONS] COMMAND [ARGS]...\n\n   \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2557   \u2588\u2588\u2557\u2588\u2588\u2557     \u2588\u2588\u2557     \u2588\u2588\u2588\u2557   \u2588\u2588\u2588\u2557\n  \u2588\u2588\u2554\u2550\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2551\u2588\u2588\u2551     \u2588\u2588\u2551     \u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2551\n  \u2588\u2588\u2551   \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2551     \u2588\u2588\u2551     \u2588\u2588\u2554\u2588\u2588\u2588\u2588\u2554\u2588\u2588\u2551\n  \u2588\u2588\u2551   \u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u255d \u2588\u2588\u2554\u2550\u2550\u255d  \u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2551     \u2588\u2588\u2551     \u2588\u2588\u2551\u255a\u2588\u2588\u2554\u255d\u2588\u2588\u2551\n  \u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551     \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551 \u255a\u2550\u255d \u2588\u2588\u2551\n   \u255a\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d     \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u2550\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d     \u255a\u2550\u255d.\n\n  An open platform for operating large language models in production.\n  Fine-tune, serve, deploy, and monitor any LLMs with ease.\n\nOptions:\n  -v, --version  Show the version and exit.\n  -h, --help     Show this message and exit.\n\nCommands:\n  build       Package a given models into a BentoLLM.\n  import      Setup LLM interactively.\n  models      List all supported models.\n  prune       Remove all saved models, (and optionally bentos) built with OpenLLM locally.\n  query       Query a LLM interactively, from a terminal.\n  start       Start a LLMServer for any supported LLM.\n  start-grpc  Start a gRPC LLMServer for any supported LLM.\n\nExtensions:\n  build-base-container  Base image builder for BentoLLM.\n  dive-bentos           Dive into a BentoLLM.\n  get-containerfile     Return Containerfile of any given Bento.\n  get-prompt            Get the default prompt used by OpenLLM.\n  list-bentos           List available bentos built by OpenLLM.\n  list-models           This is equivalent to openllm models...\n  playground            OpenLLM Playground.\n```\n\n",
        "token": [
            "Install",
            "OpenLLM",
            "Install",
            "OpenLLM",
            "by",
            "using",
            "`",
            "pip",
            "`",
            "as",
            "follows",
            ":",
            "``",
            "`",
            "bash",
            "pip",
            "install",
            "openllm",
            "``",
            "`",
            "To",
            "verify",
            "the",
            "installation",
            ",",
            "run",
            ":",
            "``",
            "`",
            "bash",
            "$",
            "openllm",
            "-h",
            "Usage",
            ":",
            "openllm",
            "[",
            "OPTIONS",
            "]",
            "COMMAND",
            "[",
            "ARGS",
            "]",
            "...",
            "\u2588\u2588\u2588\u2588\u2588\u2588\u2557",
            "\u2588\u2588\u2588\u2588\u2588\u2588\u2557",
            "\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2557",
            "\u2588\u2588\u2557\u2588\u2588\u2557",
            "\u2588\u2588\u2557",
            "\u2588\u2588\u2588\u2557",
            "\u2588\u2588\u2588\u2557",
            "\u2588\u2588\u2554\u2550\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2588\u2588\u2557",
            "\u2588\u2588\u2551\u2588\u2588\u2551",
            "\u2588\u2588\u2551",
            "\u2588\u2588\u2588\u2588\u2557",
            "\u2588\u2588\u2588\u2588\u2551",
            "\u2588\u2588\u2551",
            "\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2588\u2588\u2588\u2557",
            "\u2588\u2588\u2554\u2588\u2588\u2557",
            "\u2588\u2588\u2551\u2588\u2588\u2551",
            "\u2588\u2588\u2551",
            "\u2588\u2588\u2554\u2588\u2588\u2588\u2588\u2554\u2588\u2588\u2551",
            "\u2588\u2588\u2551",
            "\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u255d",
            "\u2588\u2588\u2554\u2550\u2550\u255d",
            "\u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2551",
            "\u2588\u2588\u2551",
            "\u2588\u2588\u2551\u255a\u2588\u2588\u2554\u255d\u2588\u2588\u2551",
            "\u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551",
            "\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551",
            "\u255a\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551",
            "\u255a\u2550\u255d",
            "\u2588\u2588\u2551",
            "\u255a\u2550\u2550\u2550\u2550\u2550\u255d",
            "\u255a\u2550\u255d",
            "\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d",
            "\u255a\u2550\u2550\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d",
            "\u255a\u2550\u255d",
            ".",
            "An",
            "open",
            "platform",
            "for",
            "operating",
            "large",
            "language",
            "models",
            "in",
            "production",
            ".",
            "Fine-tune",
            ",",
            "serve",
            ",",
            "deploy",
            ",",
            "and",
            "monitor",
            "any",
            "LLMs",
            "with",
            "ease",
            ".",
            "Options",
            ":",
            "-v",
            ",",
            "--",
            "version",
            "Show",
            "the",
            "version",
            "and",
            "exit",
            ".",
            "-h",
            ",",
            "--",
            "help",
            "Show",
            "this",
            "message",
            "and",
            "exit",
            ".",
            "Commands",
            ":",
            "build",
            "Package",
            "a",
            "given",
            "models",
            "into",
            "a",
            "BentoLLM",
            ".",
            "import",
            "Setup",
            "LLM",
            "interactively",
            ".",
            "models",
            "List",
            "all",
            "supported",
            "models",
            ".",
            "prune",
            "Remove",
            "all",
            "saved",
            "models",
            ",",
            "(",
            "and",
            "optionally",
            "bentos",
            ")",
            "built",
            "with",
            "OpenLLM",
            "locally",
            ".",
            "query",
            "Query",
            "a",
            "LLM",
            "interactively",
            ",",
            "from",
            "a",
            "terminal",
            ".",
            "start",
            "Start",
            "a",
            "LLMServer",
            "for",
            "any",
            "supported",
            "LLM",
            ".",
            "start-grpc",
            "Start",
            "a",
            "gRPC",
            "LLMServer",
            "for",
            "any",
            "supported",
            "LLM",
            ".",
            "Extensions",
            ":",
            "build-base-container",
            "Base",
            "image",
            "builder",
            "for",
            "BentoLLM",
            ".",
            "dive-bentos",
            "Dive",
            "into",
            "a",
            "BentoLLM",
            ".",
            "get-containerfile",
            "Return",
            "Containerfile",
            "of",
            "any",
            "given",
            "Bento",
            ".",
            "get-prompt",
            "Get",
            "the",
            "default",
            "prompt",
            "used",
            "by",
            "OpenLLM",
            ".",
            "list-bentos",
            "List",
            "available",
            "bentos",
            "built",
            "by",
            "OpenLLM",
            ".",
            "list-models",
            "This",
            "is",
            "equivalent",
            "to",
            "openllm",
            "models",
            "...",
            "playground",
            "OpenLLM",
            "Playground",
            ".",
            "``",
            "`"
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/bentoml/OpenLLM",
        "readme_url": "https://raw.githubusercontent.com/bentoml/OpenLLM/main/README.md",
        "topic": [
            "ai",
            "bentoml",
            "falcon",
            "fine-tuning",
            "llama",
            "llama2",
            "llm",
            "llm-inference",
            "llm-ops",
            "llm-serving",
            "llmops",
            "mistral",
            "ml",
            "mlops",
            "model-inference",
            "mpt",
            "open-source-llm",
            "openllm",
            "stablelm",
            "vicuna"
        ],
        "text": "\ud83e\udde9 Supported models\n\nOpenLLM currently supports the following models. By default, OpenLLM doesn't include dependencies to run all models. The extra model-specific dependencies can be installed with the instructions below.\n\n<!-- update-readme.py: start -->\n<details>\n\n<summary>Baichuan</summary>\n\n\n",
        "token": [
            "\ud83e\udde9",
            "Supported",
            "models",
            "OpenLLM",
            "currently",
            "supports",
            "the",
            "following",
            "models",
            ".",
            "By",
            "default",
            ",",
            "OpenLLM",
            "does",
            "n't",
            "include",
            "dependencies",
            "to",
            "run",
            "all",
            "models",
            ".",
            "The",
            "extra",
            "model-specific",
            "dependencies",
            "can",
            "be",
            "installed",
            "with",
            "the",
            "instructions",
            "below",
            ".",
            "<",
            "!",
            "--",
            "update-readme.py",
            ":",
            "start",
            "--",
            ">",
            "<",
            "details",
            ">",
            "<",
            "summary",
            ">",
            "Baichuan",
            "<",
            "/summary",
            ">"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/bentoml/OpenLLM",
        "readme_url": "https://raw.githubusercontent.com/bentoml/OpenLLM/main/README.md",
        "topic": [
            "ai",
            "bentoml",
            "falcon",
            "fine-tuning",
            "llama",
            "llama2",
            "llm",
            "llm-inference",
            "llm-ops",
            "llm-serving",
            "llmops",
            "mistral",
            "ml",
            "mlops",
            "model-inference",
            "mpt",
            "open-source-llm",
            "openllm",
            "stablelm",
            "vicuna"
        ],
        "text": "Quickstart\n\n\n\n> **Note:** Baichuan requires to install with:\n> ```bash\n> pip install \"openllm[baichuan]\"\n> ```\n\n\nRun the following command to quickly spin up a Baichuan server:\n\n```bash\nTRUST_REMOTE_CODE=True openllm start baichuan-inc/baichuan-7b\n```\nIn a different terminal, run the following command to interact with the server:\n\n```bash\nexport OPENLLM_ENDPOINT=http://localhost:3000\nopenllm query 'What are large language models?'\n```\n\n\n> **Note:** Any Baichuan variants can be deployed with OpenLLM. Visit the [HuggingFace Model Hub](https://huggingface.co/models?sort=trending&search=baichuan) to see more Baichuan-compatible models.\n\n\n\n",
        "token": [
            "Quickstart",
            ">",
            "*",
            "*",
            "Note",
            ":",
            "*",
            "*",
            "Baichuan",
            "requires",
            "to",
            "install",
            "with",
            ":",
            ">",
            "``",
            "`",
            "bash",
            ">",
            "pip",
            "install",
            "``",
            "openllm",
            "[",
            "baichuan",
            "]",
            "''",
            ">",
            "``",
            "`",
            "Run",
            "the",
            "following",
            "command",
            "to",
            "quickly",
            "spin",
            "up",
            "a",
            "Baichuan",
            "server",
            ":",
            "``",
            "`",
            "bash",
            "TRUST_REMOTE_CODE=True",
            "openllm",
            "start",
            "baichuan-inc/baichuan-7b",
            "``",
            "`",
            "In",
            "a",
            "different",
            "terminal",
            ",",
            "run",
            "the",
            "following",
            "command",
            "to",
            "interact",
            "with",
            "the",
            "server",
            ":",
            "``",
            "`",
            "bash",
            "export",
            "OPENLLM_ENDPOINT=http",
            ":",
            "//localhost:3000",
            "openllm",
            "query",
            "'What",
            "are",
            "large",
            "language",
            "models",
            "?",
            "'",
            "``",
            "`",
            ">",
            "*",
            "*",
            "Note",
            ":",
            "*",
            "*",
            "Any",
            "Baichuan",
            "variants",
            "can",
            "be",
            "deployed",
            "with",
            "OpenLLM",
            ".",
            "Visit",
            "the",
            "[",
            "HuggingFace",
            "Model",
            "Hub",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/models",
            "?",
            "sort=trending",
            "&",
            "search=baichuan",
            ")",
            "to",
            "see",
            "more",
            "Baichuan-compatible",
            "models",
            "."
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/bentoml/OpenLLM",
        "readme_url": "https://raw.githubusercontent.com/bentoml/OpenLLM/main/README.md",
        "topic": [
            "ai",
            "bentoml",
            "falcon",
            "fine-tuning",
            "llama",
            "llama2",
            "llm",
            "llm-inference",
            "llm-ops",
            "llm-serving",
            "llmops",
            "mistral",
            "ml",
            "mlops",
            "model-inference",
            "mpt",
            "open-source-llm",
            "openllm",
            "stablelm",
            "vicuna"
        ],
        "text": "Supported backends\n\nOpenLLM will support vLLM and PyTorch as default backend. By default, it will use vLLM if vLLM is available, otherwise fallback to PyTorch.\n\n\n\n> **Important:** We recommend user to explicitly specify `--backend` to choose the desired backend to run the model. If you have access to a GPU, always use `--backend vllm`.\n\n\n\n- vLLM (Recommended):\n\n\nTo install vLLM, run `pip install \"openllm[vllm]\"`\n\n```bash\nTRUST_REMOTE_CODE=True openllm start baichuan-inc/baichuan2-7b-base --backend vllm\n```\n\n\n> **Important:** Using vLLM requires a GPU that has architecture newer than 8.0 to get the best performance for serving. It is recommended that for all serving usecase in production, you should choose vLLM for serving.\n\n\n\n> **Note:** Currently, adapters are yet to be supported with vLLM.\n\n\n- PyTorch:\n\n\n```bash\nTRUST_REMOTE_CODE=True openllm start baichuan-inc/baichuan2-7b-base --backend pt\n```\n\n</details>\n\n<details>\n\n<summary>ChatGLM</summary>\n\n\n",
        "token": [
            "Supported",
            "backends",
            "OpenLLM",
            "will",
            "support",
            "vLLM",
            "and",
            "PyTorch",
            "as",
            "default",
            "backend",
            ".",
            "By",
            "default",
            ",",
            "it",
            "will",
            "use",
            "vLLM",
            "if",
            "vLLM",
            "is",
            "available",
            ",",
            "otherwise",
            "fallback",
            "to",
            "PyTorch",
            ".",
            ">",
            "*",
            "*",
            "Important",
            ":",
            "*",
            "*",
            "We",
            "recommend",
            "user",
            "to",
            "explicitly",
            "specify",
            "`",
            "--",
            "backend",
            "`",
            "to",
            "choose",
            "the",
            "desired",
            "backend",
            "to",
            "run",
            "the",
            "model",
            ".",
            "If",
            "you",
            "have",
            "access",
            "to",
            "a",
            "GPU",
            ",",
            "always",
            "use",
            "`",
            "--",
            "backend",
            "vllm",
            "`",
            ".",
            "-",
            "vLLM",
            "(",
            "Recommended",
            ")",
            ":",
            "To",
            "install",
            "vLLM",
            ",",
            "run",
            "`",
            "pip",
            "install",
            "``",
            "openllm",
            "[",
            "vllm",
            "]",
            "''",
            "`",
            "``",
            "`",
            "bash",
            "TRUST_REMOTE_CODE=True",
            "openllm",
            "start",
            "baichuan-inc/baichuan2-7b-base",
            "--",
            "backend",
            "vllm",
            "``",
            "`",
            ">",
            "*",
            "*",
            "Important",
            ":",
            "*",
            "*",
            "Using",
            "vLLM",
            "requires",
            "a",
            "GPU",
            "that",
            "has",
            "architecture",
            "newer",
            "than",
            "8.0",
            "to",
            "get",
            "the",
            "best",
            "performance",
            "for",
            "serving",
            ".",
            "It",
            "is",
            "recommended",
            "that",
            "for",
            "all",
            "serving",
            "usecase",
            "in",
            "production",
            ",",
            "you",
            "should",
            "choose",
            "vLLM",
            "for",
            "serving",
            ".",
            ">",
            "*",
            "*",
            "Note",
            ":",
            "*",
            "*",
            "Currently",
            ",",
            "adapters",
            "are",
            "yet",
            "to",
            "be",
            "supported",
            "with",
            "vLLM",
            ".",
            "-",
            "PyTorch",
            ":",
            "``",
            "`",
            "bash",
            "TRUST_REMOTE_CODE=True",
            "openllm",
            "start",
            "baichuan-inc/baichuan2-7b-base",
            "--",
            "backend",
            "pt",
            "``",
            "`",
            "<",
            "/details",
            ">",
            "<",
            "details",
            ">",
            "<",
            "summary",
            ">",
            "ChatGLM",
            "<",
            "/summary",
            ">"
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/bentoml/OpenLLM",
        "readme_url": "https://raw.githubusercontent.com/bentoml/OpenLLM/main/README.md",
        "topic": [
            "ai",
            "bentoml",
            "falcon",
            "fine-tuning",
            "llama",
            "llama2",
            "llm",
            "llm-inference",
            "llm-ops",
            "llm-serving",
            "llmops",
            "mistral",
            "ml",
            "mlops",
            "model-inference",
            "mpt",
            "open-source-llm",
            "openllm",
            "stablelm",
            "vicuna"
        ],
        "text": "Quickstart\n\n\n\n> **Note:** ChatGLM requires to install with:\n> ```bash\n> pip install \"openllm[chatglm]\"\n> ```\n\n\nRun the following command to quickly spin up a ChatGLM server:\n\n```bash\nTRUST_REMOTE_CODE=True openllm start thudm/chatglm-6b\n```\nIn a different terminal, run the following command to interact with the server:\n\n```bash\nexport OPENLLM_ENDPOINT=http://localhost:3000\nopenllm query 'What are large language models?'\n```\n\n\n> **Note:** Any ChatGLM variants can be deployed with OpenLLM. Visit the [HuggingFace Model Hub](https://huggingface.co/models?sort=trending&search=chatglm) to see more ChatGLM-compatible models.\n\n\n\n",
        "token": [
            "Quickstart",
            ">",
            "*",
            "*",
            "Note",
            ":",
            "*",
            "*",
            "ChatGLM",
            "requires",
            "to",
            "install",
            "with",
            ":",
            ">",
            "``",
            "`",
            "bash",
            ">",
            "pip",
            "install",
            "``",
            "openllm",
            "[",
            "chatglm",
            "]",
            "''",
            ">",
            "``",
            "`",
            "Run",
            "the",
            "following",
            "command",
            "to",
            "quickly",
            "spin",
            "up",
            "a",
            "ChatGLM",
            "server",
            ":",
            "``",
            "`",
            "bash",
            "TRUST_REMOTE_CODE=True",
            "openllm",
            "start",
            "thudm/chatglm-6b",
            "``",
            "`",
            "In",
            "a",
            "different",
            "terminal",
            ",",
            "run",
            "the",
            "following",
            "command",
            "to",
            "interact",
            "with",
            "the",
            "server",
            ":",
            "``",
            "`",
            "bash",
            "export",
            "OPENLLM_ENDPOINT=http",
            ":",
            "//localhost:3000",
            "openllm",
            "query",
            "'What",
            "are",
            "large",
            "language",
            "models",
            "?",
            "'",
            "``",
            "`",
            ">",
            "*",
            "*",
            "Note",
            ":",
            "*",
            "*",
            "Any",
            "ChatGLM",
            "variants",
            "can",
            "be",
            "deployed",
            "with",
            "OpenLLM",
            ".",
            "Visit",
            "the",
            "[",
            "HuggingFace",
            "Model",
            "Hub",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/models",
            "?",
            "sort=trending",
            "&",
            "search=chatglm",
            ")",
            "to",
            "see",
            "more",
            "ChatGLM-compatible",
            "models",
            "."
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/bentoml/OpenLLM",
        "readme_url": "https://raw.githubusercontent.com/bentoml/OpenLLM/main/README.md",
        "topic": [
            "ai",
            "bentoml",
            "falcon",
            "fine-tuning",
            "llama",
            "llama2",
            "llm",
            "llm-inference",
            "llm-ops",
            "llm-serving",
            "llmops",
            "mistral",
            "ml",
            "mlops",
            "model-inference",
            "mpt",
            "open-source-llm",
            "openllm",
            "stablelm",
            "vicuna"
        ],
        "text": "Supported backends\n\nOpenLLM will support vLLM and PyTorch as default backend. By default, it will use vLLM if vLLM is available, otherwise fallback to PyTorch.\n\n\n\n> **Important:** We recommend user to explicitly specify `--backend` to choose the desired backend to run the model. If you have access to a GPU, always use `--backend vllm`.\n\n\n\n- vLLM (Recommended):\n\n\nTo install vLLM, run `pip install \"openllm[vllm]\"`\n\n```bash\nTRUST_REMOTE_CODE=True openllm start thudm/chatglm-6b --backend vllm\n```\n\n\n> **Important:** Using vLLM requires a GPU that has architecture newer than 8.0 to get the best performance for serving. It is recommended that for all serving usecase in production, you should choose vLLM for serving.\n\n\n\n> **Note:** Currently, adapters are yet to be supported with vLLM.\n\n\n- PyTorch:\n\n\n```bash\nTRUST_REMOTE_CODE=True openllm start thudm/chatglm-6b --backend pt\n```\n\n</details>\n\n<details>\n\n<summary>DollyV2</summary>\n\n\n",
        "token": [
            "Supported",
            "backends",
            "OpenLLM",
            "will",
            "support",
            "vLLM",
            "and",
            "PyTorch",
            "as",
            "default",
            "backend",
            ".",
            "By",
            "default",
            ",",
            "it",
            "will",
            "use",
            "vLLM",
            "if",
            "vLLM",
            "is",
            "available",
            ",",
            "otherwise",
            "fallback",
            "to",
            "PyTorch",
            ".",
            ">",
            "*",
            "*",
            "Important",
            ":",
            "*",
            "*",
            "We",
            "recommend",
            "user",
            "to",
            "explicitly",
            "specify",
            "`",
            "--",
            "backend",
            "`",
            "to",
            "choose",
            "the",
            "desired",
            "backend",
            "to",
            "run",
            "the",
            "model",
            ".",
            "If",
            "you",
            "have",
            "access",
            "to",
            "a",
            "GPU",
            ",",
            "always",
            "use",
            "`",
            "--",
            "backend",
            "vllm",
            "`",
            ".",
            "-",
            "vLLM",
            "(",
            "Recommended",
            ")",
            ":",
            "To",
            "install",
            "vLLM",
            ",",
            "run",
            "`",
            "pip",
            "install",
            "``",
            "openllm",
            "[",
            "vllm",
            "]",
            "''",
            "`",
            "``",
            "`",
            "bash",
            "TRUST_REMOTE_CODE=True",
            "openllm",
            "start",
            "thudm/chatglm-6b",
            "--",
            "backend",
            "vllm",
            "``",
            "`",
            ">",
            "*",
            "*",
            "Important",
            ":",
            "*",
            "*",
            "Using",
            "vLLM",
            "requires",
            "a",
            "GPU",
            "that",
            "has",
            "architecture",
            "newer",
            "than",
            "8.0",
            "to",
            "get",
            "the",
            "best",
            "performance",
            "for",
            "serving",
            ".",
            "It",
            "is",
            "recommended",
            "that",
            "for",
            "all",
            "serving",
            "usecase",
            "in",
            "production",
            ",",
            "you",
            "should",
            "choose",
            "vLLM",
            "for",
            "serving",
            ".",
            ">",
            "*",
            "*",
            "Note",
            ":",
            "*",
            "*",
            "Currently",
            ",",
            "adapters",
            "are",
            "yet",
            "to",
            "be",
            "supported",
            "with",
            "vLLM",
            ".",
            "-",
            "PyTorch",
            ":",
            "``",
            "`",
            "bash",
            "TRUST_REMOTE_CODE=True",
            "openllm",
            "start",
            "thudm/chatglm-6b",
            "--",
            "backend",
            "pt",
            "``",
            "`",
            "<",
            "/details",
            ">",
            "<",
            "details",
            ">",
            "<",
            "summary",
            ">",
            "DollyV2",
            "<",
            "/summary",
            ">"
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/bentoml/OpenLLM",
        "readme_url": "https://raw.githubusercontent.com/bentoml/OpenLLM/main/README.md",
        "topic": [
            "ai",
            "bentoml",
            "falcon",
            "fine-tuning",
            "llama",
            "llama2",
            "llm",
            "llm-inference",
            "llm-ops",
            "llm-serving",
            "llmops",
            "mistral",
            "ml",
            "mlops",
            "model-inference",
            "mpt",
            "open-source-llm",
            "openllm",
            "stablelm",
            "vicuna"
        ],
        "text": "Supported backends\n\nOpenLLM will support vLLM and PyTorch as default backend. By default, it will use vLLM if vLLM is available, otherwise fallback to PyTorch.\n\n\n\n> **Important:** We recommend user to explicitly specify `--backend` to choose the desired backend to run the model. If you have access to a GPU, always use `--backend vllm`.\n\n\n\n- vLLM (Recommended):\n\n\nTo install vLLM, run `pip install \"openllm[vllm]\"`\n\n```bash\nopenllm start databricks/dolly-v2-3b --backend vllm\n```\n\n\n> **Important:** Using vLLM requires a GPU that has architecture newer than 8.0 to get the best performance for serving. It is recommended that for all serving usecase in production, you should choose vLLM for serving.\n\n\n\n> **Note:** Currently, adapters are yet to be supported with vLLM.\n\n\n- PyTorch:\n\n\n```bash\nopenllm start databricks/dolly-v2-3b --backend pt\n```\n\n</details>\n\n<details>\n\n<summary>Falcon</summary>\n\n\n",
        "token": [
            "Supported",
            "backends",
            "OpenLLM",
            "will",
            "support",
            "vLLM",
            "and",
            "PyTorch",
            "as",
            "default",
            "backend",
            ".",
            "By",
            "default",
            ",",
            "it",
            "will",
            "use",
            "vLLM",
            "if",
            "vLLM",
            "is",
            "available",
            ",",
            "otherwise",
            "fallback",
            "to",
            "PyTorch",
            ".",
            ">",
            "*",
            "*",
            "Important",
            ":",
            "*",
            "*",
            "We",
            "recommend",
            "user",
            "to",
            "explicitly",
            "specify",
            "`",
            "--",
            "backend",
            "`",
            "to",
            "choose",
            "the",
            "desired",
            "backend",
            "to",
            "run",
            "the",
            "model",
            ".",
            "If",
            "you",
            "have",
            "access",
            "to",
            "a",
            "GPU",
            ",",
            "always",
            "use",
            "`",
            "--",
            "backend",
            "vllm",
            "`",
            ".",
            "-",
            "vLLM",
            "(",
            "Recommended",
            ")",
            ":",
            "To",
            "install",
            "vLLM",
            ",",
            "run",
            "`",
            "pip",
            "install",
            "``",
            "openllm",
            "[",
            "vllm",
            "]",
            "''",
            "`",
            "``",
            "`",
            "bash",
            "openllm",
            "start",
            "databricks/dolly-v2-3b",
            "--",
            "backend",
            "vllm",
            "``",
            "`",
            ">",
            "*",
            "*",
            "Important",
            ":",
            "*",
            "*",
            "Using",
            "vLLM",
            "requires",
            "a",
            "GPU",
            "that",
            "has",
            "architecture",
            "newer",
            "than",
            "8.0",
            "to",
            "get",
            "the",
            "best",
            "performance",
            "for",
            "serving",
            ".",
            "It",
            "is",
            "recommended",
            "that",
            "for",
            "all",
            "serving",
            "usecase",
            "in",
            "production",
            ",",
            "you",
            "should",
            "choose",
            "vLLM",
            "for",
            "serving",
            ".",
            ">",
            "*",
            "*",
            "Note",
            ":",
            "*",
            "*",
            "Currently",
            ",",
            "adapters",
            "are",
            "yet",
            "to",
            "be",
            "supported",
            "with",
            "vLLM",
            ".",
            "-",
            "PyTorch",
            ":",
            "``",
            "`",
            "bash",
            "openllm",
            "start",
            "databricks/dolly-v2-3b",
            "--",
            "backend",
            "pt",
            "``",
            "`",
            "<",
            "/details",
            ">",
            "<",
            "details",
            ">",
            "<",
            "summary",
            ">",
            "Falcon",
            "<",
            "/summary",
            ">"
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/bentoml/OpenLLM",
        "readme_url": "https://raw.githubusercontent.com/bentoml/OpenLLM/main/README.md",
        "topic": [
            "ai",
            "bentoml",
            "falcon",
            "fine-tuning",
            "llama",
            "llama2",
            "llm",
            "llm-inference",
            "llm-ops",
            "llm-serving",
            "llmops",
            "mistral",
            "ml",
            "mlops",
            "model-inference",
            "mpt",
            "open-source-llm",
            "openllm",
            "stablelm",
            "vicuna"
        ],
        "text": "Quickstart\n\n\n\n> **Note:** Falcon requires to install with:\n> ```bash\n> pip install \"openllm[falcon]\"\n> ```\n\n\nRun the following command to quickly spin up a Falcon server:\n\n```bash\nopenllm start tiiuae/falcon-7b\n```\nIn a different terminal, run the following command to interact with the server:\n\n```bash\nexport OPENLLM_ENDPOINT=http://localhost:3000\nopenllm query 'What are large language models?'\n```\n\n\n> **Note:** Any Falcon variants can be deployed with OpenLLM. Visit the [HuggingFace Model Hub](https://huggingface.co/models?sort=trending&search=falcon) to see more Falcon-compatible models.\n\n\n\n",
        "token": [
            "Quickstart",
            ">",
            "*",
            "*",
            "Note",
            ":",
            "*",
            "*",
            "Falcon",
            "requires",
            "to",
            "install",
            "with",
            ":",
            ">",
            "``",
            "`",
            "bash",
            ">",
            "pip",
            "install",
            "``",
            "openllm",
            "[",
            "falcon",
            "]",
            "''",
            ">",
            "``",
            "`",
            "Run",
            "the",
            "following",
            "command",
            "to",
            "quickly",
            "spin",
            "up",
            "a",
            "Falcon",
            "server",
            ":",
            "``",
            "`",
            "bash",
            "openllm",
            "start",
            "tiiuae/falcon-7b",
            "``",
            "`",
            "In",
            "a",
            "different",
            "terminal",
            ",",
            "run",
            "the",
            "following",
            "command",
            "to",
            "interact",
            "with",
            "the",
            "server",
            ":",
            "``",
            "`",
            "bash",
            "export",
            "OPENLLM_ENDPOINT=http",
            ":",
            "//localhost:3000",
            "openllm",
            "query",
            "'What",
            "are",
            "large",
            "language",
            "models",
            "?",
            "'",
            "``",
            "`",
            ">",
            "*",
            "*",
            "Note",
            ":",
            "*",
            "*",
            "Any",
            "Falcon",
            "variants",
            "can",
            "be",
            "deployed",
            "with",
            "OpenLLM",
            ".",
            "Visit",
            "the",
            "[",
            "HuggingFace",
            "Model",
            "Hub",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/models",
            "?",
            "sort=trending",
            "&",
            "search=falcon",
            ")",
            "to",
            "see",
            "more",
            "Falcon-compatible",
            "models",
            "."
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/bentoml/OpenLLM",
        "readme_url": "https://raw.githubusercontent.com/bentoml/OpenLLM/main/README.md",
        "topic": [
            "ai",
            "bentoml",
            "falcon",
            "fine-tuning",
            "llama",
            "llama2",
            "llm",
            "llm-inference",
            "llm-ops",
            "llm-serving",
            "llmops",
            "mistral",
            "ml",
            "mlops",
            "model-inference",
            "mpt",
            "open-source-llm",
            "openllm",
            "stablelm",
            "vicuna"
        ],
        "text": "Supported backends\n\nOpenLLM will support vLLM and PyTorch as default backend. By default, it will use vLLM if vLLM is available, otherwise fallback to PyTorch.\n\n\n\n> **Important:** We recommend user to explicitly specify `--backend` to choose the desired backend to run the model. If you have access to a GPU, always use `--backend vllm`.\n\n\n\n- vLLM (Recommended):\n\n\nTo install vLLM, run `pip install \"openllm[vllm]\"`\n\n```bash\nopenllm start tiiuae/falcon-7b --backend vllm\n```\n\n\n> **Important:** Using vLLM requires a GPU that has architecture newer than 8.0 to get the best performance for serving. It is recommended that for all serving usecase in production, you should choose vLLM for serving.\n\n\n\n> **Note:** Currently, adapters are yet to be supported with vLLM.\n\n\n- PyTorch:\n\n\n```bash\nopenllm start tiiuae/falcon-7b --backend pt\n```\n\n</details>\n\n<details>\n\n<summary>FlanT5</summary>\n\n\n",
        "token": [
            "Supported",
            "backends",
            "OpenLLM",
            "will",
            "support",
            "vLLM",
            "and",
            "PyTorch",
            "as",
            "default",
            "backend",
            ".",
            "By",
            "default",
            ",",
            "it",
            "will",
            "use",
            "vLLM",
            "if",
            "vLLM",
            "is",
            "available",
            ",",
            "otherwise",
            "fallback",
            "to",
            "PyTorch",
            ".",
            ">",
            "*",
            "*",
            "Important",
            ":",
            "*",
            "*",
            "We",
            "recommend",
            "user",
            "to",
            "explicitly",
            "specify",
            "`",
            "--",
            "backend",
            "`",
            "to",
            "choose",
            "the",
            "desired",
            "backend",
            "to",
            "run",
            "the",
            "model",
            ".",
            "If",
            "you",
            "have",
            "access",
            "to",
            "a",
            "GPU",
            ",",
            "always",
            "use",
            "`",
            "--",
            "backend",
            "vllm",
            "`",
            ".",
            "-",
            "vLLM",
            "(",
            "Recommended",
            ")",
            ":",
            "To",
            "install",
            "vLLM",
            ",",
            "run",
            "`",
            "pip",
            "install",
            "``",
            "openllm",
            "[",
            "vllm",
            "]",
            "''",
            "`",
            "``",
            "`",
            "bash",
            "openllm",
            "start",
            "tiiuae/falcon-7b",
            "--",
            "backend",
            "vllm",
            "``",
            "`",
            ">",
            "*",
            "*",
            "Important",
            ":",
            "*",
            "*",
            "Using",
            "vLLM",
            "requires",
            "a",
            "GPU",
            "that",
            "has",
            "architecture",
            "newer",
            "than",
            "8.0",
            "to",
            "get",
            "the",
            "best",
            "performance",
            "for",
            "serving",
            ".",
            "It",
            "is",
            "recommended",
            "that",
            "for",
            "all",
            "serving",
            "usecase",
            "in",
            "production",
            ",",
            "you",
            "should",
            "choose",
            "vLLM",
            "for",
            "serving",
            ".",
            ">",
            "*",
            "*",
            "Note",
            ":",
            "*",
            "*",
            "Currently",
            ",",
            "adapters",
            "are",
            "yet",
            "to",
            "be",
            "supported",
            "with",
            "vLLM",
            ".",
            "-",
            "PyTorch",
            ":",
            "``",
            "`",
            "bash",
            "openllm",
            "start",
            "tiiuae/falcon-7b",
            "--",
            "backend",
            "pt",
            "``",
            "`",
            "<",
            "/details",
            ">",
            "<",
            "details",
            ">",
            "<",
            "summary",
            ">",
            "FlanT5",
            "<",
            "/summary",
            ">"
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/bentoml/OpenLLM",
        "readme_url": "https://raw.githubusercontent.com/bentoml/OpenLLM/main/README.md",
        "topic": [
            "ai",
            "bentoml",
            "falcon",
            "fine-tuning",
            "llama",
            "llama2",
            "llm",
            "llm-inference",
            "llm-ops",
            "llm-serving",
            "llmops",
            "mistral",
            "ml",
            "mlops",
            "model-inference",
            "mpt",
            "open-source-llm",
            "openllm",
            "stablelm",
            "vicuna"
        ],
        "text": "Supported backends\n\nOpenLLM will support vLLM and PyTorch as default backend. By default, it will use vLLM if vLLM is available, otherwise fallback to PyTorch.\n\n\n\n> **Important:** We recommend user to explicitly specify `--backend` to choose the desired backend to run the model. If you have access to a GPU, always use `--backend vllm`.\n\n\n\n- vLLM (Recommended):\n\n\nTo install vLLM, run `pip install \"openllm[vllm]\"`\n\n```bash\nopenllm start eleutherai/gpt-neox-20b --backend vllm\n```\n\n\n> **Important:** Using vLLM requires a GPU that has architecture newer than 8.0 to get the best performance for serving. It is recommended that for all serving usecase in production, you should choose vLLM for serving.\n\n\n\n> **Note:** Currently, adapters are yet to be supported with vLLM.\n\n\n- PyTorch:\n\n\n```bash\nopenllm start eleutherai/gpt-neox-20b --backend pt\n```\n\n</details>\n\n<details>\n\n<summary>Llama</summary>\n\n\n",
        "token": [
            "Supported",
            "backends",
            "OpenLLM",
            "will",
            "support",
            "vLLM",
            "and",
            "PyTorch",
            "as",
            "default",
            "backend",
            ".",
            "By",
            "default",
            ",",
            "it",
            "will",
            "use",
            "vLLM",
            "if",
            "vLLM",
            "is",
            "available",
            ",",
            "otherwise",
            "fallback",
            "to",
            "PyTorch",
            ".",
            ">",
            "*",
            "*",
            "Important",
            ":",
            "*",
            "*",
            "We",
            "recommend",
            "user",
            "to",
            "explicitly",
            "specify",
            "`",
            "--",
            "backend",
            "`",
            "to",
            "choose",
            "the",
            "desired",
            "backend",
            "to",
            "run",
            "the",
            "model",
            ".",
            "If",
            "you",
            "have",
            "access",
            "to",
            "a",
            "GPU",
            ",",
            "always",
            "use",
            "`",
            "--",
            "backend",
            "vllm",
            "`",
            ".",
            "-",
            "vLLM",
            "(",
            "Recommended",
            ")",
            ":",
            "To",
            "install",
            "vLLM",
            ",",
            "run",
            "`",
            "pip",
            "install",
            "``",
            "openllm",
            "[",
            "vllm",
            "]",
            "''",
            "`",
            "``",
            "`",
            "bash",
            "openllm",
            "start",
            "eleutherai/gpt-neox-20b",
            "--",
            "backend",
            "vllm",
            "``",
            "`",
            ">",
            "*",
            "*",
            "Important",
            ":",
            "*",
            "*",
            "Using",
            "vLLM",
            "requires",
            "a",
            "GPU",
            "that",
            "has",
            "architecture",
            "newer",
            "than",
            "8.0",
            "to",
            "get",
            "the",
            "best",
            "performance",
            "for",
            "serving",
            ".",
            "It",
            "is",
            "recommended",
            "that",
            "for",
            "all",
            "serving",
            "usecase",
            "in",
            "production",
            ",",
            "you",
            "should",
            "choose",
            "vLLM",
            "for",
            "serving",
            ".",
            ">",
            "*",
            "*",
            "Note",
            ":",
            "*",
            "*",
            "Currently",
            ",",
            "adapters",
            "are",
            "yet",
            "to",
            "be",
            "supported",
            "with",
            "vLLM",
            ".",
            "-",
            "PyTorch",
            ":",
            "``",
            "`",
            "bash",
            "openllm",
            "start",
            "eleutherai/gpt-neox-20b",
            "--",
            "backend",
            "pt",
            "``",
            "`",
            "<",
            "/details",
            ">",
            "<",
            "details",
            ">",
            "<",
            "summary",
            ">",
            "Llama",
            "<",
            "/summary",
            ">"
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/bentoml/OpenLLM",
        "readme_url": "https://raw.githubusercontent.com/bentoml/OpenLLM/main/README.md",
        "topic": [
            "ai",
            "bentoml",
            "falcon",
            "fine-tuning",
            "llama",
            "llama2",
            "llm",
            "llm-inference",
            "llm-ops",
            "llm-serving",
            "llmops",
            "mistral",
            "ml",
            "mlops",
            "model-inference",
            "mpt",
            "open-source-llm",
            "openllm",
            "stablelm",
            "vicuna"
        ],
        "text": "Supported backends\n\nOpenLLM will support vLLM and PyTorch as default backend. By default, it will use vLLM if vLLM is available, otherwise fallback to PyTorch.\n\n\n\n> **Important:** We recommend user to explicitly specify `--backend` to choose the desired backend to run the model. If you have access to a GPU, always use `--backend vllm`.\n\n\n\n- vLLM (Recommended):\n\n\nTo install vLLM, run `pip install \"openllm[vllm]\"`\n\n```bash\nopenllm start meta-llama/Llama-2-70b-chat-hf --backend vllm\n```\n\n\n> **Important:** Using vLLM requires a GPU that has architecture newer than 8.0 to get the best performance for serving. It is recommended that for all serving usecase in production, you should choose vLLM for serving.\n\n\n\n> **Note:** Currently, adapters are yet to be supported with vLLM.\n\n\n- PyTorch:\n\n\n```bash\nopenllm start meta-llama/Llama-2-70b-chat-hf --backend pt\n```\n\n</details>\n\n<details>\n\n<summary>Mistral</summary>\n\n\n",
        "token": [
            "Supported",
            "backends",
            "OpenLLM",
            "will",
            "support",
            "vLLM",
            "and",
            "PyTorch",
            "as",
            "default",
            "backend",
            ".",
            "By",
            "default",
            ",",
            "it",
            "will",
            "use",
            "vLLM",
            "if",
            "vLLM",
            "is",
            "available",
            ",",
            "otherwise",
            "fallback",
            "to",
            "PyTorch",
            ".",
            ">",
            "*",
            "*",
            "Important",
            ":",
            "*",
            "*",
            "We",
            "recommend",
            "user",
            "to",
            "explicitly",
            "specify",
            "`",
            "--",
            "backend",
            "`",
            "to",
            "choose",
            "the",
            "desired",
            "backend",
            "to",
            "run",
            "the",
            "model",
            ".",
            "If",
            "you",
            "have",
            "access",
            "to",
            "a",
            "GPU",
            ",",
            "always",
            "use",
            "`",
            "--",
            "backend",
            "vllm",
            "`",
            ".",
            "-",
            "vLLM",
            "(",
            "Recommended",
            ")",
            ":",
            "To",
            "install",
            "vLLM",
            ",",
            "run",
            "`",
            "pip",
            "install",
            "``",
            "openllm",
            "[",
            "vllm",
            "]",
            "''",
            "`",
            "``",
            "`",
            "bash",
            "openllm",
            "start",
            "meta-llama/Llama-2-70b-chat-hf",
            "--",
            "backend",
            "vllm",
            "``",
            "`",
            ">",
            "*",
            "*",
            "Important",
            ":",
            "*",
            "*",
            "Using",
            "vLLM",
            "requires",
            "a",
            "GPU",
            "that",
            "has",
            "architecture",
            "newer",
            "than",
            "8.0",
            "to",
            "get",
            "the",
            "best",
            "performance",
            "for",
            "serving",
            ".",
            "It",
            "is",
            "recommended",
            "that",
            "for",
            "all",
            "serving",
            "usecase",
            "in",
            "production",
            ",",
            "you",
            "should",
            "choose",
            "vLLM",
            "for",
            "serving",
            ".",
            ">",
            "*",
            "*",
            "Note",
            ":",
            "*",
            "*",
            "Currently",
            ",",
            "adapters",
            "are",
            "yet",
            "to",
            "be",
            "supported",
            "with",
            "vLLM",
            ".",
            "-",
            "PyTorch",
            ":",
            "``",
            "`",
            "bash",
            "openllm",
            "start",
            "meta-llama/Llama-2-70b-chat-hf",
            "--",
            "backend",
            "pt",
            "``",
            "`",
            "<",
            "/details",
            ">",
            "<",
            "details",
            ">",
            "<",
            "summary",
            ">",
            "Mistral",
            "<",
            "/summary",
            ">"
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/bentoml/OpenLLM",
        "readme_url": "https://raw.githubusercontent.com/bentoml/OpenLLM/main/README.md",
        "topic": [
            "ai",
            "bentoml",
            "falcon",
            "fine-tuning",
            "llama",
            "llama2",
            "llm",
            "llm-inference",
            "llm-ops",
            "llm-serving",
            "llmops",
            "mistral",
            "ml",
            "mlops",
            "model-inference",
            "mpt",
            "open-source-llm",
            "openllm",
            "stablelm",
            "vicuna"
        ],
        "text": "Supported backends\n\nOpenLLM will support vLLM and PyTorch as default backend. By default, it will use vLLM if vLLM is available, otherwise fallback to PyTorch.\n\n\n\n> **Important:** We recommend user to explicitly specify `--backend` to choose the desired backend to run the model. If you have access to a GPU, always use `--backend vllm`.\n\n\n\n- vLLM (Recommended):\n\n\nTo install vLLM, run `pip install \"openllm[vllm]\"`\n\n```bash\nopenllm start HuggingFaceH4/zephyr-7b-alpha --backend vllm\n```\n\n\n> **Important:** Using vLLM requires a GPU that has architecture newer than 8.0 to get the best performance for serving. It is recommended that for all serving usecase in production, you should choose vLLM for serving.\n\n\n\n> **Note:** Currently, adapters are yet to be supported with vLLM.\n\n\n- PyTorch:\n\n\n```bash\nopenllm start HuggingFaceH4/zephyr-7b-alpha --backend pt\n```\n\n</details>\n\n<details>\n\n<summary>Mixtral</summary>\n\n\n",
        "token": [
            "Supported",
            "backends",
            "OpenLLM",
            "will",
            "support",
            "vLLM",
            "and",
            "PyTorch",
            "as",
            "default",
            "backend",
            ".",
            "By",
            "default",
            ",",
            "it",
            "will",
            "use",
            "vLLM",
            "if",
            "vLLM",
            "is",
            "available",
            ",",
            "otherwise",
            "fallback",
            "to",
            "PyTorch",
            ".",
            ">",
            "*",
            "*",
            "Important",
            ":",
            "*",
            "*",
            "We",
            "recommend",
            "user",
            "to",
            "explicitly",
            "specify",
            "`",
            "--",
            "backend",
            "`",
            "to",
            "choose",
            "the",
            "desired",
            "backend",
            "to",
            "run",
            "the",
            "model",
            ".",
            "If",
            "you",
            "have",
            "access",
            "to",
            "a",
            "GPU",
            ",",
            "always",
            "use",
            "`",
            "--",
            "backend",
            "vllm",
            "`",
            ".",
            "-",
            "vLLM",
            "(",
            "Recommended",
            ")",
            ":",
            "To",
            "install",
            "vLLM",
            ",",
            "run",
            "`",
            "pip",
            "install",
            "``",
            "openllm",
            "[",
            "vllm",
            "]",
            "''",
            "`",
            "``",
            "`",
            "bash",
            "openllm",
            "start",
            "HuggingFaceH4/zephyr-7b-alpha",
            "--",
            "backend",
            "vllm",
            "``",
            "`",
            ">",
            "*",
            "*",
            "Important",
            ":",
            "*",
            "*",
            "Using",
            "vLLM",
            "requires",
            "a",
            "GPU",
            "that",
            "has",
            "architecture",
            "newer",
            "than",
            "8.0",
            "to",
            "get",
            "the",
            "best",
            "performance",
            "for",
            "serving",
            ".",
            "It",
            "is",
            "recommended",
            "that",
            "for",
            "all",
            "serving",
            "usecase",
            "in",
            "production",
            ",",
            "you",
            "should",
            "choose",
            "vLLM",
            "for",
            "serving",
            ".",
            ">",
            "*",
            "*",
            "Note",
            ":",
            "*",
            "*",
            "Currently",
            ",",
            "adapters",
            "are",
            "yet",
            "to",
            "be",
            "supported",
            "with",
            "vLLM",
            ".",
            "-",
            "PyTorch",
            ":",
            "``",
            "`",
            "bash",
            "openllm",
            "start",
            "HuggingFaceH4/zephyr-7b-alpha",
            "--",
            "backend",
            "pt",
            "``",
            "`",
            "<",
            "/details",
            ">",
            "<",
            "details",
            ">",
            "<",
            "summary",
            ">",
            "Mixtral",
            "<",
            "/summary",
            ">"
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/bentoml/OpenLLM",
        "readme_url": "https://raw.githubusercontent.com/bentoml/OpenLLM/main/README.md",
        "topic": [
            "ai",
            "bentoml",
            "falcon",
            "fine-tuning",
            "llama",
            "llama2",
            "llm",
            "llm-inference",
            "llm-ops",
            "llm-serving",
            "llmops",
            "mistral",
            "ml",
            "mlops",
            "model-inference",
            "mpt",
            "open-source-llm",
            "openllm",
            "stablelm",
            "vicuna"
        ],
        "text": "Supported backends\n\nOpenLLM will support vLLM and PyTorch as default backend. By default, it will use vLLM if vLLM is available, otherwise fallback to PyTorch.\n\n\n\n> **Important:** We recommend user to explicitly specify `--backend` to choose the desired backend to run the model. If you have access to a GPU, always use `--backend vllm`.\n\n\n\n- vLLM (Recommended):\n\n\nTo install vLLM, run `pip install \"openllm[vllm]\"`\n\n```bash\nopenllm start mistralai/Mixtral-8x7B-Instruct-v0.1 --backend vllm\n```\n\n\n> **Important:** Using vLLM requires a GPU that has architecture newer than 8.0 to get the best performance for serving. It is recommended that for all serving usecase in production, you should choose vLLM for serving.\n\n\n\n> **Note:** Currently, adapters are yet to be supported with vLLM.\n\n\n- PyTorch:\n\n\n```bash\nopenllm start mistralai/Mixtral-8x7B-Instruct-v0.1 --backend pt\n```\n\n</details>\n\n<details>\n\n<summary>MPT</summary>\n\n\n",
        "token": [
            "Supported",
            "backends",
            "OpenLLM",
            "will",
            "support",
            "vLLM",
            "and",
            "PyTorch",
            "as",
            "default",
            "backend",
            ".",
            "By",
            "default",
            ",",
            "it",
            "will",
            "use",
            "vLLM",
            "if",
            "vLLM",
            "is",
            "available",
            ",",
            "otherwise",
            "fallback",
            "to",
            "PyTorch",
            ".",
            ">",
            "*",
            "*",
            "Important",
            ":",
            "*",
            "*",
            "We",
            "recommend",
            "user",
            "to",
            "explicitly",
            "specify",
            "`",
            "--",
            "backend",
            "`",
            "to",
            "choose",
            "the",
            "desired",
            "backend",
            "to",
            "run",
            "the",
            "model",
            ".",
            "If",
            "you",
            "have",
            "access",
            "to",
            "a",
            "GPU",
            ",",
            "always",
            "use",
            "`",
            "--",
            "backend",
            "vllm",
            "`",
            ".",
            "-",
            "vLLM",
            "(",
            "Recommended",
            ")",
            ":",
            "To",
            "install",
            "vLLM",
            ",",
            "run",
            "`",
            "pip",
            "install",
            "``",
            "openllm",
            "[",
            "vllm",
            "]",
            "''",
            "`",
            "``",
            "`",
            "bash",
            "openllm",
            "start",
            "mistralai/Mixtral-8x7B-Instruct-v0.1",
            "--",
            "backend",
            "vllm",
            "``",
            "`",
            ">",
            "*",
            "*",
            "Important",
            ":",
            "*",
            "*",
            "Using",
            "vLLM",
            "requires",
            "a",
            "GPU",
            "that",
            "has",
            "architecture",
            "newer",
            "than",
            "8.0",
            "to",
            "get",
            "the",
            "best",
            "performance",
            "for",
            "serving",
            ".",
            "It",
            "is",
            "recommended",
            "that",
            "for",
            "all",
            "serving",
            "usecase",
            "in",
            "production",
            ",",
            "you",
            "should",
            "choose",
            "vLLM",
            "for",
            "serving",
            ".",
            ">",
            "*",
            "*",
            "Note",
            ":",
            "*",
            "*",
            "Currently",
            ",",
            "adapters",
            "are",
            "yet",
            "to",
            "be",
            "supported",
            "with",
            "vLLM",
            ".",
            "-",
            "PyTorch",
            ":",
            "``",
            "`",
            "bash",
            "openllm",
            "start",
            "mistralai/Mixtral-8x7B-Instruct-v0.1",
            "--",
            "backend",
            "pt",
            "``",
            "`",
            "<",
            "/details",
            ">",
            "<",
            "details",
            ">",
            "<",
            "summary",
            ">",
            "MPT",
            "<",
            "/summary",
            ">"
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/bentoml/OpenLLM",
        "readme_url": "https://raw.githubusercontent.com/bentoml/OpenLLM/main/README.md",
        "topic": [
            "ai",
            "bentoml",
            "falcon",
            "fine-tuning",
            "llama",
            "llama2",
            "llm",
            "llm-inference",
            "llm-ops",
            "llm-serving",
            "llmops",
            "mistral",
            "ml",
            "mlops",
            "model-inference",
            "mpt",
            "open-source-llm",
            "openllm",
            "stablelm",
            "vicuna"
        ],
        "text": "Quickstart\n\n\n\n> **Note:** MPT requires to install with:\n> ```bash\n> pip install \"openllm[mpt]\"\n> ```\n\n\nRun the following command to quickly spin up a MPT server:\n\n```bash\nTRUST_REMOTE_CODE=True openllm start mosaicml/mpt-7b-instruct\n```\nIn a different terminal, run the following command to interact with the server:\n\n```bash\nexport OPENLLM_ENDPOINT=http://localhost:3000\nopenllm query 'What are large language models?'\n```\n\n\n> **Note:** Any MPT variants can be deployed with OpenLLM. Visit the [HuggingFace Model Hub](https://huggingface.co/models?sort=trending&search=mpt) to see more MPT-compatible models.\n\n\n\n",
        "token": [
            "Quickstart",
            ">",
            "*",
            "*",
            "Note",
            ":",
            "*",
            "*",
            "MPT",
            "requires",
            "to",
            "install",
            "with",
            ":",
            ">",
            "``",
            "`",
            "bash",
            ">",
            "pip",
            "install",
            "``",
            "openllm",
            "[",
            "mpt",
            "]",
            "''",
            ">",
            "``",
            "`",
            "Run",
            "the",
            "following",
            "command",
            "to",
            "quickly",
            "spin",
            "up",
            "a",
            "MPT",
            "server",
            ":",
            "``",
            "`",
            "bash",
            "TRUST_REMOTE_CODE=True",
            "openllm",
            "start",
            "mosaicml/mpt-7b-instruct",
            "``",
            "`",
            "In",
            "a",
            "different",
            "terminal",
            ",",
            "run",
            "the",
            "following",
            "command",
            "to",
            "interact",
            "with",
            "the",
            "server",
            ":",
            "``",
            "`",
            "bash",
            "export",
            "OPENLLM_ENDPOINT=http",
            ":",
            "//localhost:3000",
            "openllm",
            "query",
            "'What",
            "are",
            "large",
            "language",
            "models",
            "?",
            "'",
            "``",
            "`",
            ">",
            "*",
            "*",
            "Note",
            ":",
            "*",
            "*",
            "Any",
            "MPT",
            "variants",
            "can",
            "be",
            "deployed",
            "with",
            "OpenLLM",
            ".",
            "Visit",
            "the",
            "[",
            "HuggingFace",
            "Model",
            "Hub",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/models",
            "?",
            "sort=trending",
            "&",
            "search=mpt",
            ")",
            "to",
            "see",
            "more",
            "MPT-compatible",
            "models",
            "."
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/bentoml/OpenLLM",
        "readme_url": "https://raw.githubusercontent.com/bentoml/OpenLLM/main/README.md",
        "topic": [
            "ai",
            "bentoml",
            "falcon",
            "fine-tuning",
            "llama",
            "llama2",
            "llm",
            "llm-inference",
            "llm-ops",
            "llm-serving",
            "llmops",
            "mistral",
            "ml",
            "mlops",
            "model-inference",
            "mpt",
            "open-source-llm",
            "openllm",
            "stablelm",
            "vicuna"
        ],
        "text": "Supported backends\n\nOpenLLM will support vLLM and PyTorch as default backend. By default, it will use vLLM if vLLM is available, otherwise fallback to PyTorch.\n\n\n\n> **Important:** We recommend user to explicitly specify `--backend` to choose the desired backend to run the model. If you have access to a GPU, always use `--backend vllm`.\n\n\n\n- vLLM (Recommended):\n\n\nTo install vLLM, run `pip install \"openllm[vllm]\"`\n\n```bash\nTRUST_REMOTE_CODE=True openllm start mosaicml/mpt-7b --backend vllm\n```\n\n\n> **Important:** Using vLLM requires a GPU that has architecture newer than 8.0 to get the best performance for serving. It is recommended that for all serving usecase in production, you should choose vLLM for serving.\n\n\n\n> **Note:** Currently, adapters are yet to be supported with vLLM.\n\n\n- PyTorch:\n\n\n```bash\nTRUST_REMOTE_CODE=True openllm start mosaicml/mpt-7b --backend pt\n```\n\n</details>\n\n<details>\n\n<summary>OPT</summary>\n\n\n",
        "token": [
            "Supported",
            "backends",
            "OpenLLM",
            "will",
            "support",
            "vLLM",
            "and",
            "PyTorch",
            "as",
            "default",
            "backend",
            ".",
            "By",
            "default",
            ",",
            "it",
            "will",
            "use",
            "vLLM",
            "if",
            "vLLM",
            "is",
            "available",
            ",",
            "otherwise",
            "fallback",
            "to",
            "PyTorch",
            ".",
            ">",
            "*",
            "*",
            "Important",
            ":",
            "*",
            "*",
            "We",
            "recommend",
            "user",
            "to",
            "explicitly",
            "specify",
            "`",
            "--",
            "backend",
            "`",
            "to",
            "choose",
            "the",
            "desired",
            "backend",
            "to",
            "run",
            "the",
            "model",
            ".",
            "If",
            "you",
            "have",
            "access",
            "to",
            "a",
            "GPU",
            ",",
            "always",
            "use",
            "`",
            "--",
            "backend",
            "vllm",
            "`",
            ".",
            "-",
            "vLLM",
            "(",
            "Recommended",
            ")",
            ":",
            "To",
            "install",
            "vLLM",
            ",",
            "run",
            "`",
            "pip",
            "install",
            "``",
            "openllm",
            "[",
            "vllm",
            "]",
            "''",
            "`",
            "``",
            "`",
            "bash",
            "TRUST_REMOTE_CODE=True",
            "openllm",
            "start",
            "mosaicml/mpt-7b",
            "--",
            "backend",
            "vllm",
            "``",
            "`",
            ">",
            "*",
            "*",
            "Important",
            ":",
            "*",
            "*",
            "Using",
            "vLLM",
            "requires",
            "a",
            "GPU",
            "that",
            "has",
            "architecture",
            "newer",
            "than",
            "8.0",
            "to",
            "get",
            "the",
            "best",
            "performance",
            "for",
            "serving",
            ".",
            "It",
            "is",
            "recommended",
            "that",
            "for",
            "all",
            "serving",
            "usecase",
            "in",
            "production",
            ",",
            "you",
            "should",
            "choose",
            "vLLM",
            "for",
            "serving",
            ".",
            ">",
            "*",
            "*",
            "Note",
            ":",
            "*",
            "*",
            "Currently",
            ",",
            "adapters",
            "are",
            "yet",
            "to",
            "be",
            "supported",
            "with",
            "vLLM",
            ".",
            "-",
            "PyTorch",
            ":",
            "``",
            "`",
            "bash",
            "TRUST_REMOTE_CODE=True",
            "openllm",
            "start",
            "mosaicml/mpt-7b",
            "--",
            "backend",
            "pt",
            "``",
            "`",
            "<",
            "/details",
            ">",
            "<",
            "details",
            ">",
            "<",
            "summary",
            ">",
            "OPT",
            "<",
            "/summary",
            ">"
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/bentoml/OpenLLM",
        "readme_url": "https://raw.githubusercontent.com/bentoml/OpenLLM/main/README.md",
        "topic": [
            "ai",
            "bentoml",
            "falcon",
            "fine-tuning",
            "llama",
            "llama2",
            "llm",
            "llm-inference",
            "llm-ops",
            "llm-serving",
            "llmops",
            "mistral",
            "ml",
            "mlops",
            "model-inference",
            "mpt",
            "open-source-llm",
            "openllm",
            "stablelm",
            "vicuna"
        ],
        "text": "Supported backends\n\nOpenLLM will support vLLM and PyTorch as default backend. By default, it will use vLLM if vLLM is available, otherwise fallback to PyTorch.\n\n\n\n> **Important:** We recommend user to explicitly specify `--backend` to choose the desired backend to run the model. If you have access to a GPU, always use `--backend vllm`.\n\n\n\n- vLLM (Recommended):\n\n\nTo install vLLM, run `pip install \"openllm[vllm]\"`\n\n```bash\nopenllm start facebook/opt-125m --backend vllm\n```\n\n\n> **Important:** Using vLLM requires a GPU that has architecture newer than 8.0 to get the best performance for serving. It is recommended that for all serving usecase in production, you should choose vLLM for serving.\n\n\n\n> **Note:** Currently, adapters are yet to be supported with vLLM.\n\n\n- PyTorch:\n\n\n```bash\nopenllm start facebook/opt-125m --backend pt\n```\n\n</details>\n\n<details>\n\n<summary>Phi</summary>\n\n\n",
        "token": [
            "Supported",
            "backends",
            "OpenLLM",
            "will",
            "support",
            "vLLM",
            "and",
            "PyTorch",
            "as",
            "default",
            "backend",
            ".",
            "By",
            "default",
            ",",
            "it",
            "will",
            "use",
            "vLLM",
            "if",
            "vLLM",
            "is",
            "available",
            ",",
            "otherwise",
            "fallback",
            "to",
            "PyTorch",
            ".",
            ">",
            "*",
            "*",
            "Important",
            ":",
            "*",
            "*",
            "We",
            "recommend",
            "user",
            "to",
            "explicitly",
            "specify",
            "`",
            "--",
            "backend",
            "`",
            "to",
            "choose",
            "the",
            "desired",
            "backend",
            "to",
            "run",
            "the",
            "model",
            ".",
            "If",
            "you",
            "have",
            "access",
            "to",
            "a",
            "GPU",
            ",",
            "always",
            "use",
            "`",
            "--",
            "backend",
            "vllm",
            "`",
            ".",
            "-",
            "vLLM",
            "(",
            "Recommended",
            ")",
            ":",
            "To",
            "install",
            "vLLM",
            ",",
            "run",
            "`",
            "pip",
            "install",
            "``",
            "openllm",
            "[",
            "vllm",
            "]",
            "''",
            "`",
            "``",
            "`",
            "bash",
            "openllm",
            "start",
            "facebook/opt-125m",
            "--",
            "backend",
            "vllm",
            "``",
            "`",
            ">",
            "*",
            "*",
            "Important",
            ":",
            "*",
            "*",
            "Using",
            "vLLM",
            "requires",
            "a",
            "GPU",
            "that",
            "has",
            "architecture",
            "newer",
            "than",
            "8.0",
            "to",
            "get",
            "the",
            "best",
            "performance",
            "for",
            "serving",
            ".",
            "It",
            "is",
            "recommended",
            "that",
            "for",
            "all",
            "serving",
            "usecase",
            "in",
            "production",
            ",",
            "you",
            "should",
            "choose",
            "vLLM",
            "for",
            "serving",
            ".",
            ">",
            "*",
            "*",
            "Note",
            ":",
            "*",
            "*",
            "Currently",
            ",",
            "adapters",
            "are",
            "yet",
            "to",
            "be",
            "supported",
            "with",
            "vLLM",
            ".",
            "-",
            "PyTorch",
            ":",
            "``",
            "`",
            "bash",
            "openllm",
            "start",
            "facebook/opt-125m",
            "--",
            "backend",
            "pt",
            "``",
            "`",
            "<",
            "/details",
            ">",
            "<",
            "details",
            ">",
            "<",
            "summary",
            ">",
            "Phi",
            "<",
            "/summary",
            ">"
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/bentoml/OpenLLM",
        "readme_url": "https://raw.githubusercontent.com/bentoml/OpenLLM/main/README.md",
        "topic": [
            "ai",
            "bentoml",
            "falcon",
            "fine-tuning",
            "llama",
            "llama2",
            "llm",
            "llm-inference",
            "llm-ops",
            "llm-serving",
            "llmops",
            "mistral",
            "ml",
            "mlops",
            "model-inference",
            "mpt",
            "open-source-llm",
            "openllm",
            "stablelm",
            "vicuna"
        ],
        "text": "Supported backends\n\nOpenLLM will support vLLM and PyTorch as default backend. By default, it will use vLLM if vLLM is available, otherwise fallback to PyTorch.\n\n\n\n> **Important:** We recommend user to explicitly specify `--backend` to choose the desired backend to run the model. If you have access to a GPU, always use `--backend vllm`.\n\n\n\n- vLLM (Recommended):\n\n\nTo install vLLM, run `pip install \"openllm[vllm]\"`\n\n```bash\nTRUST_REMOTE_CODE=True openllm start microsoft/phi-1_5 --backend vllm\n```\n\n\n> **Important:** Using vLLM requires a GPU that has architecture newer than 8.0 to get the best performance for serving. It is recommended that for all serving usecase in production, you should choose vLLM for serving.\n\n\n\n> **Note:** Currently, adapters are yet to be supported with vLLM.\n\n\n- PyTorch:\n\n\n```bash\nTRUST_REMOTE_CODE=True openllm start microsoft/phi-1_5 --backend pt\n```\n\n</details>\n\n<details>\n\n<summary>Qwen</summary>\n\n\n",
        "token": [
            "Supported",
            "backends",
            "OpenLLM",
            "will",
            "support",
            "vLLM",
            "and",
            "PyTorch",
            "as",
            "default",
            "backend",
            ".",
            "By",
            "default",
            ",",
            "it",
            "will",
            "use",
            "vLLM",
            "if",
            "vLLM",
            "is",
            "available",
            ",",
            "otherwise",
            "fallback",
            "to",
            "PyTorch",
            ".",
            ">",
            "*",
            "*",
            "Important",
            ":",
            "*",
            "*",
            "We",
            "recommend",
            "user",
            "to",
            "explicitly",
            "specify",
            "`",
            "--",
            "backend",
            "`",
            "to",
            "choose",
            "the",
            "desired",
            "backend",
            "to",
            "run",
            "the",
            "model",
            ".",
            "If",
            "you",
            "have",
            "access",
            "to",
            "a",
            "GPU",
            ",",
            "always",
            "use",
            "`",
            "--",
            "backend",
            "vllm",
            "`",
            ".",
            "-",
            "vLLM",
            "(",
            "Recommended",
            ")",
            ":",
            "To",
            "install",
            "vLLM",
            ",",
            "run",
            "`",
            "pip",
            "install",
            "``",
            "openllm",
            "[",
            "vllm",
            "]",
            "''",
            "`",
            "``",
            "`",
            "bash",
            "TRUST_REMOTE_CODE=True",
            "openllm",
            "start",
            "microsoft/phi-1_5",
            "--",
            "backend",
            "vllm",
            "``",
            "`",
            ">",
            "*",
            "*",
            "Important",
            ":",
            "*",
            "*",
            "Using",
            "vLLM",
            "requires",
            "a",
            "GPU",
            "that",
            "has",
            "architecture",
            "newer",
            "than",
            "8.0",
            "to",
            "get",
            "the",
            "best",
            "performance",
            "for",
            "serving",
            ".",
            "It",
            "is",
            "recommended",
            "that",
            "for",
            "all",
            "serving",
            "usecase",
            "in",
            "production",
            ",",
            "you",
            "should",
            "choose",
            "vLLM",
            "for",
            "serving",
            ".",
            ">",
            "*",
            "*",
            "Note",
            ":",
            "*",
            "*",
            "Currently",
            ",",
            "adapters",
            "are",
            "yet",
            "to",
            "be",
            "supported",
            "with",
            "vLLM",
            ".",
            "-",
            "PyTorch",
            ":",
            "``",
            "`",
            "bash",
            "TRUST_REMOTE_CODE=True",
            "openllm",
            "start",
            "microsoft/phi-1_5",
            "--",
            "backend",
            "pt",
            "``",
            "`",
            "<",
            "/details",
            ">",
            "<",
            "details",
            ">",
            "<",
            "summary",
            ">",
            "Qwen",
            "<",
            "/summary",
            ">"
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/bentoml/OpenLLM",
        "readme_url": "https://raw.githubusercontent.com/bentoml/OpenLLM/main/README.md",
        "topic": [
            "ai",
            "bentoml",
            "falcon",
            "fine-tuning",
            "llama",
            "llama2",
            "llm",
            "llm-inference",
            "llm-ops",
            "llm-serving",
            "llmops",
            "mistral",
            "ml",
            "mlops",
            "model-inference",
            "mpt",
            "open-source-llm",
            "openllm",
            "stablelm",
            "vicuna"
        ],
        "text": "Quickstart\n\n\n\n> **Note:** Qwen requires to install with:\n> ```bash\n> pip install \"openllm[qwen]\"\n> ```\n\n\nRun the following command to quickly spin up a Qwen server:\n\n```bash\nTRUST_REMOTE_CODE=True openllm start qwen/Qwen-7B-Chat\n```\nIn a different terminal, run the following command to interact with the server:\n\n```bash\nexport OPENLLM_ENDPOINT=http://localhost:3000\nopenllm query 'What are large language models?'\n```\n\n\n> **Note:** Any Qwen variants can be deployed with OpenLLM. Visit the [HuggingFace Model Hub](https://huggingface.co/models?sort=trending&search=qwen) to see more Qwen-compatible models.\n\n\n\n",
        "token": [
            "Quickstart",
            ">",
            "*",
            "*",
            "Note",
            ":",
            "*",
            "*",
            "Qwen",
            "requires",
            "to",
            "install",
            "with",
            ":",
            ">",
            "``",
            "`",
            "bash",
            ">",
            "pip",
            "install",
            "``",
            "openllm",
            "[",
            "qwen",
            "]",
            "''",
            ">",
            "``",
            "`",
            "Run",
            "the",
            "following",
            "command",
            "to",
            "quickly",
            "spin",
            "up",
            "a",
            "Qwen",
            "server",
            ":",
            "``",
            "`",
            "bash",
            "TRUST_REMOTE_CODE=True",
            "openllm",
            "start",
            "qwen/Qwen-7B-Chat",
            "``",
            "`",
            "In",
            "a",
            "different",
            "terminal",
            ",",
            "run",
            "the",
            "following",
            "command",
            "to",
            "interact",
            "with",
            "the",
            "server",
            ":",
            "``",
            "`",
            "bash",
            "export",
            "OPENLLM_ENDPOINT=http",
            ":",
            "//localhost:3000",
            "openllm",
            "query",
            "'What",
            "are",
            "large",
            "language",
            "models",
            "?",
            "'",
            "``",
            "`",
            ">",
            "*",
            "*",
            "Note",
            ":",
            "*",
            "*",
            "Any",
            "Qwen",
            "variants",
            "can",
            "be",
            "deployed",
            "with",
            "OpenLLM",
            ".",
            "Visit",
            "the",
            "[",
            "HuggingFace",
            "Model",
            "Hub",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/models",
            "?",
            "sort=trending",
            "&",
            "search=qwen",
            ")",
            "to",
            "see",
            "more",
            "Qwen-compatible",
            "models",
            "."
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/bentoml/OpenLLM",
        "readme_url": "https://raw.githubusercontent.com/bentoml/OpenLLM/main/README.md",
        "topic": [
            "ai",
            "bentoml",
            "falcon",
            "fine-tuning",
            "llama",
            "llama2",
            "llm",
            "llm-inference",
            "llm-ops",
            "llm-serving",
            "llmops",
            "mistral",
            "ml",
            "mlops",
            "model-inference",
            "mpt",
            "open-source-llm",
            "openllm",
            "stablelm",
            "vicuna"
        ],
        "text": "Supported backends\n\nOpenLLM will support vLLM and PyTorch as default backend. By default, it will use vLLM if vLLM is available, otherwise fallback to PyTorch.\n\n\n\n> **Important:** We recommend user to explicitly specify `--backend` to choose the desired backend to run the model. If you have access to a GPU, always use `--backend vllm`.\n\n\n\n- vLLM (Recommended):\n\n\nTo install vLLM, run `pip install \"openllm[vllm]\"`\n\n```bash\nTRUST_REMOTE_CODE=True openllm start qwen/Qwen-7B-Chat --backend vllm\n```\n\n\n> **Important:** Using vLLM requires a GPU that has architecture newer than 8.0 to get the best performance for serving. It is recommended that for all serving usecase in production, you should choose vLLM for serving.\n\n\n\n> **Note:** Currently, adapters are yet to be supported with vLLM.\n\n\n- PyTorch:\n\n\n```bash\nTRUST_REMOTE_CODE=True openllm start qwen/Qwen-7B-Chat --backend pt\n```\n\n</details>\n\n<details>\n\n<summary>StableLM</summary>\n\n\n",
        "token": [
            "Supported",
            "backends",
            "OpenLLM",
            "will",
            "support",
            "vLLM",
            "and",
            "PyTorch",
            "as",
            "default",
            "backend",
            ".",
            "By",
            "default",
            ",",
            "it",
            "will",
            "use",
            "vLLM",
            "if",
            "vLLM",
            "is",
            "available",
            ",",
            "otherwise",
            "fallback",
            "to",
            "PyTorch",
            ".",
            ">",
            "*",
            "*",
            "Important",
            ":",
            "*",
            "*",
            "We",
            "recommend",
            "user",
            "to",
            "explicitly",
            "specify",
            "`",
            "--",
            "backend",
            "`",
            "to",
            "choose",
            "the",
            "desired",
            "backend",
            "to",
            "run",
            "the",
            "model",
            ".",
            "If",
            "you",
            "have",
            "access",
            "to",
            "a",
            "GPU",
            ",",
            "always",
            "use",
            "`",
            "--",
            "backend",
            "vllm",
            "`",
            ".",
            "-",
            "vLLM",
            "(",
            "Recommended",
            ")",
            ":",
            "To",
            "install",
            "vLLM",
            ",",
            "run",
            "`",
            "pip",
            "install",
            "``",
            "openllm",
            "[",
            "vllm",
            "]",
            "''",
            "`",
            "``",
            "`",
            "bash",
            "TRUST_REMOTE_CODE=True",
            "openllm",
            "start",
            "qwen/Qwen-7B-Chat",
            "--",
            "backend",
            "vllm",
            "``",
            "`",
            ">",
            "*",
            "*",
            "Important",
            ":",
            "*",
            "*",
            "Using",
            "vLLM",
            "requires",
            "a",
            "GPU",
            "that",
            "has",
            "architecture",
            "newer",
            "than",
            "8.0",
            "to",
            "get",
            "the",
            "best",
            "performance",
            "for",
            "serving",
            ".",
            "It",
            "is",
            "recommended",
            "that",
            "for",
            "all",
            "serving",
            "usecase",
            "in",
            "production",
            ",",
            "you",
            "should",
            "choose",
            "vLLM",
            "for",
            "serving",
            ".",
            ">",
            "*",
            "*",
            "Note",
            ":",
            "*",
            "*",
            "Currently",
            ",",
            "adapters",
            "are",
            "yet",
            "to",
            "be",
            "supported",
            "with",
            "vLLM",
            ".",
            "-",
            "PyTorch",
            ":",
            "``",
            "`",
            "bash",
            "TRUST_REMOTE_CODE=True",
            "openllm",
            "start",
            "qwen/Qwen-7B-Chat",
            "--",
            "backend",
            "pt",
            "``",
            "`",
            "<",
            "/details",
            ">",
            "<",
            "details",
            ">",
            "<",
            "summary",
            ">",
            "StableLM",
            "<",
            "/summary",
            ">"
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/bentoml/OpenLLM",
        "readme_url": "https://raw.githubusercontent.com/bentoml/OpenLLM/main/README.md",
        "topic": [
            "ai",
            "bentoml",
            "falcon",
            "fine-tuning",
            "llama",
            "llama2",
            "llm",
            "llm-inference",
            "llm-ops",
            "llm-serving",
            "llmops",
            "mistral",
            "ml",
            "mlops",
            "model-inference",
            "mpt",
            "open-source-llm",
            "openllm",
            "stablelm",
            "vicuna"
        ],
        "text": "Supported backends\n\nOpenLLM will support vLLM and PyTorch as default backend. By default, it will use vLLM if vLLM is available, otherwise fallback to PyTorch.\n\n\n\n> **Important:** We recommend user to explicitly specify `--backend` to choose the desired backend to run the model. If you have access to a GPU, always use `--backend vllm`.\n\n\n\n- vLLM (Recommended):\n\n\nTo install vLLM, run `pip install \"openllm[vllm]\"`\n\n```bash\nopenllm start stabilityai/stablelm-tuned-alpha-3b --backend vllm\n```\n\n\n> **Important:** Using vLLM requires a GPU that has architecture newer than 8.0 to get the best performance for serving. It is recommended that for all serving usecase in production, you should choose vLLM for serving.\n\n\n\n> **Note:** Currently, adapters are yet to be supported with vLLM.\n\n\n- PyTorch:\n\n\n```bash\nopenllm start stabilityai/stablelm-tuned-alpha-3b --backend pt\n```\n\n</details>\n\n<details>\n\n<summary>StarCoder</summary>\n\n\n",
        "token": [
            "Supported",
            "backends",
            "OpenLLM",
            "will",
            "support",
            "vLLM",
            "and",
            "PyTorch",
            "as",
            "default",
            "backend",
            ".",
            "By",
            "default",
            ",",
            "it",
            "will",
            "use",
            "vLLM",
            "if",
            "vLLM",
            "is",
            "available",
            ",",
            "otherwise",
            "fallback",
            "to",
            "PyTorch",
            ".",
            ">",
            "*",
            "*",
            "Important",
            ":",
            "*",
            "*",
            "We",
            "recommend",
            "user",
            "to",
            "explicitly",
            "specify",
            "`",
            "--",
            "backend",
            "`",
            "to",
            "choose",
            "the",
            "desired",
            "backend",
            "to",
            "run",
            "the",
            "model",
            ".",
            "If",
            "you",
            "have",
            "access",
            "to",
            "a",
            "GPU",
            ",",
            "always",
            "use",
            "`",
            "--",
            "backend",
            "vllm",
            "`",
            ".",
            "-",
            "vLLM",
            "(",
            "Recommended",
            ")",
            ":",
            "To",
            "install",
            "vLLM",
            ",",
            "run",
            "`",
            "pip",
            "install",
            "``",
            "openllm",
            "[",
            "vllm",
            "]",
            "''",
            "`",
            "``",
            "`",
            "bash",
            "openllm",
            "start",
            "stabilityai/stablelm-tuned-alpha-3b",
            "--",
            "backend",
            "vllm",
            "``",
            "`",
            ">",
            "*",
            "*",
            "Important",
            ":",
            "*",
            "*",
            "Using",
            "vLLM",
            "requires",
            "a",
            "GPU",
            "that",
            "has",
            "architecture",
            "newer",
            "than",
            "8.0",
            "to",
            "get",
            "the",
            "best",
            "performance",
            "for",
            "serving",
            ".",
            "It",
            "is",
            "recommended",
            "that",
            "for",
            "all",
            "serving",
            "usecase",
            "in",
            "production",
            ",",
            "you",
            "should",
            "choose",
            "vLLM",
            "for",
            "serving",
            ".",
            ">",
            "*",
            "*",
            "Note",
            ":",
            "*",
            "*",
            "Currently",
            ",",
            "adapters",
            "are",
            "yet",
            "to",
            "be",
            "supported",
            "with",
            "vLLM",
            ".",
            "-",
            "PyTorch",
            ":",
            "``",
            "`",
            "bash",
            "openllm",
            "start",
            "stabilityai/stablelm-tuned-alpha-3b",
            "--",
            "backend",
            "pt",
            "``",
            "`",
            "<",
            "/details",
            ">",
            "<",
            "details",
            ">",
            "<",
            "summary",
            ">",
            "StarCoder",
            "<",
            "/summary",
            ">"
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/bentoml/OpenLLM",
        "readme_url": "https://raw.githubusercontent.com/bentoml/OpenLLM/main/README.md",
        "topic": [
            "ai",
            "bentoml",
            "falcon",
            "fine-tuning",
            "llama",
            "llama2",
            "llm",
            "llm-inference",
            "llm-ops",
            "llm-serving",
            "llmops",
            "mistral",
            "ml",
            "mlops",
            "model-inference",
            "mpt",
            "open-source-llm",
            "openllm",
            "stablelm",
            "vicuna"
        ],
        "text": "Quickstart\n\n\n\n> **Note:** StarCoder requires to install with:\n> ```bash\n> pip install \"openllm[starcoder]\"\n> ```\n\n\nRun the following command to quickly spin up a StarCoder server:\n\n```bash\nopenllm start bigcode/starcoder\n```\nIn a different terminal, run the following command to interact with the server:\n\n```bash\nexport OPENLLM_ENDPOINT=http://localhost:3000\nopenllm query 'What are large language models?'\n```\n\n\n> **Note:** Any StarCoder variants can be deployed with OpenLLM. Visit the [HuggingFace Model Hub](https://huggingface.co/models?sort=trending&search=starcoder) to see more StarCoder-compatible models.\n\n\n\n",
        "token": [
            "Quickstart",
            ">",
            "*",
            "*",
            "Note",
            ":",
            "*",
            "*",
            "StarCoder",
            "requires",
            "to",
            "install",
            "with",
            ":",
            ">",
            "``",
            "`",
            "bash",
            ">",
            "pip",
            "install",
            "``",
            "openllm",
            "[",
            "starcoder",
            "]",
            "''",
            ">",
            "``",
            "`",
            "Run",
            "the",
            "following",
            "command",
            "to",
            "quickly",
            "spin",
            "up",
            "a",
            "StarCoder",
            "server",
            ":",
            "``",
            "`",
            "bash",
            "openllm",
            "start",
            "bigcode/starcoder",
            "``",
            "`",
            "In",
            "a",
            "different",
            "terminal",
            ",",
            "run",
            "the",
            "following",
            "command",
            "to",
            "interact",
            "with",
            "the",
            "server",
            ":",
            "``",
            "`",
            "bash",
            "export",
            "OPENLLM_ENDPOINT=http",
            ":",
            "//localhost:3000",
            "openllm",
            "query",
            "'What",
            "are",
            "large",
            "language",
            "models",
            "?",
            "'",
            "``",
            "`",
            ">",
            "*",
            "*",
            "Note",
            ":",
            "*",
            "*",
            "Any",
            "StarCoder",
            "variants",
            "can",
            "be",
            "deployed",
            "with",
            "OpenLLM",
            ".",
            "Visit",
            "the",
            "[",
            "HuggingFace",
            "Model",
            "Hub",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/models",
            "?",
            "sort=trending",
            "&",
            "search=starcoder",
            ")",
            "to",
            "see",
            "more",
            "StarCoder-compatible",
            "models",
            "."
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/bentoml/OpenLLM",
        "readme_url": "https://raw.githubusercontent.com/bentoml/OpenLLM/main/README.md",
        "topic": [
            "ai",
            "bentoml",
            "falcon",
            "fine-tuning",
            "llama",
            "llama2",
            "llm",
            "llm-inference",
            "llm-ops",
            "llm-serving",
            "llmops",
            "mistral",
            "ml",
            "mlops",
            "model-inference",
            "mpt",
            "open-source-llm",
            "openllm",
            "stablelm",
            "vicuna"
        ],
        "text": "Supported backends\n\nOpenLLM will support vLLM and PyTorch as default backend. By default, it will use vLLM if vLLM is available, otherwise fallback to PyTorch.\n\n\n\n> **Important:** We recommend user to explicitly specify `--backend` to choose the desired backend to run the model. If you have access to a GPU, always use `--backend vllm`.\n\n\n\n- vLLM (Recommended):\n\n\nTo install vLLM, run `pip install \"openllm[vllm]\"`\n\n```bash\nopenllm start bigcode/starcoder --backend vllm\n```\n\n\n> **Important:** Using vLLM requires a GPU that has architecture newer than 8.0 to get the best performance for serving. It is recommended that for all serving usecase in production, you should choose vLLM for serving.\n\n\n\n> **Note:** Currently, adapters are yet to be supported with vLLM.\n\n\n- PyTorch:\n\n\n```bash\nopenllm start bigcode/starcoder --backend pt\n```\n\n</details>\n\n<details>\n\n<summary>Yi</summary>\n\n\n",
        "token": [
            "Supported",
            "backends",
            "OpenLLM",
            "will",
            "support",
            "vLLM",
            "and",
            "PyTorch",
            "as",
            "default",
            "backend",
            ".",
            "By",
            "default",
            ",",
            "it",
            "will",
            "use",
            "vLLM",
            "if",
            "vLLM",
            "is",
            "available",
            ",",
            "otherwise",
            "fallback",
            "to",
            "PyTorch",
            ".",
            ">",
            "*",
            "*",
            "Important",
            ":",
            "*",
            "*",
            "We",
            "recommend",
            "user",
            "to",
            "explicitly",
            "specify",
            "`",
            "--",
            "backend",
            "`",
            "to",
            "choose",
            "the",
            "desired",
            "backend",
            "to",
            "run",
            "the",
            "model",
            ".",
            "If",
            "you",
            "have",
            "access",
            "to",
            "a",
            "GPU",
            ",",
            "always",
            "use",
            "`",
            "--",
            "backend",
            "vllm",
            "`",
            ".",
            "-",
            "vLLM",
            "(",
            "Recommended",
            ")",
            ":",
            "To",
            "install",
            "vLLM",
            ",",
            "run",
            "`",
            "pip",
            "install",
            "``",
            "openllm",
            "[",
            "vllm",
            "]",
            "''",
            "`",
            "``",
            "`",
            "bash",
            "openllm",
            "start",
            "bigcode/starcoder",
            "--",
            "backend",
            "vllm",
            "``",
            "`",
            ">",
            "*",
            "*",
            "Important",
            ":",
            "*",
            "*",
            "Using",
            "vLLM",
            "requires",
            "a",
            "GPU",
            "that",
            "has",
            "architecture",
            "newer",
            "than",
            "8.0",
            "to",
            "get",
            "the",
            "best",
            "performance",
            "for",
            "serving",
            ".",
            "It",
            "is",
            "recommended",
            "that",
            "for",
            "all",
            "serving",
            "usecase",
            "in",
            "production",
            ",",
            "you",
            "should",
            "choose",
            "vLLM",
            "for",
            "serving",
            ".",
            ">",
            "*",
            "*",
            "Note",
            ":",
            "*",
            "*",
            "Currently",
            ",",
            "adapters",
            "are",
            "yet",
            "to",
            "be",
            "supported",
            "with",
            "vLLM",
            ".",
            "-",
            "PyTorch",
            ":",
            "``",
            "`",
            "bash",
            "openllm",
            "start",
            "bigcode/starcoder",
            "--",
            "backend",
            "pt",
            "``",
            "`",
            "<",
            "/details",
            ">",
            "<",
            "details",
            ">",
            "<",
            "summary",
            ">",
            "Yi",
            "<",
            "/summary",
            ">"
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/bentoml/OpenLLM",
        "readme_url": "https://raw.githubusercontent.com/bentoml/OpenLLM/main/README.md",
        "topic": [
            "ai",
            "bentoml",
            "falcon",
            "fine-tuning",
            "llama",
            "llama2",
            "llm",
            "llm-inference",
            "llm-ops",
            "llm-serving",
            "llmops",
            "mistral",
            "ml",
            "mlops",
            "model-inference",
            "mpt",
            "open-source-llm",
            "openllm",
            "stablelm",
            "vicuna"
        ],
        "text": "Supported backends\n\nOpenLLM will support vLLM and PyTorch as default backend. By default, it will use vLLM if vLLM is available, otherwise fallback to PyTorch.\n\n\n\n> **Important:** We recommend user to explicitly specify `--backend` to choose the desired backend to run the model. If you have access to a GPU, always use `--backend vllm`.\n\n\n\n- vLLM (Recommended):\n\n\nTo install vLLM, run `pip install \"openllm[vllm]\"`\n\n```bash\nTRUST_REMOTE_CODE=True openllm start 01-ai/Yi-6B --backend vllm\n```\n\n\n> **Important:** Using vLLM requires a GPU that has architecture newer than 8.0 to get the best performance for serving. It is recommended that for all serving usecase in production, you should choose vLLM for serving.\n\n\n\n> **Note:** Currently, adapters are yet to be supported with vLLM.\n\n\n- PyTorch:\n\n\n```bash\nTRUST_REMOTE_CODE=True openllm start 01-ai/Yi-6B --backend pt\n```\n\n</details>\n\n<!-- update-readme.py: stop -->\n\nMore models will be integrated with OpenLLM and we welcome your contributions if you want to incorporate your custom LLMs into the ecosystem. Check out [Adding a New Model Guide](https://github.com/bentoml/OpenLLM/blob/main/ADDING_NEW_MODEL.md) to learn more.\n\n",
        "token": [
            "Supported",
            "backends",
            "OpenLLM",
            "will",
            "support",
            "vLLM",
            "and",
            "PyTorch",
            "as",
            "default",
            "backend",
            ".",
            "By",
            "default",
            ",",
            "it",
            "will",
            "use",
            "vLLM",
            "if",
            "vLLM",
            "is",
            "available",
            ",",
            "otherwise",
            "fallback",
            "to",
            "PyTorch",
            ".",
            ">",
            "*",
            "*",
            "Important",
            ":",
            "*",
            "*",
            "We",
            "recommend",
            "user",
            "to",
            "explicitly",
            "specify",
            "`",
            "--",
            "backend",
            "`",
            "to",
            "choose",
            "the",
            "desired",
            "backend",
            "to",
            "run",
            "the",
            "model",
            ".",
            "If",
            "you",
            "have",
            "access",
            "to",
            "a",
            "GPU",
            ",",
            "always",
            "use",
            "`",
            "--",
            "backend",
            "vllm",
            "`",
            ".",
            "-",
            "vLLM",
            "(",
            "Recommended",
            ")",
            ":",
            "To",
            "install",
            "vLLM",
            ",",
            "run",
            "`",
            "pip",
            "install",
            "``",
            "openllm",
            "[",
            "vllm",
            "]",
            "''",
            "`",
            "``",
            "`",
            "bash",
            "TRUST_REMOTE_CODE=True",
            "openllm",
            "start",
            "01-ai/Yi-6B",
            "--",
            "backend",
            "vllm",
            "``",
            "`",
            ">",
            "*",
            "*",
            "Important",
            ":",
            "*",
            "*",
            "Using",
            "vLLM",
            "requires",
            "a",
            "GPU",
            "that",
            "has",
            "architecture",
            "newer",
            "than",
            "8.0",
            "to",
            "get",
            "the",
            "best",
            "performance",
            "for",
            "serving",
            ".",
            "It",
            "is",
            "recommended",
            "that",
            "for",
            "all",
            "serving",
            "usecase",
            "in",
            "production",
            ",",
            "you",
            "should",
            "choose",
            "vLLM",
            "for",
            "serving",
            ".",
            ">",
            "*",
            "*",
            "Note",
            ":",
            "*",
            "*",
            "Currently",
            ",",
            "adapters",
            "are",
            "yet",
            "to",
            "be",
            "supported",
            "with",
            "vLLM",
            ".",
            "-",
            "PyTorch",
            ":",
            "``",
            "`",
            "bash",
            "TRUST_REMOTE_CODE=True",
            "openllm",
            "start",
            "01-ai/Yi-6B",
            "--",
            "backend",
            "pt",
            "``",
            "`",
            "<",
            "/details",
            ">",
            "<",
            "!",
            "--",
            "update-readme.py",
            ":",
            "stop",
            "--",
            ">",
            "More",
            "models",
            "will",
            "be",
            "integrated",
            "with",
            "OpenLLM",
            "and",
            "we",
            "welcome",
            "your",
            "contributions",
            "if",
            "you",
            "want",
            "to",
            "incorporate",
            "your",
            "custom",
            "LLMs",
            "into",
            "the",
            "ecosystem",
            ".",
            "Check",
            "out",
            "[",
            "Adding",
            "a",
            "New",
            "Model",
            "Guide",
            "]",
            "(",
            "https",
            ":",
            "//github.com/bentoml/OpenLLM/blob/main/ADDING_NEW_MODEL.md",
            ")",
            "to",
            "learn",
            "more",
            "."
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/bentoml/OpenLLM",
        "readme_url": "https://raw.githubusercontent.com/bentoml/OpenLLM/main/README.md",
        "topic": [
            "ai",
            "bentoml",
            "falcon",
            "fine-tuning",
            "llama",
            "llama2",
            "llm",
            "llm-inference",
            "llm-ops",
            "llm-serving",
            "llmops",
            "mistral",
            "ml",
            "mlops",
            "model-inference",
            "mpt",
            "open-source-llm",
            "openllm",
            "stablelm",
            "vicuna"
        ],
        "text": "PyTorch backend\n\nWith PyTorch backend, OpenLLM supports `int8`, `int4`, and `gptq`.\n\nFor using int8 and int4 quantization through `bitsandbytes`, you can use the following command:\n\n```bash\nTRUST_REMOTE_CODE=True openllm start microsoft/phi-2 --quantize int8\n```\n\nTo run inference with\u00a0`gptq`, simply pass\u00a0`--quantize gptq`:\n\n```bash\nopenllm start TheBloke/Llama-2-7B-Chat-GPTQ --quantize gptq\n```\n\n> [!NOTE]\n> In order to run GPTQ, make sure you run\u00a0`pip install \"openllm[gptq]\"`\n> first to install the dependency. From the GPTQ paper, it is recommended to quantized the weights before serving.\n> See\u00a0[AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)\u00a0for more information on GPTQ quantization.\n\n",
        "token": [
            "PyTorch",
            "backend",
            "With",
            "PyTorch",
            "backend",
            ",",
            "OpenLLM",
            "supports",
            "`",
            "int8",
            "`",
            ",",
            "`",
            "int4",
            "`",
            ",",
            "and",
            "`",
            "gptq",
            "`",
            ".",
            "For",
            "using",
            "int8",
            "and",
            "int4",
            "quantization",
            "through",
            "`",
            "bitsandbytes",
            "`",
            ",",
            "you",
            "can",
            "use",
            "the",
            "following",
            "command",
            ":",
            "``",
            "`",
            "bash",
            "TRUST_REMOTE_CODE=True",
            "openllm",
            "start",
            "microsoft/phi-2",
            "--",
            "quantize",
            "int8",
            "``",
            "`",
            "To",
            "run",
            "inference",
            "with",
            "`",
            "gptq",
            "`",
            ",",
            "simply",
            "pass",
            "`",
            "--",
            "quantize",
            "gptq",
            "`",
            ":",
            "``",
            "`",
            "bash",
            "openllm",
            "start",
            "TheBloke/Llama-2-7B-Chat-GPTQ",
            "--",
            "quantize",
            "gptq",
            "``",
            "`",
            ">",
            "[",
            "!",
            "NOTE",
            "]",
            ">",
            "In",
            "order",
            "to",
            "run",
            "GPTQ",
            ",",
            "make",
            "sure",
            "you",
            "run",
            "`",
            "pip",
            "install",
            "``",
            "openllm",
            "[",
            "gptq",
            "]",
            "''",
            "`",
            ">",
            "first",
            "to",
            "install",
            "the",
            "dependency",
            ".",
            "From",
            "the",
            "GPTQ",
            "paper",
            ",",
            "it",
            "is",
            "recommended",
            "to",
            "quantized",
            "the",
            "weights",
            "before",
            "serving",
            ".",
            ">",
            "See",
            "[",
            "AutoGPTQ",
            "]",
            "(",
            "https",
            ":",
            "//github.com/PanQiWei/AutoGPTQ",
            ")",
            "for",
            "more",
            "information",
            "on",
            "GPTQ",
            "quantization",
            "."
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/bentoml/OpenLLM",
        "readme_url": "https://raw.githubusercontent.com/bentoml/OpenLLM/main/README.md",
        "topic": [
            "ai",
            "bentoml",
            "falcon",
            "fine-tuning",
            "llama",
            "llama2",
            "llm",
            "llm-inference",
            "llm-ops",
            "llm-serving",
            "llmops",
            "mistral",
            "ml",
            "mlops",
            "model-inference",
            "mpt",
            "open-source-llm",
            "openllm",
            "stablelm",
            "vicuna"
        ],
        "text": "\ud83d\udc0d Python SDK\n\nEach LLM can be instantiated with `openllm.LLM`:\n\n```python\nimport openllm\n\nllm = openllm.LLM('microsoft/phi-2')\n```\n\nThe main inference API is the streaming `generate_iterator` method:\n\n```python\nasync for generation in llm.generate_iterator('What is the meaning of life?'):\n  print(generation.outputs[0].text)\n```\n\n> [!NOTE]\n> The motivation behind making `llm.generate_iterator` an async generator is to provide support for Continuous batching with vLLM backend. By having the async endpoints, each prompt\n> will be added correctly to the request queue to process with vLLM backend.\n\nThere is also a _one-shot_ `generate` method:\n\n```python\nawait llm.generate('What is the meaning of life?')\n```\n\nThis method is easy to use for one-shot generation use case, but merely served as an example how to use `llm.generate_iterator` as it uses `generate_iterator` under the hood.\n\n> [!IMPORTANT]\n> If you need to call your code in a synchronous context, you can use `asyncio.run` that wraps an async function:\n>\n> ```python\n> import asyncio\n> async def generate(prompt, **attrs): return await llm.generate(prompt, **attrs)\n> asyncio.run(generate(\"The meaning of life is\", temperature=0.23))\n> ```\n\n",
        "token": [
            "\ud83d\udc0d",
            "Python",
            "SDK",
            "Each",
            "LLM",
            "can",
            "be",
            "instantiated",
            "with",
            "`",
            "openllm.LLM",
            "`",
            ":",
            "``",
            "`",
            "python",
            "import",
            "openllm",
            "llm",
            "=",
            "openllm.LLM",
            "(",
            "'microsoft/phi-2",
            "'",
            ")",
            "``",
            "`",
            "The",
            "main",
            "inference",
            "API",
            "is",
            "the",
            "streaming",
            "`",
            "generate_iterator",
            "`",
            "method",
            ":",
            "``",
            "`",
            "python",
            "async",
            "for",
            "generation",
            "in",
            "llm.generate_iterator",
            "(",
            "'What",
            "is",
            "the",
            "meaning",
            "of",
            "life",
            "?",
            "'",
            ")",
            ":",
            "print",
            "(",
            "generation.outputs",
            "[",
            "0",
            "]",
            ".text",
            ")",
            "``",
            "`",
            ">",
            "[",
            "!",
            "NOTE",
            "]",
            ">",
            "The",
            "motivation",
            "behind",
            "making",
            "`",
            "llm.generate_iterator",
            "`",
            "an",
            "async",
            "generator",
            "is",
            "to",
            "provide",
            "support",
            "for",
            "Continuous",
            "batching",
            "with",
            "vLLM",
            "backend",
            ".",
            "By",
            "having",
            "the",
            "async",
            "endpoints",
            ",",
            "each",
            "prompt",
            ">",
            "will",
            "be",
            "added",
            "correctly",
            "to",
            "the",
            "request",
            "queue",
            "to",
            "process",
            "with",
            "vLLM",
            "backend",
            ".",
            "There",
            "is",
            "also",
            "a",
            "_one-shot_",
            "`",
            "generate",
            "`",
            "method",
            ":",
            "``",
            "`",
            "python",
            "await",
            "llm.generate",
            "(",
            "'What",
            "is",
            "the",
            "meaning",
            "of",
            "life",
            "?",
            "'",
            ")",
            "``",
            "`",
            "This",
            "method",
            "is",
            "easy",
            "to",
            "use",
            "for",
            "one-shot",
            "generation",
            "use",
            "case",
            ",",
            "but",
            "merely",
            "served",
            "as",
            "an",
            "example",
            "how",
            "to",
            "use",
            "`",
            "llm.generate_iterator",
            "`",
            "as",
            "it",
            "uses",
            "`",
            "generate_iterator",
            "`",
            "under",
            "the",
            "hood",
            ".",
            ">",
            "[",
            "!",
            "IMPORTANT",
            "]",
            ">",
            "If",
            "you",
            "need",
            "to",
            "call",
            "your",
            "code",
            "in",
            "a",
            "synchronous",
            "context",
            ",",
            "you",
            "can",
            "use",
            "`",
            "asyncio.run",
            "`",
            "that",
            "wraps",
            "an",
            "async",
            "function",
            ":",
            ">",
            ">",
            "``",
            "`",
            "python",
            ">",
            "import",
            "asyncio",
            ">",
            "async",
            "def",
            "generate",
            "(",
            "prompt",
            ",",
            "*",
            "*",
            "attrs",
            ")",
            ":",
            "return",
            "await",
            "llm.generate",
            "(",
            "prompt",
            ",",
            "*",
            "*",
            "attrs",
            ")",
            ">",
            "asyncio.run",
            "(",
            "generate",
            "(",
            "``",
            "The",
            "meaning",
            "of",
            "life",
            "is",
            "''",
            ",",
            "temperature=0.23",
            ")",
            ")",
            ">",
            "``",
            "`"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/bentoml/OpenLLM",
        "readme_url": "https://raw.githubusercontent.com/bentoml/OpenLLM/main/README.md",
        "topic": [
            "ai",
            "bentoml",
            "falcon",
            "fine-tuning",
            "llama",
            "llama2",
            "llm",
            "llm-inference",
            "llm-ops",
            "llm-serving",
            "llmops",
            "mistral",
            "ml",
            "mlops",
            "model-inference",
            "mpt",
            "open-source-llm",
            "openllm",
            "stablelm",
            "vicuna"
        ],
        "text": "Transformers Agents\n\nOpenLLM seamlessly integrates with\n[Transformers Agents](https://huggingface.co/docs/transformers/transformers_agents).\n\n> [!WARNING]\n> The Transformers Agent is still at an experimental stage. It is\n> recommended to install OpenLLM with `pip install -r nightly-requirements.txt`\n> to get the latest API update for HuggingFace agent.\n\n```python\nimport transformers\n\nagent = transformers.HfAgent('http://localhost:3000/hf/agent')  ",
        "token": [
            "Transformers",
            "Agents",
            "OpenLLM",
            "seamlessly",
            "integrates",
            "with",
            "[",
            "Transformers",
            "Agents",
            "]",
            "(",
            "https",
            ":",
            "//huggingface.co/docs/transformers/transformers_agents",
            ")",
            ".",
            ">",
            "[",
            "!",
            "WARNING",
            "]",
            ">",
            "The",
            "Transformers",
            "Agent",
            "is",
            "still",
            "at",
            "an",
            "experimental",
            "stage",
            ".",
            "It",
            "is",
            ">",
            "recommended",
            "to",
            "install",
            "OpenLLM",
            "with",
            "`",
            "pip",
            "install",
            "-r",
            "nightly-requirements.txt",
            "`",
            ">",
            "to",
            "get",
            "the",
            "latest",
            "API",
            "update",
            "for",
            "HuggingFace",
            "agent",
            ".",
            "``",
            "`",
            "python",
            "import",
            "transformers",
            "agent",
            "=",
            "transformers.HfAgent",
            "(",
            "'http",
            ":",
            "//localhost:3000/hf/agent",
            "'",
            ")"
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/cpacker/MemGPT",
        "readme_url": "https://raw.githubusercontent.com/cpacker/MemGPT/main/README.md",
        "topic": [
            "chat",
            "chatbot",
            "gpt",
            "gpt-4",
            "llm",
            "llm-agent"
        ],
        "text": "Quick setup\r\nJoin <a href=\"https://discord.gg/9GEQrxmVyE\">Discord</a></strong> and message the MemGPT bot (in the `#memgpt` channel). Then run the following commands (messaged to \"MemGPT Bot\"):\r\n* `/profile` (to create your profile)\r\n* `/key` (to enter your OpenAI key)\r\n* `/create` (to create a MemGPT chatbot)\r\n\r\nMake sure your privacy settings on this server are open so that MemGPT Bot can DM you: \\\r\nMemGPT \u2192 Privacy Settings \u2192 Direct Messages set to ON\r\n<div align=\"center\">\r\n <img src=\"https://research.memgpt.ai/assets/img/discord/dm_settings.png\" alt=\"set DMs settings on MemGPT server to be open in MemGPT so that MemGPT Bot can message you\" width=\"400\">\r\n</div>\r\n\r\nYou can see the full list of available commands when you enter `/` into the message box.\r\n<div align=\"center\">\r\n <img src=\"https://research.memgpt.ai/assets/img/discord/slash_commands.png\" alt=\"MemGPT Bot slash commands\" width=\"400\">\r\n</div>\r\n\r\n",
        "token": [
            "Quick",
            "setup",
            "Join",
            "<",
            "a",
            "href=",
            "''",
            "https",
            ":",
            "//discord.gg/9GEQrxmVyE",
            "''",
            ">",
            "Discord",
            "<",
            "/a",
            ">",
            "<",
            "/strong",
            ">",
            "and",
            "message",
            "the",
            "MemGPT",
            "bot",
            "(",
            "in",
            "the",
            "`",
            "#",
            "memgpt",
            "`",
            "channel",
            ")",
            ".",
            "Then",
            "run",
            "the",
            "following",
            "commands",
            "(",
            "messaged",
            "to",
            "``",
            "MemGPT",
            "Bot",
            "''",
            ")",
            ":",
            "*",
            "`",
            "/profile",
            "`",
            "(",
            "to",
            "create",
            "your",
            "profile",
            ")",
            "*",
            "`",
            "/key",
            "`",
            "(",
            "to",
            "enter",
            "your",
            "OpenAI",
            "key",
            ")",
            "*",
            "`",
            "/create",
            "`",
            "(",
            "to",
            "create",
            "a",
            "MemGPT",
            "chatbot",
            ")",
            "Make",
            "sure",
            "your",
            "privacy",
            "settings",
            "on",
            "this",
            "server",
            "are",
            "open",
            "so",
            "that",
            "MemGPT",
            "Bot",
            "can",
            "DM",
            "you",
            ":",
            "\\",
            "MemGPT",
            "\u2192",
            "Privacy",
            "Settings",
            "\u2192",
            "Direct",
            "Messages",
            "set",
            "to",
            "ON",
            "<",
            "div",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "<",
            "img",
            "src=",
            "''",
            "https",
            ":",
            "//research.memgpt.ai/assets/img/discord/dm_settings.png",
            "''",
            "alt=",
            "''",
            "set",
            "DMs",
            "settings",
            "on",
            "MemGPT",
            "server",
            "to",
            "be",
            "open",
            "in",
            "MemGPT",
            "so",
            "that",
            "MemGPT",
            "Bot",
            "can",
            "message",
            "you",
            "''",
            "width=",
            "''",
            "400",
            "''",
            ">",
            "<",
            "/div",
            ">",
            "You",
            "can",
            "see",
            "the",
            "full",
            "list",
            "of",
            "available",
            "commands",
            "when",
            "you",
            "enter",
            "`",
            "/",
            "`",
            "into",
            "the",
            "message",
            "box",
            ".",
            "<",
            "div",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "<",
            "img",
            "src=",
            "''",
            "https",
            ":",
            "//research.memgpt.ai/assets/img/discord/slash_commands.png",
            "''",
            "alt=",
            "''",
            "MemGPT",
            "Bot",
            "slash",
            "commands",
            "''",
            "width=",
            "''",
            "400",
            "''",
            ">",
            "<",
            "/div",
            ">"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/cpacker/MemGPT",
        "readme_url": "https://raw.githubusercontent.com/cpacker/MemGPT/main/README.md",
        "topic": [
            "chat",
            "chatbot",
            "gpt",
            "gpt-4",
            "llm",
            "llm-agent"
        ],
        "text": "Running MemGPT locally\r\nInstall MemGPT:\r\n```sh\r\npip install -U pymemgpt\r\n```\r\n\r\nNow, you can run MemGPT and start chatting with a MemGPT agent with:\r\n```sh\r\nmemgpt run\r\n```\r\n\r\nIf you're running MemGPT for the first time, you'll see two quickstart options:\r\n\r\n1. **OpenAI**: select this if you'd like to run MemGPT with OpenAI models like GPT-4 (requires an OpenAI API key)\r\n2. **MemGPT Free Endpoint**: select this if you'd like to try MemGPT on a top open LLM for free (currently variants of Mixtral 8x7b!)\r\n\r\nNeither of these options require you to have an LLM running on your own machine. If you'd like to run MemGPT with your custom LLM setup (or on OpenAI Azure), select **Other** to proceed to the advanced setup.\r\n\r\n",
        "token": [
            "Running",
            "MemGPT",
            "locally",
            "Install",
            "MemGPT",
            ":",
            "``",
            "`",
            "sh",
            "pip",
            "install",
            "-U",
            "pymemgpt",
            "``",
            "`",
            "Now",
            ",",
            "you",
            "can",
            "run",
            "MemGPT",
            "and",
            "start",
            "chatting",
            "with",
            "a",
            "MemGPT",
            "agent",
            "with",
            ":",
            "``",
            "`",
            "sh",
            "memgpt",
            "run",
            "``",
            "`",
            "If",
            "you",
            "'re",
            "running",
            "MemGPT",
            "for",
            "the",
            "first",
            "time",
            ",",
            "you",
            "'ll",
            "see",
            "two",
            "quickstart",
            "options",
            ":",
            "1",
            ".",
            "*",
            "*",
            "OpenAI",
            "*",
            "*",
            ":",
            "select",
            "this",
            "if",
            "you",
            "'d",
            "like",
            "to",
            "run",
            "MemGPT",
            "with",
            "OpenAI",
            "models",
            "like",
            "GPT-4",
            "(",
            "requires",
            "an",
            "OpenAI",
            "API",
            "key",
            ")",
            "2",
            ".",
            "*",
            "*",
            "MemGPT",
            "Free",
            "Endpoint",
            "*",
            "*",
            ":",
            "select",
            "this",
            "if",
            "you",
            "'d",
            "like",
            "to",
            "try",
            "MemGPT",
            "on",
            "a",
            "top",
            "open",
            "LLM",
            "for",
            "free",
            "(",
            "currently",
            "variants",
            "of",
            "Mixtral",
            "8x7b",
            "!",
            ")",
            "Neither",
            "of",
            "these",
            "options",
            "require",
            "you",
            "to",
            "have",
            "an",
            "LLM",
            "running",
            "on",
            "your",
            "own",
            "machine",
            ".",
            "If",
            "you",
            "'d",
            "like",
            "to",
            "run",
            "MemGPT",
            "with",
            "your",
            "custom",
            "LLM",
            "setup",
            "(",
            "or",
            "on",
            "OpenAI",
            "Azure",
            ")",
            ",",
            "select",
            "*",
            "*",
            "Other",
            "*",
            "*",
            "to",
            "proceed",
            "to",
            "the",
            "advanced",
            "setup",
            "."
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/cpacker/MemGPT",
        "readme_url": "https://raw.githubusercontent.com/cpacker/MemGPT/main/README.md",
        "topic": [
            "chat",
            "chatbot",
            "gpt",
            "gpt-4",
            "llm",
            "llm-agent"
        ],
        "text": "Advanced setup\r\nYou can reconfigure MemGPT's default settings by running:\r\n```sh\r\nmemgpt configure\r\n```\r\n\r\n",
        "token": [
            "Advanced",
            "setup",
            "You",
            "can",
            "reconfigure",
            "MemGPT",
            "'s",
            "default",
            "settings",
            "by",
            "running",
            ":",
            "``",
            "`",
            "sh",
            "memgpt",
            "configure",
            "``",
            "`"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/cpacker/MemGPT",
        "readme_url": "https://raw.githubusercontent.com/cpacker/MemGPT/main/README.md",
        "topic": [
            "chat",
            "chatbot",
            "gpt",
            "gpt-4",
            "llm",
            "llm-agent"
        ],
        "text": "Installing from source\r\nTo install MemGPT from source, start by cloning the repo:\r\n```sh\r\ngit clone git@github.com:cpacker/MemGPT.git\r\n```\r\n\r\nThen navigate to the main `MemGPT` directory, and do:\r\n```sh\r\npip install -e .\r\n```\r\n\r\nNow, you should be able to run `memgpt` from the command-line using the downloaded source code.\r\n\r\nIf you are having dependency issues using `pip install -e .`, we recommend you install the package using Poetry (see below). Installing MemGPT from source using Poetry will ensure that you are using exact package versions that have been tested for the production build.\r\n\r\n<details>\r\n <summary>\r\n  <strong>Installing from source (using Poetry)</strong>\r\n </summary>\r\n\r\nFirst, install Poetry using [the official instructions here](https://python-poetry.org/docs/#installing-with-the-official-installer).\r\n\r\nThen, you can install MemGPT from source with:\r\n```sh\r\ngit clone git@github.com:cpacker/MemGPT.git\r\npoetry shell\r\npoetry install\r\n```\r\n</details>\r\n\r\n",
        "token": [
            "Installing",
            "from",
            "source",
            "To",
            "install",
            "MemGPT",
            "from",
            "source",
            ",",
            "start",
            "by",
            "cloning",
            "the",
            "repo",
            ":",
            "``",
            "`",
            "sh",
            "git",
            "clone",
            "git",
            "@",
            "github.com",
            ":",
            "cpacker/MemGPT.git",
            "``",
            "`",
            "Then",
            "navigate",
            "to",
            "the",
            "main",
            "`",
            "MemGPT",
            "`",
            "directory",
            ",",
            "and",
            "do",
            ":",
            "``",
            "`",
            "sh",
            "pip",
            "install",
            "-e",
            ".",
            "``",
            "`",
            "Now",
            ",",
            "you",
            "should",
            "be",
            "able",
            "to",
            "run",
            "`",
            "memgpt",
            "`",
            "from",
            "the",
            "command-line",
            "using",
            "the",
            "downloaded",
            "source",
            "code",
            ".",
            "If",
            "you",
            "are",
            "having",
            "dependency",
            "issues",
            "using",
            "`",
            "pip",
            "install",
            "-e",
            ".",
            "`",
            ",",
            "we",
            "recommend",
            "you",
            "install",
            "the",
            "package",
            "using",
            "Poetry",
            "(",
            "see",
            "below",
            ")",
            ".",
            "Installing",
            "MemGPT",
            "from",
            "source",
            "using",
            "Poetry",
            "will",
            "ensure",
            "that",
            "you",
            "are",
            "using",
            "exact",
            "package",
            "versions",
            "that",
            "have",
            "been",
            "tested",
            "for",
            "the",
            "production",
            "build",
            ".",
            "<",
            "details",
            ">",
            "<",
            "summary",
            ">",
            "<",
            "strong",
            ">",
            "Installing",
            "from",
            "source",
            "(",
            "using",
            "Poetry",
            ")",
            "<",
            "/strong",
            ">",
            "<",
            "/summary",
            ">",
            "First",
            ",",
            "install",
            "Poetry",
            "using",
            "[",
            "the",
            "official",
            "instructions",
            "here",
            "]",
            "(",
            "https",
            ":",
            "//python-poetry.org/docs/",
            "#",
            "installing-with-the-official-installer",
            ")",
            ".",
            "Then",
            ",",
            "you",
            "can",
            "install",
            "MemGPT",
            "from",
            "source",
            "with",
            ":",
            "``",
            "`",
            "sh",
            "git",
            "clone",
            "git",
            "@",
            "github.com",
            ":",
            "cpacker/MemGPT.git",
            "poetry",
            "shell",
            "poetry",
            "install",
            "``",
            "`",
            "<",
            "/details",
            ">"
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/activeloopai/deeplake",
        "readme_url": "https://raw.githubusercontent.com/activeloopai/deeplake/main/README.md",
        "topic": [
            "ai",
            "computer-vision",
            "cv",
            "data-science",
            "data-version-control",
            "datalake",
            "datasets",
            "deep-learning",
            "image-processing",
            "langchain",
            "large-language-models",
            "llm",
            "machine-learning",
            "ml",
            "mlops",
            "python",
            "pytorch",
            "tensorflow",
            "vector-database",
            "vector-search"
        ],
        "text": "\ud83d\ude80 How to install Deep Lake\nDeep Lake can be installed using pip:\n```sh\npip3 install deeplake\n```\n**By default, Deep Lake does not install dependencies for audio, video, google-cloud, and other features. Details on all installation options are [available here](https://docs.deeplake.ai/en/latest/Installation.html).**\n\n",
        "token": [
            "\ud83d\ude80",
            "How",
            "to",
            "install",
            "Deep",
            "Lake",
            "Deep",
            "Lake",
            "can",
            "be",
            "installed",
            "using",
            "pip",
            ":",
            "``",
            "`",
            "sh",
            "pip3",
            "install",
            "deeplake",
            "``",
            "`",
            "*",
            "*",
            "By",
            "default",
            ",",
            "Deep",
            "Lake",
            "does",
            "not",
            "install",
            "dependencies",
            "for",
            "audio",
            ",",
            "video",
            ",",
            "google-cloud",
            ",",
            "and",
            "other",
            "features",
            ".",
            "Details",
            "on",
            "all",
            "installation",
            "options",
            "are",
            "[",
            "available",
            "here",
            "]",
            "(",
            "https",
            ":",
            "//docs.deeplake.ai/en/latest/Installation.html",
            ")",
            ".",
            "*",
            "*"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/activeloopai/deeplake",
        "readme_url": "https://raw.githubusercontent.com/activeloopai/deeplake/main/README.md",
        "topic": [
            "ai",
            "computer-vision",
            "cv",
            "data-science",
            "data-version-control",
            "datalake",
            "datasets",
            "deep-learning",
            "image-processing",
            "langchain",
            "large-language-models",
            "llm",
            "machine-learning",
            "ml",
            "mlops",
            "python",
            "pytorch",
            "tensorflow",
            "vector-database",
            "vector-search"
        ],
        "text": "- [Vector Store Getting Started Guide](https://docs.activeloop.ai/getting-started/vector-store)\n",
        "token": [
            "-",
            "[",
            "Vector",
            "Store",
            "Getting",
            "Started",
            "Guide",
            "]",
            "(",
            "https",
            ":",
            "//docs.activeloop.ai/getting-started/vector-store",
            ")"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/activeloopai/deeplake",
        "readme_url": "https://raw.githubusercontent.com/activeloopai/deeplake/main/README.md",
        "topic": [
            "ai",
            "computer-vision",
            "cv",
            "data-science",
            "data-version-control",
            "datalake",
            "datasets",
            "deep-learning",
            "image-processing",
            "langchain",
            "large-language-models",
            "llm",
            "machine-learning",
            "ml",
            "mlops",
            "python",
            "pytorch",
            "tensorflow",
            "vector-database",
            "vector-search"
        ],
        "text": "- [Deep Learning Getting Started Guide](https://docs.activeloop.ai/getting-started/deep-learning)\n",
        "token": [
            "-",
            "[",
            "Deep",
            "Learning",
            "Getting",
            "Started",
            "Guide",
            "]",
            "(",
            "https",
            ":",
            "//docs.activeloop.ai/getting-started/deep-learning",
            ")"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/activeloopai/deeplake",
        "readme_url": "https://raw.githubusercontent.com/activeloopai/deeplake/main/README.md",
        "topic": [
            "ai",
            "computer-vision",
            "cv",
            "data-science",
            "data-version-control",
            "datalake",
            "datasets",
            "deep-learning",
            "image-processing",
            "langchain",
            "large-language-models",
            "llm",
            "machine-learning",
            "ml",
            "mlops",
            "python",
            "pytorch",
            "tensorflow",
            "vector-database",
            "vector-search"
        ],
        "text": "\ud83d\udcda Documentation\n\nGetting started guides, examples, tutorials, API reference, and other useful information can be found on our [documentation page](http://docs.activeloop.ai/?utm_source=github&utm_medium=repo&utm_campaign=readme).\n\n",
        "token": [
            "\ud83d\udcda",
            "Documentation",
            "Getting",
            "started",
            "guides",
            ",",
            "examples",
            ",",
            "tutorials",
            ",",
            "API",
            "reference",
            ",",
            "and",
            "other",
            "useful",
            "information",
            "can",
            "be",
            "found",
            "on",
            "our",
            "[",
            "documentation",
            "page",
            "]",
            "(",
            "http",
            ":",
            "//docs.activeloop.ai/",
            "?",
            "utm_source=github",
            "&",
            "utm_medium=repo",
            "&",
            "utm_campaign=readme",
            ")",
            "."
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/activeloopai/deeplake",
        "readme_url": "https://raw.githubusercontent.com/activeloopai/deeplake/main/README.md",
        "topic": [
            "ai",
            "computer-vision",
            "cv",
            "data-science",
            "data-version-control",
            "datalake",
            "datasets",
            "deep-learning",
            "image-processing",
            "langchain",
            "large-language-models",
            "llm",
            "machine-learning",
            "ml",
            "mlops",
            "python",
            "pytorch",
            "tensorflow",
            "vector-database",
            "vector-search"
        ],
        "text": "Disclaimers\n\n<details>\n  <summary><b> Dataset Licenses</b></summary>\n  \nDeep Lake users may have access to a variety of publicly available datasets. We do not host or distribute these datasets, vouch for their quality or fairness, or claim that you have a license to use the datasets. It is your responsibility to determine whether you have permission to use the datasets under their license.\n\nIf you're a dataset owner and do not want your dataset to be included in this library, please get in touch through a [GitHub issue](https://github.com/activeloopai/deeplake/issues/new). Thank you for your contribution to the ML community!\n\n</details>\n\n<details>\n  <summary><b> Usage Tracking</b></summary>\n\nBy default, we collect usage data using Bugout (here's the [code](https://github.com/activeloopai/deeplake/blob/853456a314b4fb5623c936c825601097b0685119/deeplake/__init__.py#L24) that does it). It does not collect user data other than anonymized IP address data, and it only logs the Deep Lake library's own actions. This helps our team understand how the tool is used and how to build features that matter to you! After you register with Activeloop, data is no longer anonymous. You can always opt-out of reporting using the CLI command below, or by setting an environmental variable ```BUGGER_OFF``` to ```True```:\n\n```bash\nactiveloop reporting --off\n```\n\n</details>\n\n",
        "token": [
            "Disclaimers",
            "<",
            "details",
            ">",
            "<",
            "summary",
            ">",
            "<",
            "b",
            ">",
            "Dataset",
            "Licenses",
            "<",
            "/b",
            ">",
            "<",
            "/summary",
            ">",
            "Deep",
            "Lake",
            "users",
            "may",
            "have",
            "access",
            "to",
            "a",
            "variety",
            "of",
            "publicly",
            "available",
            "datasets",
            ".",
            "We",
            "do",
            "not",
            "host",
            "or",
            "distribute",
            "these",
            "datasets",
            ",",
            "vouch",
            "for",
            "their",
            "quality",
            "or",
            "fairness",
            ",",
            "or",
            "claim",
            "that",
            "you",
            "have",
            "a",
            "license",
            "to",
            "use",
            "the",
            "datasets",
            ".",
            "It",
            "is",
            "your",
            "responsibility",
            "to",
            "determine",
            "whether",
            "you",
            "have",
            "permission",
            "to",
            "use",
            "the",
            "datasets",
            "under",
            "their",
            "license",
            ".",
            "If",
            "you",
            "'re",
            "a",
            "dataset",
            "owner",
            "and",
            "do",
            "not",
            "want",
            "your",
            "dataset",
            "to",
            "be",
            "included",
            "in",
            "this",
            "library",
            ",",
            "please",
            "get",
            "in",
            "touch",
            "through",
            "a",
            "[",
            "GitHub",
            "issue",
            "]",
            "(",
            "https",
            ":",
            "//github.com/activeloopai/deeplake/issues/new",
            ")",
            ".",
            "Thank",
            "you",
            "for",
            "your",
            "contribution",
            "to",
            "the",
            "ML",
            "community",
            "!",
            "<",
            "/details",
            ">",
            "<",
            "details",
            ">",
            "<",
            "summary",
            ">",
            "<",
            "b",
            ">",
            "Usage",
            "Tracking",
            "<",
            "/b",
            ">",
            "<",
            "/summary",
            ">",
            "By",
            "default",
            ",",
            "we",
            "collect",
            "usage",
            "data",
            "using",
            "Bugout",
            "(",
            "here",
            "'s",
            "the",
            "[",
            "code",
            "]",
            "(",
            "https",
            ":",
            "//github.com/activeloopai/deeplake/blob/853456a314b4fb5623c936c825601097b0685119/deeplake/__init__.py",
            "#",
            "L24",
            ")",
            "that",
            "does",
            "it",
            ")",
            ".",
            "It",
            "does",
            "not",
            "collect",
            "user",
            "data",
            "other",
            "than",
            "anonymized",
            "IP",
            "address",
            "data",
            ",",
            "and",
            "it",
            "only",
            "logs",
            "the",
            "Deep",
            "Lake",
            "library",
            "'s",
            "own",
            "actions",
            ".",
            "This",
            "helps",
            "our",
            "team",
            "understand",
            "how",
            "the",
            "tool",
            "is",
            "used",
            "and",
            "how",
            "to",
            "build",
            "features",
            "that",
            "matter",
            "to",
            "you",
            "!",
            "After",
            "you",
            "register",
            "with",
            "Activeloop",
            ",",
            "data",
            "is",
            "no",
            "longer",
            "anonymous",
            ".",
            "You",
            "can",
            "always",
            "opt-out",
            "of",
            "reporting",
            "using",
            "the",
            "CLI",
            "command",
            "below",
            ",",
            "or",
            "by",
            "setting",
            "an",
            "environmental",
            "variable",
            "``",
            "`",
            "BUGGER_OFF",
            "``",
            "`",
            "to",
            "``",
            "`",
            "True",
            "``",
            "`",
            ":",
            "``",
            "`",
            "bash",
            "activeloop",
            "reporting",
            "--",
            "off",
            "``",
            "`",
            "<",
            "/details",
            ">"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/embedchain/embedchain",
        "readme_url": "https://raw.githubusercontent.com/embedchain/embedchain/main/README.md",
        "topic": [
            "ai",
            "application",
            "chatbots",
            "chatgpt",
            "embeddings",
            "llm",
            "python",
            "rag",
            "vector-database"
        ],
        "text": "\ud83d\udd27 Quick install\n\n",
        "token": [
            "\ud83d\udd27",
            "Quick",
            "install"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/embedchain/embedchain",
        "readme_url": "https://raw.githubusercontent.com/embedchain/embedchain/main/README.md",
        "topic": [
            "ai",
            "application",
            "chatbots",
            "chatgpt",
            "embeddings",
            "llm",
            "python",
            "rag",
            "vector-database"
        ],
        "text": "Python API\n\n```bash\npip install embedchain\n```\n\n",
        "token": [
            "Python",
            "API",
            "``",
            "`",
            "bash",
            "pip",
            "install",
            "embedchain",
            "``",
            "`"
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/embedchain/embedchain",
        "readme_url": "https://raw.githubusercontent.com/embedchain/embedchain/main/README.md",
        "topic": [
            "ai",
            "application",
            "chatbots",
            "chatgpt",
            "embeddings",
            "llm",
            "python",
            "rag",
            "vector-database"
        ],
        "text": "\ud83d\udcd6 Documentation\nComprehensive guides and API documentation are available to help you get the most out of Embedchain:\n\n- [Introduction](https://docs.embedchain.ai/get-started/introduction#what-is-embedchain)\n- [Getting Started](https://docs.embedchain.ai/get-started/quickstart)\n- [Examples](https://docs.embedchain.ai/examples)\n- [Supported data types](https://docs.embedchain.ai/components/data-sources/overview)\n\n",
        "token": [
            "\ud83d\udcd6",
            "Documentation",
            "Comprehensive",
            "guides",
            "and",
            "API",
            "documentation",
            "are",
            "available",
            "to",
            "help",
            "you",
            "get",
            "the",
            "most",
            "out",
            "of",
            "Embedchain",
            ":",
            "-",
            "[",
            "Introduction",
            "]",
            "(",
            "https",
            ":",
            "//docs.embedchain.ai/get-started/introduction",
            "#",
            "what-is-embedchain",
            ")",
            "-",
            "[",
            "Getting",
            "Started",
            "]",
            "(",
            "https",
            ":",
            "//docs.embedchain.ai/get-started/quickstart",
            ")",
            "-",
            "[",
            "Examples",
            "]",
            "(",
            "https",
            ":",
            "//docs.embedchain.ai/examples",
            ")",
            "-",
            "[",
            "Supported",
            "data",
            "types",
            "]",
            "(",
            "https",
            ":",
            "//docs.embedchain.ai/components/data-sources/overview",
            ")"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/microsoft/TypeChat",
        "readme_url": "https://raw.githubusercontent.com/microsoft/TypeChat/main/README.md",
        "topic": [
            "ai",
            "llm",
            "natural-language",
            "types"
        ],
        "text": "Getting Started\n\nInstall TypeChat:\n\n```\nnpm install typechat\n```\n\nYou can also build TypeChat from source:\n\n```\ngit clone https://github.com/microsoft/TypeChat\ncd TypeChat\nnpm run build\n```\n\nTo see TypeChat in action, we recommend exploring the [TypeChat example projects](./examples). You can try them on your local machine or in a GitHub Codespace.\n\nTo learn more about TypeChat, visit the [documentation](https://microsoft.github.io/TypeChat) which includes more information on TypeChat and how to get started.\n\n",
        "token": [
            "Getting",
            "Started",
            "Install",
            "TypeChat",
            ":",
            "``",
            "`",
            "npm",
            "install",
            "typechat",
            "``",
            "`",
            "You",
            "can",
            "also",
            "build",
            "TypeChat",
            "from",
            "source",
            ":",
            "``",
            "`",
            "git",
            "clone",
            "https",
            ":",
            "//github.com/microsoft/TypeChat",
            "cd",
            "TypeChat",
            "npm",
            "run",
            "build",
            "``",
            "`",
            "To",
            "see",
            "TypeChat",
            "in",
            "action",
            ",",
            "we",
            "recommend",
            "exploring",
            "the",
            "[",
            "TypeChat",
            "example",
            "projects",
            "]",
            "(",
            "./examples",
            ")",
            ".",
            "You",
            "can",
            "try",
            "them",
            "on",
            "your",
            "local",
            "machine",
            "or",
            "in",
            "a",
            "GitHub",
            "Codespace",
            ".",
            "To",
            "learn",
            "more",
            "about",
            "TypeChat",
            ",",
            "visit",
            "the",
            "[",
            "documentation",
            "]",
            "(",
            "https",
            ":",
            "//microsoft.github.io/TypeChat",
            ")",
            "which",
            "includes",
            "more",
            "information",
            "on",
            "TypeChat",
            "and",
            "how",
            "to",
            "get",
            "started",
            "."
        ],
        "level of complexity": 2
    },
    {
        "url": "https://github.com/mistralai/mistral-src",
        "readme_url": "https://raw.githubusercontent.com/mistralai/mistral-src/main/README.md",
        "topic": [
            "llm",
            "llm-inference",
            "mistralai"
        ],
        "text": "Installation\n\n```\npip install -r requirements.txt\n```\n\n",
        "token": [
            "Installation",
            "``",
            "`",
            "pip",
            "install",
            "-r",
            "requirements.txt",
            "``",
            "`"
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/microsoft/promptflow",
        "readme_url": "https://raw.githubusercontent.com/microsoft/promptflow/main/README.md",
        "topic": [
            "ai",
            "ai-application-development",
            "ai-applications",
            "chatgpt",
            "gpt",
            "llm",
            "prompt",
            "prompt-engineering"
        ],
        "text": "Installation\n\nTo get started quickly, you can use a pre-built development environment. **Click the button below** to open the repo in GitHub Codespaces, and then continue the readme!\n\n[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/promptflow?quickstart=1)\n\nIf you want to get started in your local environment, first install the packages:\n\nEnsure you have a python environment, `python=3.9` is recommended.\n\n```sh\npip install promptflow promptflow-tools\n```\n\n",
        "token": [
            "Installation",
            "To",
            "get",
            "started",
            "quickly",
            ",",
            "you",
            "can",
            "use",
            "a",
            "pre-built",
            "development",
            "environment",
            ".",
            "*",
            "*",
            "Click",
            "the",
            "button",
            "below",
            "*",
            "*",
            "to",
            "open",
            "the",
            "repo",
            "in",
            "GitHub",
            "Codespaces",
            ",",
            "and",
            "then",
            "continue",
            "the",
            "readme",
            "!",
            "[",
            "!",
            "[",
            "Open",
            "in",
            "GitHub",
            "Codespaces",
            "]",
            "(",
            "https",
            ":",
            "//github.com/codespaces/badge.svg",
            ")",
            "]",
            "(",
            "https",
            ":",
            "//codespaces.new/microsoft/promptflow",
            "?",
            "quickstart=1",
            ")",
            "If",
            "you",
            "want",
            "to",
            "get",
            "started",
            "in",
            "your",
            "local",
            "environment",
            ",",
            "first",
            "install",
            "the",
            "packages",
            ":",
            "Ensure",
            "you",
            "have",
            "a",
            "python",
            "environment",
            ",",
            "`",
            "python=3.9",
            "`",
            "is",
            "recommended",
            ".",
            "``",
            "`",
            "sh",
            "pip",
            "install",
            "promptflow",
            "promptflow-tools",
            "``",
            "`"
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/microsoft/promptflow",
        "readme_url": "https://raw.githubusercontent.com/microsoft/promptflow/main/README.md",
        "topic": [
            "ai",
            "ai-application-development",
            "ai-applications",
            "chatgpt",
            "gpt",
            "llm",
            "prompt",
            "prompt-engineering"
        ],
        "text": "Quick Start \u26a1\n\n**Create a chatbot with prompt flow**\n\nRun the command to initiate a prompt flow from a chat template, it creates folder named `my_chatbot` and generates required files within it:\n\n```sh\npf flow init --flow ./my_chatbot --type chat\n```\n\n**Setup a connection for your API key**\n\nFor OpenAI key, establish a connection by running the command, using the `openai.yaml` file in the `my_chatbot` folder, which stores your OpenAI key (override keys and name with --set to avoid yaml file changes):\n\n```sh\npf connection create --file ./my_chatbot/openai.yaml --set api_key=<your_api_key> --name open_ai_connection\n```\n\nFor Azure OpenAI key, establish the connection by running the command, using the `azure_openai.yaml` file:\n\n```sh\npf connection create --file ./my_chatbot/azure_openai.yaml --set api_key=<your_api_key> api_base=<your_api_base> --name open_ai_connection\n```\n\n**Chat with your flow**\n\nIn the `my_chatbot` folder, there's a `flow.dag.yaml` file that outlines the flow, including inputs/outputs, nodes,  connection, and the LLM model, etc\n\n> Note that in the `chat` node, we're using a connection named `open_ai_connection` (specified in `connection` field) and the `gpt-35-turbo` model (specified in `deployment_name` field). The deployment_name filed is to specify the OpenAI model, or the Azure OpenAI deployment resource.\n\nInteract with your chatbot by running: (press `Ctrl + C` to end the session)\n\n```sh\npf flow test --flow ./my_chatbot --interactive\n```\n\n**Core value: ensuring \"High Quality\u201d from prototype to production**\n\nExplore our [**15-minute tutorial**](examples/tutorials/flow-fine-tuning-evaluation/promptflow-quality-improvement.md) that guides you through prompt tuning \u27a1 batch testing \u27a1 evaluation, all designed to ensure high quality ready for production.\n\nNext Step! Continue with the **Tutorial**  \ud83d\udc47 section to delve deeper into prompt flow.\n\n",
        "token": [
            "Quick",
            "Start",
            "\u26a1",
            "*",
            "*",
            "Create",
            "a",
            "chatbot",
            "with",
            "prompt",
            "flow",
            "*",
            "*",
            "Run",
            "the",
            "command",
            "to",
            "initiate",
            "a",
            "prompt",
            "flow",
            "from",
            "a",
            "chat",
            "template",
            ",",
            "it",
            "creates",
            "folder",
            "named",
            "`",
            "my_chatbot",
            "`",
            "and",
            "generates",
            "required",
            "files",
            "within",
            "it",
            ":",
            "``",
            "`",
            "sh",
            "pf",
            "flow",
            "init",
            "--",
            "flow",
            "./my_chatbot",
            "--",
            "type",
            "chat",
            "``",
            "`",
            "*",
            "*",
            "Setup",
            "a",
            "connection",
            "for",
            "your",
            "API",
            "key",
            "*",
            "*",
            "For",
            "OpenAI",
            "key",
            ",",
            "establish",
            "a",
            "connection",
            "by",
            "running",
            "the",
            "command",
            ",",
            "using",
            "the",
            "`",
            "openai.yaml",
            "`",
            "file",
            "in",
            "the",
            "`",
            "my_chatbot",
            "`",
            "folder",
            ",",
            "which",
            "stores",
            "your",
            "OpenAI",
            "key",
            "(",
            "override",
            "keys",
            "and",
            "name",
            "with",
            "--",
            "set",
            "to",
            "avoid",
            "yaml",
            "file",
            "changes",
            ")",
            ":",
            "``",
            "`",
            "sh",
            "pf",
            "connection",
            "create",
            "--",
            "file",
            "./my_chatbot/openai.yaml",
            "--",
            "set",
            "api_key=",
            "<",
            "your_api_key",
            ">",
            "--",
            "name",
            "open_ai_connection",
            "``",
            "`",
            "For",
            "Azure",
            "OpenAI",
            "key",
            ",",
            "establish",
            "the",
            "connection",
            "by",
            "running",
            "the",
            "command",
            ",",
            "using",
            "the",
            "`",
            "azure_openai.yaml",
            "`",
            "file",
            ":",
            "``",
            "`",
            "sh",
            "pf",
            "connection",
            "create",
            "--",
            "file",
            "./my_chatbot/azure_openai.yaml",
            "--",
            "set",
            "api_key=",
            "<",
            "your_api_key",
            ">",
            "api_base=",
            "<",
            "your_api_base",
            ">",
            "--",
            "name",
            "open_ai_connection",
            "``",
            "`",
            "*",
            "*",
            "Chat",
            "with",
            "your",
            "flow",
            "*",
            "*",
            "In",
            "the",
            "`",
            "my_chatbot",
            "`",
            "folder",
            ",",
            "there",
            "'s",
            "a",
            "`",
            "flow.dag.yaml",
            "`",
            "file",
            "that",
            "outlines",
            "the",
            "flow",
            ",",
            "including",
            "inputs/outputs",
            ",",
            "nodes",
            ",",
            "connection",
            ",",
            "and",
            "the",
            "LLM",
            "model",
            ",",
            "etc",
            ">",
            "Note",
            "that",
            "in",
            "the",
            "`",
            "chat",
            "`",
            "node",
            ",",
            "we",
            "'re",
            "using",
            "a",
            "connection",
            "named",
            "`",
            "open_ai_connection",
            "`",
            "(",
            "specified",
            "in",
            "`",
            "connection",
            "`",
            "field",
            ")",
            "and",
            "the",
            "`",
            "gpt-35-turbo",
            "`",
            "model",
            "(",
            "specified",
            "in",
            "`",
            "deployment_name",
            "`",
            "field",
            ")",
            ".",
            "The",
            "deployment_name",
            "filed",
            "is",
            "to",
            "specify",
            "the",
            "OpenAI",
            "model",
            ",",
            "or",
            "the",
            "Azure",
            "OpenAI",
            "deployment",
            "resource",
            ".",
            "Interact",
            "with",
            "your",
            "chatbot",
            "by",
            "running",
            ":",
            "(",
            "press",
            "`",
            "Ctrl",
            "+",
            "C",
            "`",
            "to",
            "end",
            "the",
            "session",
            ")",
            "``",
            "`",
            "sh",
            "pf",
            "flow",
            "test",
            "--",
            "flow",
            "./my_chatbot",
            "--",
            "interactive",
            "``",
            "`",
            "*",
            "*",
            "Core",
            "value",
            ":",
            "ensuring",
            "``",
            "High",
            "Quality",
            "\u201d",
            "from",
            "prototype",
            "to",
            "production",
            "*",
            "*",
            "Explore",
            "our",
            "[",
            "*",
            "*",
            "15-minute",
            "tutorial",
            "*",
            "*",
            "]",
            "(",
            "examples/tutorials/flow-fine-tuning-evaluation/promptflow-quality-improvement.md",
            ")",
            "that",
            "guides",
            "you",
            "through",
            "prompt",
            "tuning",
            "\u27a1",
            "batch",
            "testing",
            "\u27a1",
            "evaluation",
            ",",
            "all",
            "designed",
            "to",
            "ensure",
            "high",
            "quality",
            "ready",
            "for",
            "production",
            ".",
            "Next",
            "Step",
            "!",
            "Continue",
            "with",
            "the",
            "*",
            "*",
            "Tutorial",
            "*",
            "*",
            "\ud83d\udc47",
            "section",
            "to",
            "delve",
            "deeper",
            "into",
            "prompt",
            "flow",
            "."
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/microsoft/promptflow",
        "readme_url": "https://raw.githubusercontent.com/microsoft/promptflow/main/README.md",
        "topic": [
            "ai",
            "ai-application-development",
            "ai-applications",
            "chatgpt",
            "gpt",
            "llm",
            "prompt",
            "prompt-engineering"
        ],
        "text": "VS Code Extension\n\nWe also offer a VS Code extension (a flow designer) for an interactive flow development experience with UI.\n\n<img src=\"examples/tutorials/quick-start/media/vsc.png\" alt=\"vsc\" width=\"1000\"/>\n\nYou can install it from the <a href=\"https://marketplace.visualstudio.com/items?itemName=prompt-flow.prompt-flow\">visualstudio marketplace</a>.\n\n",
        "token": [
            "VS",
            "Code",
            "Extension",
            "We",
            "also",
            "offer",
            "a",
            "VS",
            "Code",
            "extension",
            "(",
            "a",
            "flow",
            "designer",
            ")",
            "for",
            "an",
            "interactive",
            "flow",
            "development",
            "experience",
            "with",
            "UI",
            ".",
            "<",
            "img",
            "src=",
            "''",
            "examples/tutorials/quick-start/media/vsc.png",
            "''",
            "alt=",
            "''",
            "vsc",
            "''",
            "width=",
            "''",
            "1000",
            "''",
            "/",
            ">",
            "You",
            "can",
            "install",
            "it",
            "from",
            "the",
            "<",
            "a",
            "href=",
            "''",
            "https",
            ":",
            "//marketplace.visualstudio.com/items",
            "?",
            "itemName=prompt-flow.prompt-flow",
            "''",
            ">",
            "visualstudio",
            "marketplace",
            "<",
            "/a",
            ">",
            "."
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/microsoft/promptflow",
        "readme_url": "https://raw.githubusercontent.com/microsoft/promptflow/main/README.md",
        "topic": [
            "ai",
            "ai-application-development",
            "ai-applications",
            "chatgpt",
            "gpt",
            "llm",
            "prompt",
            "prompt-engineering"
        ],
        "text": "Deep delve into flow development\n\n[Getting started with prompt flow](https://microsoft.github.io/promptflow/how-to-guides/quick-start.html): A step by step guidance to invoke your first flow run.\n\n",
        "token": [
            "Deep",
            "delve",
            "into",
            "flow",
            "development",
            "[",
            "Getting",
            "started",
            "with",
            "prompt",
            "flow",
            "]",
            "(",
            "https",
            ":",
            "//microsoft.github.io/promptflow/how-to-guides/quick-start.html",
            ")",
            ":",
            "A",
            "step",
            "by",
            "step",
            "guidance",
            "to",
            "invoke",
            "your",
            "first",
            "flow",
            "run",
            "."
        ],
        "level of complexity": 2
    },
    {
        "url": "https://github.com/microsoft/promptflow",
        "readme_url": "https://raw.githubusercontent.com/microsoft/promptflow/main/README.md",
        "topic": [
            "ai",
            "ai-application-development",
            "ai-applications",
            "chatgpt",
            "gpt",
            "llm",
            "prompt",
            "prompt-engineering"
        ],
        "text": "Learn from use cases\n\n[Tutorial: Chat with PDF](https://github.com/microsoft/promptflow/blob/main/examples/tutorials/e2e-development/chat-with-pdf.md): An end-to-end tutorial on how to build a high quality chat application with prompt flow, including flow development and evaluation with metrics.\n> More examples can be found [here](https://microsoft.github.io/promptflow/tutorials/index.html#samples). We welcome contributions of new use cases!\n\n",
        "token": [
            "Learn",
            "from",
            "use",
            "cases",
            "[",
            "Tutorial",
            ":",
            "Chat",
            "with",
            "PDF",
            "]",
            "(",
            "https",
            ":",
            "//github.com/microsoft/promptflow/blob/main/examples/tutorials/e2e-development/chat-with-pdf.md",
            ")",
            ":",
            "An",
            "end-to-end",
            "tutorial",
            "on",
            "how",
            "to",
            "build",
            "a",
            "high",
            "quality",
            "chat",
            "application",
            "with",
            "prompt",
            "flow",
            ",",
            "including",
            "flow",
            "development",
            "and",
            "evaluation",
            "with",
            "metrics",
            ".",
            ">",
            "More",
            "examples",
            "can",
            "be",
            "found",
            "[",
            "here",
            "]",
            "(",
            "https",
            ":",
            "//microsoft.github.io/promptflow/tutorials/index.html",
            "#",
            "samples",
            ")",
            ".",
            "We",
            "welcome",
            "contributions",
            "of",
            "new",
            "use",
            "cases",
            "!"
        ],
        "level of complexity": 2
    },
    {
        "url": "https://github.com/microsoft/promptflow",
        "readme_url": "https://raw.githubusercontent.com/microsoft/promptflow/main/README.md",
        "topic": [
            "ai",
            "ai-application-development",
            "ai-applications",
            "chatgpt",
            "gpt",
            "llm",
            "prompt",
            "prompt-engineering"
        ],
        "text": "Setup for contributors\n\nIf you're interested in contributing, please start with our dev setup guide: [dev_setup.md](./docs/dev/dev_setup.md).\n\nNext Step! Continue with the **Contributing**  \ud83d\udc47 section to contribute to prompt flow.\n\n",
        "token": [
            "Setup",
            "for",
            "contributors",
            "If",
            "you",
            "'re",
            "interested",
            "in",
            "contributing",
            ",",
            "please",
            "start",
            "with",
            "our",
            "dev",
            "setup",
            "guide",
            ":",
            "[",
            "dev_setup.md",
            "]",
            "(",
            "./docs/dev/dev_setup.md",
            ")",
            ".",
            "Next",
            "Step",
            "!",
            "Continue",
            "with",
            "the",
            "*",
            "*",
            "Contributing",
            "*",
            "*",
            "\ud83d\udc47",
            "section",
            "to",
            "contribute",
            "to",
            "prompt",
            "flow",
            "."
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/TheR1D/shell_gpt",
        "readme_url": "https://raw.githubusercontent.com/TheR1D/shell_gpt/main/README.md",
        "topic": [
            "chatgpt",
            "cheat-sheet",
            "cli",
            "commands",
            "gpt-3",
            "gpt-4",
            "linux",
            "llm",
            "openai",
            "productivity",
            "python",
            "shell",
            "terminal"
        ],
        "text": "Installation\n```shell\npip install shell-gpt\n```\n\nYou'll need an OpenAI API key, you can generate one [here](https://beta.openai.com/account/api-keys). \nYou will be prompted for your key which will then be stored in `~/.config/shell_gpt/.sgptrc`. \n\n",
        "token": [
            "Installation",
            "``",
            "`",
            "shell",
            "pip",
            "install",
            "shell-gpt",
            "``",
            "`",
            "You",
            "'ll",
            "need",
            "an",
            "OpenAI",
            "API",
            "key",
            ",",
            "you",
            "can",
            "generate",
            "one",
            "[",
            "here",
            "]",
            "(",
            "https",
            ":",
            "//beta.openai.com/account/api-keys",
            ")",
            ".",
            "You",
            "will",
            "be",
            "prompted",
            "for",
            "your",
            "key",
            "which",
            "will",
            "then",
            "be",
            "stored",
            "in",
            "`",
            "~/.config/shell_gpt/.sgptrc",
            "`",
            "."
        ],
        "level of complexity": 0
    },
    {
        "url": "https://github.com/TheR1D/shell_gpt",
        "readme_url": "https://raw.githubusercontent.com/TheR1D/shell_gpt/main/README.md",
        "topic": [
            "chatgpt",
            "cheat-sheet",
            "cli",
            "commands",
            "gpt-3",
            "gpt-4",
            "linux",
            "llm",
            "openai",
            "productivity",
            "python",
            "shell",
            "terminal"
        ],
        "text": "Shell integration\nThis is a **very handy feature**, which allows you to use `sgpt` shell completions directly in your terminal, without the need to type `sgpt` with prompt and arguments. Shell integration enables the use of ShellGPT with hotkeys in your terminal, supported by both Bash and ZSH shells. This feature puts `sgpt` completions directly into terminal buffer (input line), allowing for immediate editing of suggested commands.\n\nhttps://github.com/TheR1D/shell_gpt/assets/16740832/bead0dab-0dd9-436d-88b7-6abfb2c556c1\n\nTo install shell integration, run `sgpt --install-integration` and restart your terminal to apply changes. This will add few lines to your `.bashrc` or `.zshrc` file. After that, you can use `Ctrl+l` (by default) to invoke ShellGPT. When you press `Ctrl+l` it will replace you current input line (buffer) with suggested command. You can then edit it and just press `Enter` to execute.\n\n",
        "token": [
            "Shell",
            "integration",
            "This",
            "is",
            "a",
            "*",
            "*",
            "very",
            "handy",
            "feature",
            "*",
            "*",
            ",",
            "which",
            "allows",
            "you",
            "to",
            "use",
            "`",
            "sgpt",
            "`",
            "shell",
            "completions",
            "directly",
            "in",
            "your",
            "terminal",
            ",",
            "without",
            "the",
            "need",
            "to",
            "type",
            "`",
            "sgpt",
            "`",
            "with",
            "prompt",
            "and",
            "arguments",
            ".",
            "Shell",
            "integration",
            "enables",
            "the",
            "use",
            "of",
            "ShellGPT",
            "with",
            "hotkeys",
            "in",
            "your",
            "terminal",
            ",",
            "supported",
            "by",
            "both",
            "Bash",
            "and",
            "ZSH",
            "shells",
            ".",
            "This",
            "feature",
            "puts",
            "`",
            "sgpt",
            "`",
            "completions",
            "directly",
            "into",
            "terminal",
            "buffer",
            "(",
            "input",
            "line",
            ")",
            ",",
            "allowing",
            "for",
            "immediate",
            "editing",
            "of",
            "suggested",
            "commands",
            ".",
            "https",
            ":",
            "//github.com/TheR1D/shell_gpt/assets/16740832/bead0dab-0dd9-436d-88b7-6abfb2c556c1",
            "To",
            "install",
            "shell",
            "integration",
            ",",
            "run",
            "`",
            "sgpt",
            "--",
            "install-integration",
            "`",
            "and",
            "restart",
            "your",
            "terminal",
            "to",
            "apply",
            "changes",
            ".",
            "This",
            "will",
            "add",
            "few",
            "lines",
            "to",
            "your",
            "`",
            ".bashrc",
            "`",
            "or",
            "`",
            ".zshrc",
            "`",
            "file",
            ".",
            "After",
            "that",
            ",",
            "you",
            "can",
            "use",
            "`",
            "Ctrl+l",
            "`",
            "(",
            "by",
            "default",
            ")",
            "to",
            "invoke",
            "ShellGPT",
            ".",
            "When",
            "you",
            "press",
            "`",
            "Ctrl+l",
            "`",
            "it",
            "will",
            "replace",
            "you",
            "current",
            "input",
            "line",
            "(",
            "buffer",
            ")",
            "with",
            "suggested",
            "command",
            ".",
            "You",
            "can",
            "then",
            "edit",
            "it",
            "and",
            "just",
            "press",
            "`",
            "Enter",
            "`",
            "to",
            "execute",
            "."
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/TheR1D/shell_gpt",
        "readme_url": "https://raw.githubusercontent.com/TheR1D/shell_gpt/main/README.md",
        "topic": [
            "chatgpt",
            "cheat-sheet",
            "cli",
            "commands",
            "gpt-3",
            "gpt-4",
            "linux",
            "llm",
            "openai",
            "productivity",
            "python",
            "shell",
            "terminal"
        ],
        "text": "Function calling  \n[Function calls](https://platform.openai.com/docs/guides/function-calling) is a powerful feature OpenAI provides. It allows LLM to execute functions in your system, which can be used to accomplish a variety of tasks. To install [default functions](https://github.com/TheR1D/shell_gpt/tree/main/sgpt/default_functions/) run:\n```shell\nsgpt --install-functions\n```\n\nShellGPT has a convenient way to define functions and use them. In order to create your custom function, navigate to `~/.config/shell_gpt/functions` and create a new .py file with the function name. Inside this file, you can define your function using the following syntax:\n```python\n",
        "token": [
            "Function",
            "calling",
            "[",
            "Function",
            "calls",
            "]",
            "(",
            "https",
            ":",
            "//platform.openai.com/docs/guides/function-calling",
            ")",
            "is",
            "a",
            "powerful",
            "feature",
            "OpenAI",
            "provides",
            ".",
            "It",
            "allows",
            "LLM",
            "to",
            "execute",
            "functions",
            "in",
            "your",
            "system",
            ",",
            "which",
            "can",
            "be",
            "used",
            "to",
            "accomplish",
            "a",
            "variety",
            "of",
            "tasks",
            ".",
            "To",
            "install",
            "[",
            "default",
            "functions",
            "]",
            "(",
            "https",
            ":",
            "//github.com/TheR1D/shell_gpt/tree/main/sgpt/default_functions/",
            ")",
            "run",
            ":",
            "``",
            "`",
            "shell",
            "sgpt",
            "--",
            "install-functions",
            "``",
            "`",
            "ShellGPT",
            "has",
            "a",
            "convenient",
            "way",
            "to",
            "define",
            "functions",
            "and",
            "use",
            "them",
            ".",
            "In",
            "order",
            "to",
            "create",
            "your",
            "custom",
            "function",
            ",",
            "navigate",
            "to",
            "`",
            "~/.config/shell_gpt/functions",
            "`",
            "and",
            "create",
            "a",
            "new",
            ".py",
            "file",
            "with",
            "the",
            "function",
            "name",
            ".",
            "Inside",
            "this",
            "file",
            ",",
            "you",
            "can",
            "define",
            "your",
            "function",
            "using",
            "the",
            "following",
            "syntax",
            ":",
            "``",
            "`",
            "python"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/TheR1D/shell_gpt",
        "readme_url": "https://raw.githubusercontent.com/TheR1D/shell_gpt/main/README.md",
        "topic": [
            "chatgpt",
            "cheat-sheet",
            "cli",
            "commands",
            "gpt-3",
            "gpt-4",
            "linux",
            "llm",
            "openai",
            "productivity",
            "python",
            "shell",
            "terminal"
        ],
        "text": "-> test.json\n```\n\nNote that if for some reason the function (execute_shell_command) will return an error, LLM might try to accomplish the task based on the output. Let's say we don't have installed `jq` in our system, and we ask LLM to parse JSON file:\n```shell\nsgpt \"parse /tmp/test.json file using jq and return only email value\"\n",
        "token": [
            "-",
            ">",
            "test.json",
            "``",
            "`",
            "Note",
            "that",
            "if",
            "for",
            "some",
            "reason",
            "the",
            "function",
            "(",
            "execute_shell_command",
            ")",
            "will",
            "return",
            "an",
            "error",
            ",",
            "LLM",
            "might",
            "try",
            "to",
            "accomplish",
            "the",
            "task",
            "based",
            "on",
            "the",
            "output",
            ".",
            "Let",
            "'s",
            "say",
            "we",
            "do",
            "n't",
            "have",
            "installed",
            "`",
            "jq",
            "`",
            "in",
            "our",
            "system",
            ",",
            "and",
            "we",
            "ask",
            "LLM",
            "to",
            "parse",
            "JSON",
            "file",
            ":",
            "``",
            "`",
            "shell",
            "sgpt",
            "``",
            "parse",
            "/tmp/test.json",
            "file",
            "using",
            "jq",
            "and",
            "return",
            "only",
            "email",
            "value",
            "''"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/TheR1D/shell_gpt",
        "readme_url": "https://raw.githubusercontent.com/TheR1D/shell_gpt/main/README.md",
        "topic": [
            "chatgpt",
            "cheat-sheet",
            "cli",
            "commands",
            "gpt-3",
            "gpt-4",
            "linux",
            "llm",
            "openai",
            "productivity",
            "python",
            "shell",
            "terminal"
        ],
        "text": "-> It appears that jq is not installed on the system. Let me try to install it using brew.\n",
        "token": [
            "-",
            ">",
            "It",
            "appears",
            "that",
            "jq",
            "is",
            "not",
            "installed",
            "on",
            "the",
            "system",
            ".",
            "Let",
            "me",
            "try",
            "to",
            "install",
            "it",
            "using",
            "brew",
            "."
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/TheR1D/shell_gpt",
        "readme_url": "https://raw.githubusercontent.com/TheR1D/shell_gpt/main/README.md",
        "topic": [
            "chatgpt",
            "cheat-sheet",
            "cli",
            "commands",
            "gpt-3",
            "gpt-4",
            "linux",
            "llm",
            "openai",
            "productivity",
            "python",
            "shell",
            "terminal"
        ],
        "text": "-> @FunctionCall execute_shell_command(shell_command=\"brew install jq\")\n",
        "token": [
            "-",
            ">",
            "@",
            "FunctionCall",
            "execute_shell_command",
            "(",
            "shell_command=",
            "''",
            "brew",
            "install",
            "jq",
            "''",
            ")"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/TheR1D/shell_gpt",
        "readme_url": "https://raw.githubusercontent.com/TheR1D/shell_gpt/main/README.md",
        "topic": [
            "chatgpt",
            "cheat-sheet",
            "cli",
            "commands",
            "gpt-3",
            "gpt-4",
            "linux",
            "llm",
            "openai",
            "productivity",
            "python",
            "shell",
            "terminal"
        ],
        "text": "-> jq has been successfully installed. Let me try to parse the file again.\n",
        "token": [
            "-",
            ">",
            "jq",
            "has",
            "been",
            "successfully",
            "installed",
            ".",
            "Let",
            "me",
            "try",
            "to",
            "parse",
            "the",
            "file",
            "again",
            "."
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/TheR1D/shell_gpt",
        "readme_url": "https://raw.githubusercontent.com/TheR1D/shell_gpt/main/README.md",
        "topic": [
            "chatgpt",
            "cheat-sheet",
            "cli",
            "commands",
            "gpt-3",
            "gpt-4",
            "linux",
            "llm",
            "openai",
            "productivity",
            "python",
            "shell",
            "terminal"
        ],
        "text": "Runtime configuration file\nYou can setup some parameters in runtime configuration file `~/.config/shell_gpt/.sgptrc`:\n```text\n",
        "token": [
            "Runtime",
            "configuration",
            "file",
            "You",
            "can",
            "setup",
            "some",
            "parameters",
            "in",
            "runtime",
            "configuration",
            "file",
            "`",
            "~/.config/shell_gpt/.sgptrc",
            "`",
            ":",
            "``",
            "`",
            "text"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/rasbt/LLMs-from-scratch",
        "readme_url": "https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/README.md",
        "topic": [
            "chatgpt",
            "gpt",
            "large-language-models",
            "llm",
            "python",
            "pytorch"
        ],
        "text": "Table of Contents\n\nPlease note that the `Readme.md` file is a Markdown (`.md`) file. If you have downloaded this code bundle from the Manning website and are viewing it on your local computer, I recommend using a Markdown editor or previewer for proper viewing. If you haven't installed a Markdown editor yet, [MarkText](https://www.marktext.cc) is a good free option.\n\nAlternatively, you can view this and other files on GitHub at [https://github.com/rasbt/LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch).\n\n<br>\n<br>\n\n| Chapter Title                                  | Main Code (for quick access)                                                                                                    | All Code + Supplementary      |\n|------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------|-------------------------------|\n| Ch 1: Understanding Large Language Models      | No code                                                                                                                         | No code                       |\n| Ch 2: Working with Text Data                   | - [ch02.ipynb](ch02/01_main-chapter-code/ch02.ipynb)<br/>- [dataloader.ipynb](ch02/01_main-chapter-code/dataloader.ipynb) (summary)<br/>- [exercise-solutions.ipynb](ch02/01_main-chapter-code/exercise-solutions.ipynb) | [./ch02](./ch02)              |\n| Ch 3: Coding Attention Mechanisms              | - [ch03.ipynb](ch03/01_main-chapter-code/ch03.ipynb)<br/>- [multihead-attention.ipynb](ch03/01_main-chapter-code/multihead-attention.ipynb) (summary) | [./ch03](./ch03)              |\n| Ch 4: Implementing a GPT Model from Scratch    | coming soon                                                                                                                     | ...                           |\n| Ch 5: Pretraining on Unlabeled Data            | Q1 2024                                                                                                                         | ...                           |\n| Ch 6: Finetuning for Text Classification       | Q2 2024                                                                                                                         | ...                           |\n| Ch 7: Finetuning with Human Feedback           | Q2 2024                                                                                                                         | ...                           |\n| Ch 8: Using Large Language Models in Practice  | Q2/3 2024                                                                                                                       | ...                           |\n| Appendix A: Introduction to PyTorch*           | - [code-part1.ipynb](appendix-A/03_main-chapter-code/code-part1.ipynb)<br/>- [code-part2.ipynb](appendix-A/03_main-chapter-code/code-part2.ipynb)<br/>- [DDP-script.py](appendix-A/03_main-chapter-code/DDP-script.py)<br/>- [exercise-solutions.ipynb](appendix-A/03_main-chapter-code/exercise-solutions.ipynb) | [./appendix-A](./appendix-A) |\n\n(* Please see [this](appendix-A/01_optional-python-setup-preferences) and [this](appendix-A/02_installing-python-libraries) folder if you need more guidance on installing Python and Python packages.)\n\n\n\n<br>\n<br>\n\n<img src=\"images/mental-model.jpg\" width=\"600px\">\n\n(A mental model summarizing the contents covered in this book.)\n\n",
        "token": [
            "Table",
            "of",
            "Contents",
            "Please",
            "note",
            "that",
            "the",
            "`",
            "Readme.md",
            "`",
            "file",
            "is",
            "a",
            "Markdown",
            "(",
            "`",
            ".md",
            "`",
            ")",
            "file",
            ".",
            "If",
            "you",
            "have",
            "downloaded",
            "this",
            "code",
            "bundle",
            "from",
            "the",
            "Manning",
            "website",
            "and",
            "are",
            "viewing",
            "it",
            "on",
            "your",
            "local",
            "computer",
            ",",
            "I",
            "recommend",
            "using",
            "a",
            "Markdown",
            "editor",
            "or",
            "previewer",
            "for",
            "proper",
            "viewing",
            ".",
            "If",
            "you",
            "have",
            "n't",
            "installed",
            "a",
            "Markdown",
            "editor",
            "yet",
            ",",
            "[",
            "MarkText",
            "]",
            "(",
            "https",
            ":",
            "//www.marktext.cc",
            ")",
            "is",
            "a",
            "good",
            "free",
            "option",
            ".",
            "Alternatively",
            ",",
            "you",
            "can",
            "view",
            "this",
            "and",
            "other",
            "files",
            "on",
            "GitHub",
            "at",
            "[",
            "https",
            ":",
            "//github.com/rasbt/LLMs-from-scratch",
            "]",
            "(",
            "https",
            ":",
            "//github.com/rasbt/LLMs-from-scratch",
            ")",
            ".",
            "<",
            "br",
            ">",
            "<",
            "br",
            ">",
            "|",
            "Chapter",
            "Title",
            "|",
            "Main",
            "Code",
            "(",
            "for",
            "quick",
            "access",
            ")",
            "|",
            "All",
            "Code",
            "+",
            "Supplementary",
            "|",
            "|",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "|",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "-|",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "-|",
            "|",
            "Ch",
            "1",
            ":",
            "Understanding",
            "Large",
            "Language",
            "Models",
            "|",
            "No",
            "code",
            "|",
            "No",
            "code",
            "|",
            "|",
            "Ch",
            "2",
            ":",
            "Working",
            "with",
            "Text",
            "Data",
            "|",
            "-",
            "[",
            "ch02.ipynb",
            "]",
            "(",
            "ch02/01_main-chapter-code/ch02.ipynb",
            ")",
            "<",
            "br/",
            ">",
            "-",
            "[",
            "dataloader.ipynb",
            "]",
            "(",
            "ch02/01_main-chapter-code/dataloader.ipynb",
            ")",
            "(",
            "summary",
            ")",
            "<",
            "br/",
            ">",
            "-",
            "[",
            "exercise-solutions.ipynb",
            "]",
            "(",
            "ch02/01_main-chapter-code/exercise-solutions.ipynb",
            ")",
            "|",
            "[",
            "./ch02",
            "]",
            "(",
            "./ch02",
            ")",
            "|",
            "|",
            "Ch",
            "3",
            ":",
            "Coding",
            "Attention",
            "Mechanisms",
            "|",
            "-",
            "[",
            "ch03.ipynb",
            "]",
            "(",
            "ch03/01_main-chapter-code/ch03.ipynb",
            ")",
            "<",
            "br/",
            ">",
            "-",
            "[",
            "multihead-attention.ipynb",
            "]",
            "(",
            "ch03/01_main-chapter-code/multihead-attention.ipynb",
            ")",
            "(",
            "summary",
            ")",
            "|",
            "[",
            "./ch03",
            "]",
            "(",
            "./ch03",
            ")",
            "|",
            "|",
            "Ch",
            "4",
            ":",
            "Implementing",
            "a",
            "GPT",
            "Model",
            "from",
            "Scratch",
            "|",
            "coming",
            "soon",
            "|",
            "...",
            "|",
            "|",
            "Ch",
            "5",
            ":",
            "Pretraining",
            "on",
            "Unlabeled",
            "Data",
            "|",
            "Q1",
            "2024",
            "|",
            "...",
            "|",
            "|",
            "Ch",
            "6",
            ":",
            "Finetuning",
            "for",
            "Text",
            "Classification",
            "|",
            "Q2",
            "2024",
            "|",
            "...",
            "|",
            "|",
            "Ch",
            "7",
            ":",
            "Finetuning",
            "with",
            "Human",
            "Feedback",
            "|",
            "Q2",
            "2024",
            "|",
            "...",
            "|",
            "|",
            "Ch",
            "8",
            ":",
            "Using",
            "Large",
            "Language",
            "Models",
            "in",
            "Practice",
            "|",
            "Q2/3",
            "2024",
            "|",
            "...",
            "|",
            "|",
            "Appendix",
            "A",
            ":",
            "Introduction",
            "to",
            "PyTorch",
            "*",
            "|",
            "-",
            "[",
            "code-part1.ipynb",
            "]",
            "(",
            "appendix-A/03_main-chapter-code/code-part1.ipynb",
            ")",
            "<",
            "br/",
            ">",
            "-",
            "[",
            "code-part2.ipynb",
            "]",
            "(",
            "appendix-A/03_main-chapter-code/code-part2.ipynb",
            ")",
            "<",
            "br/",
            ">",
            "-",
            "[",
            "DDP-script.py",
            "]",
            "(",
            "appendix-A/03_main-chapter-code/DDP-script.py",
            ")",
            "<",
            "br/",
            ">",
            "-",
            "[",
            "exercise-solutions.ipynb",
            "]",
            "(",
            "appendix-A/03_main-chapter-code/exercise-solutions.ipynb",
            ")",
            "|",
            "[",
            "./appendix-A",
            "]",
            "(",
            "./appendix-A",
            ")",
            "|",
            "(",
            "*",
            "Please",
            "see",
            "[",
            "this",
            "]",
            "(",
            "appendix-A/01_optional-python-setup-preferences",
            ")",
            "and",
            "[",
            "this",
            "]",
            "(",
            "appendix-A/02_installing-python-libraries",
            ")",
            "folder",
            "if",
            "you",
            "need",
            "more",
            "guidance",
            "on",
            "installing",
            "Python",
            "and",
            "Python",
            "packages",
            ".",
            ")",
            "<",
            "br",
            ">",
            "<",
            "br",
            ">",
            "<",
            "img",
            "src=",
            "''",
            "images/mental-model.jpg",
            "''",
            "width=",
            "''",
            "600px",
            "''",
            ">",
            "(",
            "A",
            "mental",
            "model",
            "summarizing",
            "the",
            "contents",
            "covered",
            "in",
            "this",
            "book",
            ".",
            ")"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/sweepai/sweep",
        "readme_url": "https://raw.githubusercontent.com/sweepai/sweep/main/README.md",
        "topic": [
            "ai",
            "code-assistant",
            "code-search",
            "developer-tools",
            "gen-ai",
            "github-app",
            "gpt-35-turbo",
            "gpt-4-32k",
            "llm"
        ],
        "text": "<p align=\"center\">\n    <img src=\"https://github.com/sweepai/sweep/assets/26889185/39d500fc-9276-402c-9ec7-3e61f57ad233\">\n</p>\n<p align=\"center\">\n    <i>Github Issues \u27f6&nbsp; Pull Requests! </i>\n</p>\n<p align=\"center\">\n    <a href=\"https://github.com/apps/sweep-ai\">\n        <img alt=\"Install Sweep Github App\" src=\"https://img.shields.io/badge/Install Sweep-GitHub App-purple?link=https://github.com/apps/sweep-ai\">\n    </a>\n    <a href=\"https://discord.gg/sweep\">\n        <img src=\"https://dcbadge.vercel.app/api/server/sweep?style=flat\" />\n    </a>\n    <a href=\"https://hub.docker.com/r/sweepai/sweep\">\n        <img alt=\"Docker Pulls\" src=\"https://img.shields.io/docker/pulls/sweepai/sweep\" />\n    </a>\n    <a href=\"https://docs.sweep.dev/\">\n        <img alt=\"Docs\" src=\"https://img.shields.io/badge/Docs-docs.sweep.dev-red?link=https%3A%2F%2Fdocs.sweep.dev\">\n    </a>\n    <a href=\"https://github.com/sweepai/sweep\">\n        <img src=\"https://img.shields.io/github/commit-activity/m/sweepai/sweep\" />\n    </a>\n    <a href=\"https://uptime.betterstack.com/?utm_source=status_badge\">\n        <img src=\"https://uptime.betterstack.com/status-badges/v1/monitor/v3bu.svg\" alt=\"Better Stack Badge\">\n    </a>\n    <a href=\"https://hub.docker.com/r/sweepai/sweep\">\n        <img alt=\"Self Host Sweep Docker Image\" src=\"https://img.shields.io/badge/Host Sweep-Docker Image-2496ED?link=https://hub.docker.com/r/sweepai/sweep\">\n    </a>\n    <a href=\"https://github.com/sweepai/sweep/actions/workflows/unittest.yml\">\n        <img src=\"https://github.com/sweepai/sweep/actions/workflows/unittest.yml/badge.svg\" alt=\"Python Unit Tests\">\n    </a>\n</p>\n\n---\n\n<b>Sweep</b> is an AI junior developer that turns bugs and feature requests into code changes. Sweep automatically handles devex improvements like adding typehints/improving test coverage. :robot:\n\n[Install Sweep](https://github.com/apps/sweep-ai) and open a Github Issue like: `Sweep: Add typehints to src/utils/github_utils.py` and Sweep will:\n1. Search through your codebase to find the dependencies of github_utils.py\n2. Modify the code to add typehints\n3. **Run and debug your code to write a Pull Request** \u26a1\n\n",
        "token": [
            "<",
            "p",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "<",
            "img",
            "src=",
            "''",
            "https",
            ":",
            "//github.com/sweepai/sweep/assets/26889185/39d500fc-9276-402c-9ec7-3e61f57ad233",
            "''",
            ">",
            "<",
            "/p",
            ">",
            "<",
            "p",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "<",
            "i",
            ">",
            "Github",
            "Issues",
            "\u27f6",
            "&",
            "nbsp",
            ";",
            "Pull",
            "Requests",
            "!",
            "<",
            "/i",
            ">",
            "<",
            "/p",
            ">",
            "<",
            "p",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "<",
            "a",
            "href=",
            "''",
            "https",
            ":",
            "//github.com/apps/sweep-ai",
            "''",
            ">",
            "<",
            "img",
            "alt=",
            "''",
            "Install",
            "Sweep",
            "Github",
            "App",
            "''",
            "src=",
            "''",
            "https",
            ":",
            "//img.shields.io/badge/Install",
            "Sweep-GitHub",
            "App-purple",
            "?",
            "link=https",
            ":",
            "//github.com/apps/sweep-ai",
            "''",
            ">",
            "<",
            "/a",
            ">",
            "<",
            "a",
            "href=",
            "''",
            "https",
            ":",
            "//discord.gg/sweep",
            "''",
            ">",
            "<",
            "img",
            "src=",
            "''",
            "https",
            ":",
            "//dcbadge.vercel.app/api/server/sweep",
            "?",
            "style=flat",
            "''",
            "/",
            ">",
            "<",
            "/a",
            ">",
            "<",
            "a",
            "href=",
            "''",
            "https",
            ":",
            "//hub.docker.com/r/sweepai/sweep",
            "''",
            ">",
            "<",
            "img",
            "alt=",
            "''",
            "Docker",
            "Pulls",
            "''",
            "src=",
            "''",
            "https",
            ":",
            "//img.shields.io/docker/pulls/sweepai/sweep",
            "''",
            "/",
            ">",
            "<",
            "/a",
            ">",
            "<",
            "a",
            "href=",
            "''",
            "https",
            ":",
            "//docs.sweep.dev/",
            "''",
            ">",
            "<",
            "img",
            "alt=",
            "''",
            "Docs",
            "''",
            "src=",
            "''",
            "https",
            ":",
            "//img.shields.io/badge/Docs-docs.sweep.dev-red",
            "?",
            "link=https",
            "%",
            "3A",
            "%",
            "2F",
            "%",
            "2Fdocs.sweep.dev",
            "''",
            ">",
            "<",
            "/a",
            ">",
            "<",
            "a",
            "href=",
            "''",
            "https",
            ":",
            "//github.com/sweepai/sweep",
            "''",
            ">",
            "<",
            "img",
            "src=",
            "''",
            "https",
            ":",
            "//img.shields.io/github/commit-activity/m/sweepai/sweep",
            "''",
            "/",
            ">",
            "<",
            "/a",
            ">",
            "<",
            "a",
            "href=",
            "''",
            "https",
            ":",
            "//uptime.betterstack.com/",
            "?",
            "utm_source=status_badge",
            "''",
            ">",
            "<",
            "img",
            "src=",
            "''",
            "https",
            ":",
            "//uptime.betterstack.com/status-badges/v1/monitor/v3bu.svg",
            "''",
            "alt=",
            "''",
            "Better",
            "Stack",
            "Badge",
            "''",
            ">",
            "<",
            "/a",
            ">",
            "<",
            "a",
            "href=",
            "''",
            "https",
            ":",
            "//hub.docker.com/r/sweepai/sweep",
            "''",
            ">",
            "<",
            "img",
            "alt=",
            "''",
            "Self",
            "Host",
            "Sweep",
            "Docker",
            "Image",
            "''",
            "src=",
            "''",
            "https",
            ":",
            "//img.shields.io/badge/Host",
            "Sweep-Docker",
            "Image-2496ED",
            "?",
            "link=https",
            ":",
            "//hub.docker.com/r/sweepai/sweep",
            "''",
            ">",
            "<",
            "/a",
            ">",
            "<",
            "a",
            "href=",
            "''",
            "https",
            ":",
            "//github.com/sweepai/sweep/actions/workflows/unittest.yml",
            "''",
            ">",
            "<",
            "img",
            "src=",
            "''",
            "https",
            ":",
            "//github.com/sweepai/sweep/actions/workflows/unittest.yml/badge.svg",
            "''",
            "alt=",
            "''",
            "Python",
            "Unit",
            "Tests",
            "''",
            ">",
            "<",
            "/a",
            ">",
            "<",
            "/p",
            ">",
            "--",
            "-",
            "<",
            "b",
            ">",
            "Sweep",
            "<",
            "/b",
            ">",
            "is",
            "an",
            "AI",
            "junior",
            "developer",
            "that",
            "turns",
            "bugs",
            "and",
            "feature",
            "requests",
            "into",
            "code",
            "changes",
            ".",
            "Sweep",
            "automatically",
            "handles",
            "devex",
            "improvements",
            "like",
            "adding",
            "typehints/improving",
            "test",
            "coverage",
            ".",
            ":",
            "robot",
            ":",
            "[",
            "Install",
            "Sweep",
            "]",
            "(",
            "https",
            ":",
            "//github.com/apps/sweep-ai",
            ")",
            "and",
            "open",
            "a",
            "Github",
            "Issue",
            "like",
            ":",
            "`",
            "Sweep",
            ":",
            "Add",
            "typehints",
            "to",
            "src/utils/github_utils.py",
            "`",
            "and",
            "Sweep",
            "will",
            ":",
            "1",
            ".",
            "Search",
            "through",
            "your",
            "codebase",
            "to",
            "find",
            "the",
            "dependencies",
            "of",
            "github_utils.py",
            "2",
            ".",
            "Modify",
            "the",
            "code",
            "to",
            "add",
            "typehints",
            "3",
            ".",
            "*",
            "*",
            "Run",
            "and",
            "debug",
            "your",
            "code",
            "to",
            "write",
            "a",
            "Pull",
            "Request",
            "*",
            "*",
            "\u26a1"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/sweepai/sweep",
        "readme_url": "https://raw.githubusercontent.com/sweepai/sweep/main/README.md",
        "topic": [
            "ai",
            "code-assistant",
            "code-search",
            "developer-tools",
            "gen-ai",
            "github-app",
            "gpt-35-turbo",
            "gpt-4-32k",
            "llm"
        ],
        "text": "Getting Started\n\n",
        "token": [
            "Getting",
            "Started"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/sweepai/sweep",
        "readme_url": "https://raw.githubusercontent.com/sweepai/sweep/main/README.md",
        "topic": [
            "ai",
            "code-assistant",
            "code-search",
            "developer-tools",
            "gen-ai",
            "github-app",
            "gpt-35-turbo",
            "gpt-4-32k",
            "llm"
        ],
        "text": "GitHub App\nInstall Sweep by adding the [**Sweep GitHub App**](https://github.com/apps/sweep-ai) to your desired repositories.\n\n* For more details, visit our [installation page](https://docs.sweep.dev/getting-started).\n\n* Note: Sweep only considers issues with the \"Sweep:\" title on creation and not on update. If you want Sweep to pick up an existing issue, you can add the \"Sweep\" label to the issue.\n\n* We focus on Python but support all languages GPT-4 can write. This includes JS/TS, Rust, Go, Java, C",
        "token": [
            "GitHub",
            "App",
            "Install",
            "Sweep",
            "by",
            "adding",
            "the",
            "[",
            "*",
            "*",
            "Sweep",
            "GitHub",
            "App",
            "*",
            "*",
            "]",
            "(",
            "https",
            ":",
            "//github.com/apps/sweep-ai",
            ")",
            "to",
            "your",
            "desired",
            "repositories",
            ".",
            "*",
            "For",
            "more",
            "details",
            ",",
            "visit",
            "our",
            "[",
            "installation",
            "page",
            "]",
            "(",
            "https",
            ":",
            "//docs.sweep.dev/getting-started",
            ")",
            ".",
            "*",
            "Note",
            ":",
            "Sweep",
            "only",
            "considers",
            "issues",
            "with",
            "the",
            "``",
            "Sweep",
            ":",
            "''",
            "title",
            "on",
            "creation",
            "and",
            "not",
            "on",
            "update",
            ".",
            "If",
            "you",
            "want",
            "Sweep",
            "to",
            "pick",
            "up",
            "an",
            "existing",
            "issue",
            ",",
            "you",
            "can",
            "add",
            "the",
            "``",
            "Sweep",
            "''",
            "label",
            "to",
            "the",
            "issue",
            ".",
            "*",
            "We",
            "focus",
            "on",
            "Python",
            "but",
            "support",
            "all",
            "languages",
            "GPT-4",
            "can",
            "write",
            ".",
            "This",
            "includes",
            "JS/TS",
            ",",
            "Rust",
            ",",
            "Go",
            ",",
            "Java",
            ",",
            "C"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/sweepai/sweep",
        "readme_url": "https://raw.githubusercontent.com/sweepai/sweep/main/README.md",
        "topic": [
            "ai",
            "code-assistant",
            "code-search",
            "developer-tools",
            "gen-ai",
            "github-app",
            "gpt-35-turbo",
            "gpt-4-32k",
            "llm"
        ],
        "text": "Contributing\n\nContributions are welcome and greatly appreciated! To get set up, see [Development](https://github.com/sweepai/sweep#development). For detailed guidelines on how to contribute, please see the [CONTRIBUTING.md](CONTRIBUTING.md) file.\n\n\n<h2 align=\"center\">\n    Contributors\n</h2>\n<p align=\"center\">\n    Thank you for your contribution!\n</p>\n<p align=\"center\">\n    <a href=\"https://github.com/sweepai/sweep/graphs/contributors\">\n      <img src=\"https://contrib.rocks/image?repo=sweepai/sweep\" />\n    </a>\n</p>\n<p align=\"center\">\n    and, of course, Sweep!\n</p>\n",
        "token": [
            "Contributing",
            "Contributions",
            "are",
            "welcome",
            "and",
            "greatly",
            "appreciated",
            "!",
            "To",
            "get",
            "set",
            "up",
            ",",
            "see",
            "[",
            "Development",
            "]",
            "(",
            "https",
            ":",
            "//github.com/sweepai/sweep",
            "#",
            "development",
            ")",
            ".",
            "For",
            "detailed",
            "guidelines",
            "on",
            "how",
            "to",
            "contribute",
            ",",
            "please",
            "see",
            "the",
            "[",
            "CONTRIBUTING.md",
            "]",
            "(",
            "CONTRIBUTING.md",
            ")",
            "file",
            ".",
            "<",
            "h2",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "Contributors",
            "<",
            "/h2",
            ">",
            "<",
            "p",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "Thank",
            "you",
            "for",
            "your",
            "contribution",
            "!",
            "<",
            "/p",
            ">",
            "<",
            "p",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "<",
            "a",
            "href=",
            "''",
            "https",
            ":",
            "//github.com/sweepai/sweep/graphs/contributors",
            "''",
            ">",
            "<",
            "img",
            "src=",
            "''",
            "https",
            ":",
            "//contrib.rocks/image",
            "?",
            "repo=sweepai/sweep",
            "''",
            "/",
            ">",
            "<",
            "/a",
            ">",
            "<",
            "/p",
            ">",
            "<",
            "p",
            "align=",
            "''",
            "center",
            "''",
            ">",
            "and",
            ",",
            "of",
            "course",
            ",",
            "Sweep",
            "!",
            "<",
            "/p",
            ">"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/continuedev/continue",
        "readme_url": "https://raw.githubusercontent.com/continuedev/continue/main/README.md",
        "topic": [
            "ai",
            "chatgpt",
            "copilot",
            "developer-tools",
            "intellij",
            "jetbrains",
            "llm",
            "open-source",
            "openai",
            "pycharm",
            "software-development",
            "visual-studio-code",
            "vscode"
        ],
        "text": "Getting Started\n\n",
        "token": [
            "Getting",
            "Started"
        ],
        "level of complexity": -1
    },
    {
        "url": "https://github.com/continuedev/continue",
        "readme_url": "https://raw.githubusercontent.com/continuedev/continue/main/README.md",
        "topic": [
            "ai",
            "chatgpt",
            "copilot",
            "developer-tools",
            "intellij",
            "jetbrains",
            "llm",
            "open-source",
            "openai",
            "pycharm",
            "software-development",
            "visual-studio-code",
            "vscode"
        ],
        "text": "Download for [VS Code](https://marketplace.visualstudio.com/items?itemName=Continue.continue) and [JetBrains](https://plugins.jetbrains.com/plugin/22707-continue-extension)\n\nYou can try out Continue for free using a proxy server that securely makes calls with our API key to models like GPT-4, Gemini Pro, and Phind CodeLlama via OpenAI, Google, and Together respectively.\n\nOnce you're ready to use your own API key or a different model / provider, press the `+` button in the bottom left to add a new model to your `config.json`. Learn more about the models and providers [here](https://continue.dev/docs/model-setup/overview).\n\n",
        "token": [
            "Download",
            "for",
            "[",
            "VS",
            "Code",
            "]",
            "(",
            "https",
            ":",
            "//marketplace.visualstudio.com/items",
            "?",
            "itemName=Continue.continue",
            ")",
            "and",
            "[",
            "JetBrains",
            "]",
            "(",
            "https",
            ":",
            "//plugins.jetbrains.com/plugin/22707-continue-extension",
            ")",
            "You",
            "can",
            "try",
            "out",
            "Continue",
            "for",
            "free",
            "using",
            "a",
            "proxy",
            "server",
            "that",
            "securely",
            "makes",
            "calls",
            "with",
            "our",
            "API",
            "key",
            "to",
            "models",
            "like",
            "GPT-4",
            ",",
            "Gemini",
            "Pro",
            ",",
            "and",
            "Phind",
            "CodeLlama",
            "via",
            "OpenAI",
            ",",
            "Google",
            ",",
            "and",
            "Together",
            "respectively",
            ".",
            "Once",
            "you",
            "'re",
            "ready",
            "to",
            "use",
            "your",
            "own",
            "API",
            "key",
            "or",
            "a",
            "different",
            "model",
            "/",
            "provider",
            ",",
            "press",
            "the",
            "`",
            "+",
            "`",
            "button",
            "in",
            "the",
            "bottom",
            "left",
            "to",
            "add",
            "a",
            "new",
            "model",
            "to",
            "your",
            "`",
            "config.json",
            "`",
            ".",
            "Learn",
            "more",
            "about",
            "the",
            "models",
            "and",
            "providers",
            "[",
            "here",
            "]",
            "(",
            "https",
            ":",
            "//continue.dev/docs/model-setup/overview",
            ")",
            "."
        ],
        "level of complexity": -1
    }
]