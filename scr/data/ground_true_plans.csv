readme,text,plan,"label (0:source,1:packagemanager,2:container,3:binary)"
https://raw.githubusercontent.com/AAAI-DISIM-UnivAQ/DALI/master/README.md,"## Installation

**OS X & Linux:**
1.  To download and install SICStus Prolog (it is needed), follow the instructions at https://sicstus.sics.se/download4.html.
2.  Then, you can download DALI and test it by running an example DALI MAS:
```sh
git clone https://github.com/AAAI-DISIM-UnivAQ/DALI.git
cd DALI/Examples/advanced
bash startmas.sh
```
&nbsp;&nbsp;&nbsp;&nbsp; You will see different windows opening:
* &nbsp;&nbsp;&nbsp;&nbsp; Prolog LINDA server (active_server_wi.pl)
* &nbsp;&nbsp;&nbsp;&nbsp; Prolog FIPA client (active_user_wi.pl) 
* &nbsp;&nbsp;&nbsp;&nbsp; 1 instance of DALI metaintepreter for each agent (active_dali_wi.pl)
**Windows:**
1.  To download and install SICStus Prolog (it is needed), follow the instructions at https://sicstus.sics.se/download4.html.
2.  Then, you can download DALI from https://github.com/AAAI-DISIM-UnivAQ/DALI.git.
3.  Unzip the repository, go to the folder ""DALI/Examples/basic"", and test if DALI works by duble clicking ""startmas.bat"" file (this will launch an example DALI MAS). \
\
&nbsp;&nbsp;&nbsp;&nbsp; You will see different windows opening:
* &nbsp;&nbsp;&nbsp;&nbsp; Prolog LINDA server (active_server_wi.pl)
* &nbsp;&nbsp;&nbsp;&nbsp; Prolog FIPA client (active_user_wi.pl) 
* &nbsp;&nbsp;&nbsp;&nbsp; 1 instance of DALI metaintepreter for each agent (active_dali_wi.pl)",source,0
https://raw.githubusercontent.com/cfeng783/GTT/main/README.md,"#### Install dependencies (with python 3.10) 

```shell
pip install -r requirements.txt
```

## Run Experiments

#### Run the zero-shot experiments

```shell
cd src
python test_zeroshot.py --gpu [GPUs] --batch_size [BS] --mode [mode] --data [DS] --uni [uni]
```
Specify mode to one of the following: tiny, small, large.

Specify data to one of the following: m1, m2, h1, h2, electricity, weather, traffic, ill.

Specify uni to 0 or 1, 0: multivariate forecast, 1: univariate forecast

#### Run the fine-tune experiments

```shell
cd experiments
python test_finetune.py --gpu [GPUs] --batch_size [BS] --mode [mode] --data [DS] --uni [uni] --epochs [eps]
```",source,0
https://raw.githubusercontent.com/sanjay-810/AYDIV2/main/README.md,"### **Installation**
1.  Prepare for the running environment: You can use the docker image provided by [`OpenPCDet`](https://github.com/open-mmlab/OpenPCDet). Our experiments are based on the docker provided by Voxel-R-CNN and we use NVIDIA Tesla V100 to train our Aydiv.",container,2
https://raw.githubusercontent.com/sanjay-810/AYDIV2/main/README.md,"    You can use the docker image provided by [`OpenPCDet`](https://github.com/open-mmlab/OpenPCDet). Our experiments are based on the docker provided by Voxel-R-CNN and we use NVIDIA Tesla V100 to train our Aydiv.
2. Prepare for the data.
 Convert Argoverse 2 (or) waymo open Dataset into kitti format [`converter` (https://github.com/sanjay-810/AYDIV_ICRA/tree/main/data_converter/convert) Please prepare dataset as [`OpenPCDet`](https://github.com/open-mmlab/OpenPCDet).
    ```
    cd Aydiv
    python depth_to_lidar.py
    ```",container,2
https://raw.githubusercontent.com/BingqingCheng/cace/main/README.md,Please refer to the `setup.py` file for installation instructions.,source,0
https://raw.githubusercontent.com/less-and-less-bugs/Trust_TELLER/main/README.md,"Step 1: Download the dataset folder from onedrive by [data.zip](https://portland-my.sharepoint.com/:u:/g/personal/liuhui3-c_my_cityu_edu_hk/EfApQlFP3PhFjUW4527STo0BALMdP16zs-HPMNgwQVFWsA?e=zoHlW2). Unzip this folder into the project  directory.  You can find four orginal datasets,  pre-processed datasets (i.e., val.jsonl, test.jsonl, train.jsonl in each dataset folder) and the files incuding questions and answers 
Step 2: Place you OpenAI key into the file named api_key.txt. 
openai.api.key= """"",source,0
https://raw.githubusercontent.com/utiasASRL/steam_icp/master/README.md,Clone this repository and its submodules.,container,2
https://raw.githubusercontent.com/utiasASRL/steam_icp/master/README.md,We use docker to install dependencies. The recommended way to build the docker image is,container,2
https://raw.githubusercontent.com/utiasASRL/steam_icp/master/README.md,docker build -t steam_icp \,container,2
https://raw.githubusercontent.com/utiasASRL/steam_icp/master/README.md,  --build-arg USERID=$(id -u) \,container,2
https://raw.githubusercontent.com/utiasASRL/steam_icp/master/README.md,  --build-arg GROUPID=$(id -g) \,container,2
https://raw.githubusercontent.com/utiasASRL/steam_icp/master/README.md,  --build-arg USERNAME=$(whoami) \,container,2
https://raw.githubusercontent.com/utiasASRL/steam_icp/master/README.md,  --build-arg HOMEDIR=${HOME} .,container,2
https://raw.githubusercontent.com/utiasASRL/steam_icp/master/README.md,"When starting a container, remember to mount the code, dataset, and output directories to proper locations in the container.",container,2
https://raw.githubusercontent.com/utiasASRL/steam_icp/master/README.md,An example command to start a docker container with the image is,container,2
https://raw.githubusercontent.com/utiasASRL/steam_icp/master/README.md,docker run -it --name steam_icp \,container,2
https://raw.githubusercontent.com/utiasASRL/steam_icp/master/README.md,  --privileged \,container,2
https://raw.githubusercontent.com/utiasASRL/steam_icp/master/README.md,  --network=host \,container,2
https://raw.githubusercontent.com/utiasASRL/steam_icp/master/README.md,  -e DISPLAY=$DISPLAY \,container,2
https://raw.githubusercontent.com/utiasASRL/steam_icp/master/README.md,  -v /tmp/.X11-unix:/tmp/.X11-unix \,container,2
https://raw.githubusercontent.com/utiasASRL/steam_icp/master/README.md,  -v ${HOME}:${HOME}:rw \,container,2
https://raw.githubusercontent.com/utiasASRL/steam_icp/master/README.md,  steam_icp,container,2
https://raw.githubusercontent.com/utiasASRL/steam_icp/master/README.md,(Inside Container) Go to the root directory of this repository and build STEAM-ICP,container,2
https://raw.githubusercontent.com/utiasASRL/steam_icp/master/README.md,bash build.sh,container,2
https://raw.githubusercontent.com/PFischbeck/parameter-fitting-experiments/main/Readme.md,"# Installation

- Make sure you have Python, Pip and R installed.
- Checkout this repository
- Install the python dependencies with

```
pip3 install -r requirements.txt
```
- Install the `pygirgs` package at https://github.com/PFischbeck/pygirgs
- Install the R dependencies (used for plots) with
```
R -e 'install.packages(c(""ggplot2"", ""reshape2"", ""plyr"", ""dplyr"", ""scales""), repos=""https://cloud.r-project.org/"")'
```
- Download the file `konect-data.zip` from [Zenodo](https://doi.org/10.5281/zenodo.10629451) and extract its contents into the folder `input_data/konect`",source,0
https://raw.githubusercontent.com/jonarriza96/gsft/main/README.md,"## Installation
### Dependencies
Initialize git submodules with
```
    git submodule init
    git submodule update
```
### Python environment
Install the specific versions of every package from `requirements.txt` in a new conda environment:
```
conda create --name gsft python=3.9
conda activate gsft
pip install -r requirements.txt
```
To ensure that Python paths are properly defined, update the `~/.bashrc` by adding the following lines
```
export GSFT_PATH=/path_to_gsfc
export PYTHONPATH=$PYTHONPATH:/$GSFT_PATH
```",source,0
https://raw.githubusercontent.com/EricssonResearch/Line-Based-Room-Segmentation-and-EDF/release/README.md,"## Installation
The project can be installed by running the following command in your terminal:
```bash
pip install -r requirements.txt
```",source,0
https://raw.githubusercontent.com/viralInformatics/VIGA/master/README.md,"## Installation
### Step1: Download VIGA
Download VIGA with Git from GitHub
```
git clone https://github.com/viralInformatics/VIGA.git
```
or Download ZIP to local",source,0
https://raw.githubusercontent.com/viralInformatics/VIGA/master/README.md,"### Step 2: Download Database``
1. download taxdmp.zip [Index of /pub/taxonomy (nih.gov)](https://ftp.ncbi.nlm.nih.gov/pub/taxonomy/) and unzip taxdmp.zip and put it in ./db/
2. download ""prot.accession2taxid"" file from https://ftp.ncbi.nlm.nih.gov/pub/taxonomy/accession2taxid/
3. download ""RefSeqVirusProtein"" file from
wget -c ftp.ncbi.nlm.nih.gov/refseq/release/viral/viral.1.protein.faa.gz
gzip -d viral.1.protein.faa.gz
mv viral.1.protein.faa RefSeqVirusProtein
4. download ""nr"" file from
wget -c ftp://ftp.ncbi.nlm.nih.gov/blast/db/FASTA/nr.gz
or ascp -T  -i  asperaweb_id_dsa.openssh --host=ftp.ncbi.nih.gov --user=anonftp --mode=recv /blast/db/FASTA/nr.gz ./
gzip -d nr.gz
5. Use Diamond v2.0.11.149 to create two separate databases as the indexing libraries in the current version are incompatible with each other.
6. In order to set up a reference database for DIAMOND, the makedb command needs to be executed with the following command line:
diamond makedb --in YourPath/RefSeqVirusProtein  -d Diamond_RefSeqVirusProtein --taxonmap YourPath/prot.accession2taxid --taxonnodes YourPath/nodes.dmp
diamond makedb --in nr -d Dimond_nr --taxonmap YourPath/prot.accession2taxid --taxonnodes YourPath/nodes.dmp",source,0
https://raw.githubusercontent.com/viralInformatics/VIGA/master/README.md,"### Step 3: Installation of dependent software
#### Installing Some Software Using Conda
```
conda install fastp=0.12.4 trinity=2.8.5 diamond=2.0.11.149 ragtag=2.1.0 quast=5.0.2
```",source,0
https://raw.githubusercontent.com/viralInformatics/VIGA/master/README.md,"#### Manual Installation of MetaCompass
https://github.com/marbl/MetaCompass
### Step 4: Python Dependencies
Base on python 3.6.8
```
pip install pandas=1.1.5 numpy=1.19.5  matplotlib=3.3.4  biopython=1.79
```",source,0
https://raw.githubusercontent.com/scimemia/NRN-EZ/master/README.md,"Installation instructions for Linux (Ubuntu and Pop!_OS): download the Linux zip file and, from the command window, run a bash command for the install.sh file, in the corresponding installation folder. 
Installation instructions for Mac OS: download the Mac zip file and copy the NRN-EZ app to the Applications folder. 
Installation instructions for Windows: download the Win zip file and run the installation wizard.",binary,3
https://raw.githubusercontent.com/nand1155/CausNet/main/README.md,"You can install the development version from GitHub with:
``` r
require(""devtools"")
install_github(""https://github.com/nand1155/CausNet"")",source,0
https://raw.githubusercontent.com/dyxstat/Reproduce_ViralCC/main/README.md,"*Step 1 Download and preprocess the raw data**
```
wget https://sra-downloadb.be-md.ncbi.nlm.nih.gov/sos2/sra-pub-run-13/ERR2282092/ERR2282092.1
wget https://sra-downloadb.be-md.ncbi.nlm.nih.gov/sos2/sra-pub-run-13/ERR2530126/ERR2530126.1
wget https://sra-downloadb.be-md.ncbi.nlm.nih.gov/sos2/sra-pub-run-13/ERR2530127/ERR2530127.1
fastq-dump --split-files --gzip ERR2282092.1
fastq-dump --split-files --gzip ERR2530126.1
fastq-dump --split-files --gzip ERR2530127.1
bbduk.sh  in1=ERR2282092.1_1.fastq.gz in2=ERR2282092.1_2.fastq.gz out1=COWSG1_AQ.fastq.gz out2=COWSG2_AQ.fastq.gz ref=/home1/yuxuandu/cmb/SOFTWARE/bbmap/resources/adapters.fa ktrim=r k=23 mink=11 hdist=1 minlen=50 tpe tbo
bbduk.sh  in1=ERR2530126.1_1.fastq.gz in2=ERR2530126.1_2.fastq.gz out1=S3HIC1_AQ.fastq.gz out2=S3HIC2_AQ.fastq.gz ref=/home1/yuxuandu/cmb/SOFTWARE/bbmap/resources/adapters.fa ktrim=r k=23 mink=11 hdist=1 minlen=50 tpe tbo
bbduk.sh  in1=ERR2530127.1_1.fastq.gz in2=ERR2530127.1_2.fastq.gz out1=M1HIC1_AQ.fastq.gz out2=M1HIC2_AQ.fastq.gz ref=/home1/yuxuandu/cmb/SOFTWARE/bbmap/resources/adapters.fa ktrim=r k=23 mink=11 hdist=1 minlen=50 tpe tbo
bbduk.sh  in1=S3HIC1_AQ.fastq.gz in2=S3HIC2_AQ.fastq.gz out1=S3HIC1_CL.fastq.gz out2=S3HIC2_CL.fastq.gz trimq=10 qtrim=r ftm=5 minlen=50
bbduk.sh  in1=M1HIC1_AQ.fastq.gz in2=M1HIC2_AQ.fastq.gz out1=M1HIC1_CL.fastq.gz out2=M1HIC2_CL.fastq.gz trimq=10 qtrim=r ftm=5 minlen=50
bbduk.sh  in1=COWSG1_AQ.fastq.gz in2=COWSG2_AQ.fastq.gz out1=COWSG1_CL.fastq.gz out2=COWSG2_CL.fastq.gz trimq=10 qtrim=r ftm=5 minlen=50
bbduk.sh in1=S3HIC1_CL.fastq.gz in2=S3HIC2_CL.fastq.gz out1=S3HIC1_trim.fastq.gz out2=S3HIC2_trim.fastq.gz ftl=10
bbduk.sh in1=M1HIC1_CL.fastq.gz in2=M1HIC2_CL.fastq.gz out1=M1HIC1_trim.fastq.gz out2=M1HIC2_trim.fastq.gz ftl=10
clumpify.sh in1=S3HIC1_trim.fastq.gz in2=S3HIC2_trim.fastq.gz out1=S3HIC1_dedup.fastq.gz out2=S3HIC2_dedup.fastq.gz dedupe
clumpify.sh in1=M1HIC1_trim.fastq.gz in2=M1HIC2_trim.fastq.gz out1=M1HIC1_dedup.fastq.gz out2=M1HIC2_dedup.fastq.gz dedupe
cat S3HIC1_dedup.fastq.gz M1HIC1_dedup.fastq.gz > HIC1.fastq.gz
cat S3HIC2_dedup.fastq.gz M1HIC2_dedup.fastq.gz > HIC2.fastq.gz
```
**Step 2: Assemble contigs and align processed Hi-C reads to contigs**
```
megahit -1 COWSG1_CL.fastq.gz -2 COWSG2_CL.fastq.gz -o COW_ASSEMBLY --min-contig-len 1000 --k-min 21 --k-max 141 --k-step 12 --merge-level 20,0.95
bwa index final.contigs.fa
bwa mem -5SP final.contigs.fa HIC1.fastq.gz HIC2.fastq.gz > COW_MAP.sam
samtools view -F 0x904 -bS COW_MAP.sam > COW_MAP_UNSORTED.bam
samtools sort -n COW_MAP_UNSORTED.bam -o COW_MAP_SORTED.bam
```
**Step3: Identify viral contigs from assembled contigs**
```
perl removesmalls.pl 3000 final.contigs.fa > cow_3000.fa
wrapper_phage_contigs_sorter_iPlant.pl -f cow_3000.fa --db 1 --wdir output_directory --ncpu 16 --data-dir /panfs/qcb-panasas/yuxuandu/virsorter-data
Rscript find_viral_contig.R
```
**Step4: Run ViralCC**
```
python ./viralcc.py pipeline -v final.contigs.fa COW_MAP_SORTED.bam viral.txt out_cow
```
**Step5: Evaluation draft viral genomes using CheckV**
```
python concatenation.py -p out_cow/VIRAL_BIN -o viralCC_cow_bins.fa
checkv end_to_end viralCC_cow_bins.fa output_checkv_viralcc_cow -t 16 -d /panfs/qcb-panasas/yuxuandu/checkv-db-v1.0",source,0
https://raw.githubusercontent.com/apple/ml-mgie/main/README.md,"## Requirements
```
conda create -n mgie python=3.10 -y
conda activate mgie
conda update -n base -c defaults conda setuptools -y
conda install -c conda-forge git git-lfs ffmpeg vim htop ninja gpustat -y
conda clean -a -y

pip install -U pip cmake cython==0.29.36 pydantic==1.10 numpy
pip install -U gdown pydrive2 wget jupyter jupyterlab jupyterthemes ipython
pip install -U sentencepiece transformers diffusers tokenizers datasets gradio==3.37 accelerate evaluate git+https://github.com/openai/CLIP.git
pip install -U https://download.pytorch.org/whl/cu113/torch-1.12.0%2Bcu113-cp310-cp310-linux_x86_64.whl https://download.pytorch.org/whl/cu113/torchvision-0.13.0%2Bcu113-cp310-cp310-linux_x86_64.whl https://download.pytorch.org/whl/cu113/torchaudio-0.12.0%2Bcu113-cp310-cp310-linux_x86_64.whl
pip install -U deepspeed

# git clone this repo
cd ml-mgie
git submodule update --init --recursive
cd LLaVA
pip install -e .
pip install -U https://download.pytorch.org/whl/cu113/torch-1.12.0%2Bcu113-cp310-cp310-linux_x86_64.whl https://download.pytorch.org/whl/cu113/torchvision-0.13.0%2Bcu113-cp310-cp310-linux_x86_64.whl https://download.pytorch.org/whl/cu113/torchaudio-0.12.0%2Bcu113-cp310-cp310-linux_x86_64.whl
pip install -U ninja flash-attn==1.0.2
pip install -U pydrive2 gdown wget

cd ..
cp mgie_llava.py LLaVA/llava/model/llava.py
cp mgie_train.py LLaVA/llava/train/train.py
```",source,0
https://raw.githubusercontent.com/uclaml/SPIN/main/README.md,"## Setup
The following steps provide the necessary setup to run our codes.
1. Create a Python virtual environment with Conda:
```
conda create -n myenv python=3.10
conda activate myenv
```
2. Install PyTorch `v2.1.0` with compatible cuda version, following instructions from [PyTorch Installation Page](https://pytorch.org/get-started/locally/). For example with cuda 11:
```
pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu118
```
3. Install the following Python dependencies to run the codes.
```
python -m pip install .
python -m pip install flash-attn --no-build-isolation
```
4. Login to your huggingface account for downloading models
```
huggingface-cli login --token ""${your_access_token}""
```",source,0
https://raw.githubusercontent.com/ncbi/GeneGPT/main/README.md,"## Requirements
The code has been tested with Python 3.9.13. Please first install the required packages by:
```bash
pip install -r requirements.txt
```
You also need an OpenAI API key to run GeneGPT with Codex. Replace the placeholder with your key in `config.py`:
```bash
$ cat config.py 
API_KEY = 'YOUR_OPENAI_API_KEY'
```",source,0
https://raw.githubusercontent.com/arplaboratory/learning-to-fly/master/README.MD,"## Instructions to run the code
### Docker (isolated)
We provide a pre-built Docker image with a simple web interface that can be executed using a single command (given that Docker is already installed on your machine):
```
docker run -it --rm -p 8000:8000 arpllab/learning_to_fly
```
After the container is running, navigate to [https://0.0.0.0:8000](https://0.0.0.0:8000) and you should see something like (after starting the training):

<div align=""center"">
<img src=""https://github.com/arplaboratory/learning_to_fly_media/blob/master/simulator_screenshot.png"" />
</div>",Container,2
https://raw.githubusercontent.com/arplaboratory/learning-to-fly/master/README.MD,"### Docker installation (isolated)
With the following instructions you can also easily build the Docker image yourself. If you want to run the code on bare metal jump [Native installation](#Native-installation).

First, install Docker on your machine. Then move to the original directory `learning_to_fly` and build the Docker image:
```
docker build -t arpllab/learning_to_fly .
```
If desired you can also build the container for building the firmware:
```
docker build -t arpllab/learning_to_fly_build_firmware -f Dockerfile_build_firmware .
```
After that you can run it using e.g.:
```
docker run -it --rm -p 8000:8000 arpllab/learning_to_fly
```
This will open the port `8000` for the UI of the training program and run it inside the container.

Navigate to [https://0.0.0.0:8000](https://0.0.0.0:8000) with your browser, and you should see something like in the screenshot above (after starting the training).

The training UI configuration does not log data by default. If you want to inspect the training data run:
```
docker run -it --rm -p 6006:6006 arpllab/learning_to_fly training_headless
```
Navigate to [https://0.0.0.0:6006](https://0.0.0.0:6006) with your browser to investigate the Tensorboard logs.

If you would like to benchmark the training speed you can use:
```
docker run -it --rm arpllab/learning_to_fly training_benchmark
```
This is the fastest configuration, without logging, UI, checkpointing etc.",container,2
https://raw.githubusercontent.com/arplaboratory/learning-to-fly/master/README.MD,"### Native installation
Clone this repository:
```
git clone https://github.com/arplaboratory/learning-to-fly learning_to_fly
cd learning_to_fly
```
Then instantiate the `RLtools` submodule:
```
git submodule update --init -- external/rl_tools
cd external/rl_tools
```

Then instantiate some dependencies of `RLtools` (for conveniences like checkpointing, Tensorboard logging, testing, etc.):
```
git submodule update --init -- external/cli11 external/highfive external/json/ external/tensorboard tests/lib/googletest/
```",source,0
https://raw.githubusercontent.com/LargeWorldModel/LWM/main/README.md,"## Setup

This codebase is supported on Ubuntu and has not been tested on Windows or macOS. We recommend using TPUs for training and inference, although it is also possible to use GPUs. On TPU, the code is highly optimized with Jax's Pallas and can achieve high MFUs with RingAttention at very large context sizes. On GPU, the code is based on XLA and is not as optimized as it is for TPU.

Install the requirements with:
```
conda create -n lwm python=3.10
pip install -U ""jax[cuda12_pip]==0.4.23"" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html
pip install -r requirements.txt
```
or set up TPU VM with:
```
sh tpu_vm_setup.sh
```",source,0
https://raw.githubusercontent.com/microsoft/UFO/main/README.md,"## ✨ Getting Started


### 🛠️ Step 1: Installation
UFO requires **Python >= 3.10** running on **Windows OS >= 10**. It can be installed by running the following command:
```bash
# [optional to create conda environment]
# conda create -n ufo python=3.10
# conda activate ufo

# clone the repository
git clone https://github.com/microsoft/UFO.git
cd UFO
# install the requirements
pip install -r requirements.txt
```

### ⚙️ Step 2: Configure the LLMs
Before running UFO, you need to provide your LLM configurations. You can configure `ufo/config/config.yaml` file as follows. 

#### OpenAI
```
API_TYPE: ""openai"" 
OPENAI_API_BASE: ""https://api.openai.com/v1/chat/completions"" # The base URL for the OpenAI API
OPENAI_API_KEY: ""YOUR_API_KEY""  # Set the value to the openai key for the llm model
OPENAI_API_MODEL: ""GPTV_MODEL_NAME""  # The only OpenAI model by now that accepts visual input
```

#### Azure OpenAI (AOAI)
```
API_TYPE: ""aoai"" 
OPENAI_API_BASE: ""YOUR_ENDPOINT"" # The AOAI API address. Format: https://{your-resource-name}.openai.azure.com/openai/deployments/{deployment-id}/chat/completions?api-version={api-version}
OPENAI_API_KEY: ""YOUR_API_KEY""  # Set the value to the openai key for the llm model
OPENAI_API_MODEL: ""GPTV_MODEL_NAME""  # The only OpenAI model by now that accepts visual input
```


### 🎉 Step 3: Start UFO

#### ⌨️ You can execute the following on your Windows command Line (CLI):

```bash
# assume you are in the cloned UFO folder
python -m ufo --task <your_task_name>
```

This will start the UFO process and you can interact with it through the command line interface. 
If everything goes well, you will see the following message:

```bash
Welcome to use UFO🛸, A UI-focused Agent for Windows OS Interaction.",source,0
https://raw.githubusercontent.com/catid/dora/main/README.md,"## Demo
Install conda: https://docs.conda.io/projects/miniconda/en/latest/index.html
```bash
git clone https://github.com/catid/dora.git
cd dora
conda create -n dora python=3.10 -y && conda activate dora
pip install -U -r requirements.txt
python dora.py",source,0
https://raw.githubusercontent.com/AILab-CVC/YOLO-World/master/README.md,"### 1. Installation
YOLO-World is developed based on `torch==1.11.0` `mmyolo==0.6.0` and `mmdetection==3.0.0`.
#### Clone Project 
```bash
git clone --recursive https://github.com/AILab-CVC/YOLO-World.git
```
#### Install
```bash
pip install torch wheel -q
pip install -e .
```",source,0
https://raw.githubusercontent.com/FasterDecoding/BitDelta/main/README.md,"## Install

1. Clone the repo and navigate to BitDelta:

```
git clone https://github.com/FasterDecoding/BitDelta
cd BitDelta
```
2. Set up environment:
```bash
conda create -yn bitdelta python=3.9
conda activate bitdelta
pip install -e .
```",source,0
https://raw.githubusercontent.com/tensorflow/tensorflow/master/README.md,See the [TensorFlow install guide](https://www.tensorflow.org/install) for the [pip package](https://www.tensorflow.org/install/pip),packagemanager,1
https://raw.githubusercontent.com/tensorflow/tensorflow/master/README.md,[Docker container](https://www.tensorflow.org/install/docker) ,docker,2
https://raw.githubusercontent.com/tensorflow/tensorflow/master/README.md,[build from source](https://www.tensorflow.org/install/source),source,0
https://raw.githubusercontent.com/tensorflow/tensorflow/master/README.md,"To install the current release, which includes support for
[CUDA-enabled GPU cards](https://www.tensorflow.org/install/gpu) *(Ubuntu and
Windows)*:
```
$ pip install tensorflow
```
Other devices (DirectX and MacOS-metal) are supported using
[Device plugins](https://www.tensorflow.org/install/gpu_plugins#available_devices).
A smaller CPU-only package is also available:
```
$ pip install tensorflow-cpu
```",packagemanager,1
https://raw.githubusercontent.com/tensorflow/tensorflow/master/README.md,*Nightly binaries are available for testing using the [tf-nightly](https://pypi.python.org/pypi/tf-nightly) and [tf-nightly-cpu](https://pypi.python.org/pypi/tf-nightly-cpu) packages on PyPi.*,binary,3
https://raw.githubusercontent.com/huggingface/transformers/main/README.md,"### With pip
This repository is tested on Python 3.8+, Flax 0.4.1+, PyTorch 1.11+, and TensorFlow 2.6+.
You should install 🤗 Transformers in a [virtual environment](https://docs.python.org/3/library/venv.html). If you're unfamiliar with Python virtual environments, check out the [user guide](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/).
First, create a virtual environment with the version of Python you're going to use and activate it.
Then, you will need to install at least one of Flax, PyTorch, or TensorFlow.
Please refer to [TensorFlow installation page](https://www.tensorflow.org/install/), [PyTorch installation page](https://pytorch.org/get-started/locally/#start-locally) and/or [Flax](https://github.com/google/flax#quick-install) and [Jax](https://github.com/google/jax#installation) installation pages regarding the specific installation command for your platform.
When one of those backends has been installed, 🤗 Transformers can be installed using pip as follows:
```bash
pip install transformers
```
If you'd like to play with the examples or need the bleeding edge of the code and can't wait for a new release, you must [install the library from source](https://huggingface.co/docs/transformers/installation#installing-from-source).
### With conda

🤗 Transformers can be installed using conda as follows:

```shell script
conda install conda-forge::transformers
```

> **_NOTE:_** Installing `transformers` from the `huggingface` channel is deprecated.

Follow the installation pages of Flax, PyTorch or TensorFlow to see how to install them with conda.",packagemanager,1
https://raw.githubusercontent.com/divelab/DIG/dig-stable/README.md,"## Installation
### Install from pip
The key dependencies of DIG: Dive into Graphs are PyTorch (>=1.10.0), PyTorch Geometric (>=2.0.0), and RDKit.
1. Install [PyTorch](https://pytorch.org/get-started/locally/) (>=1.10.0)
```shell script
$ python -c ""import torch; print(torch.__version__)""
>>> 1.10.0
```
2. Install [PyG](https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html#) (>=2.0.0)
```shell script
$ python -c ""import torch_geometric; print(torch_geometric.__version__)""
>>> 2.0.0
```
3. Install DIG: Dive into Graphs.
```shell script
pip install dive-into-graphs
```
After installation, you can check the version. You have successfully installed DIG: Dive into Graphs if no error occurs.
``` shell script
$ python
>>> from dig.version import __version__
>>> print(__version__)
```",packagemanager,1
https://raw.githubusercontent.com/divelab/DIG/dig-stable/README.md,"### Install from source
If you want to try the latest features that have not been released yet, you can install dig from source.
```shell script
git clone https://github.com/divelab/DIG.git
cd DIG
pip install .
```",source,0
https://gitlab.com/ungetym/blender-camera-generator/-/raw/main/README.md?ref_type=heads,"### Installation
1. Copy the `CamGen_v2` folder into the Blender [add-on folder](https://docs.blender.org/manual/en/latest/advanced/blender_directory_layout.html#platform-dependent-paths) that is right for your operating system, e.g. for Blender 4.0 under Linux ~/.config/blender/4.0/scripts/addons/
2. Open Blender and navigate to `Edit > Preferences > Add-ons`
3. Find and activate `Generic: Camera_Generator_v2` the list of available Add-ons. **You will need to press *refresh* in the Add-ons panel if you do not see the Camera_Generator option.**
4. [Optional] To enable experimental lens analysis operations and plotting of the results, additional packages have to be installed for Blender's bundled Python version.",source,0
https://raw.githubusercontent.com/facebookresearch/jepa/main/README.md,"### Setup
Create a new Conda environment, activate it, and run the [setup.py](setup.py) script:
`python setup.py install`",source,0
https://raw.githubusercontent.com/facebookresearch/DiT/main/README.md,"## Setup
First, download and set up the repo:
```bash
git clone https://github.com/facebookresearch/DiT.git
cd DiT
```
We provide an [`environment.yml`](environment.yml) file that can be used to create a Conda environment. If you only want 
to run pre-trained models locally on CPU, you can remove the `cudatoolkit` and `pytorch-cuda` requirements from the file.
```bash
conda env create -f environment.yml
conda activate DiT
```",source,0
https://raw.githubusercontent.com/XueyangFeng/ReHAC/main/README.md,"### Getting Start
You can use following scripts to install related python package through pip:
```
git clone https://github.com/XueyangFeng/ReHAC.git
cd ReHAC
pip install -r requirements.txt
```",source,0
https://raw.githubusercontent.com/langchain-ai/langchain/master/README.md,"## Quick Install
With pip:
```bash
pip install langchain
```
With conda:
```bash
conda install langchain -c conda-forge
```",packagemanager,1
https://raw.githubusercontent.com/ml-stat-Sustech/TorchCP/master/README.md,"## Installation
TorchCP is developed with Python 3.9 and PyTorch 2.0.1. To install TorchCP, simply run
```
pip install torchcp
```
To install from TestPyPI server, run
```
pip install --index-url https://test.pypi.org/simple/ --no-deps torchcp
```",packagemanager,1
https://raw.githubusercontent.com/NUS-HPC-AI-Lab/Neural-Network-Diffusion/main/README.md,"## Installation

1. Clone the repository:

```
git clone https://github.com/NUS-HPC-AI-Lab/Neural-Network-Diffusion.git
```

2. Create a new Conda environment and activate it: 

```
conda env create -f environment.yaml
conda activate pdiff
```

or install necessary package by:

```
pip install -r requirement.txt
```",source,0
https://raw.githubusercontent.com/MelosY/CAM/main/README.md,"## Setup

```bash
conda create -n cam python=3.8 -y
conda activate cam
pip install -r requirements.txt
```



## Run

1. Train

   ```bash
   bash script/train.sh
   ```

2. Evaluation

   - modify the  script/eval_tiny.sh ,change the ''path_to_pth'' to your own path, which is similar to eval_nano.sh and eval_base.sh

   ```bash
   bash script/eval_tiny.sh
   ```",source,0