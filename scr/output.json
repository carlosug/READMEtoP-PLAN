[
  {
    "id": "1",
    "name": "AAAI-DISIM-UnivAQ/DALI",
    "plans": [
      {
        "type": "Source",
        "steps": [
          {
            "text": "1.  To download and install SICStus Prolog (it is needed), follow the instructions at https://sicstus.sics.se/download4.html.",
            "note": "",
            "seq_order": 1,
            "is_optional": false,
            "commands": ""
          },
          {
            "text": "2.  Then, you can download DALI and test it by running an example DALI MAS:",
            "note": "&nbsp;&nbsp;&nbsp;&nbsp; You will see different windows opening: \n * &nbsp;&nbsp;&nbsp;&nbsp; Prolog LINDA server (active_server_wi.pl) \n * &nbsp;&nbsp;&nbsp;&nbsp; Prolog FIPA client (active_user_wi.pl) \n * &nbsp;&nbsp;&nbsp;&nbsp; 1 instance of DALI metaintepreter for each agent (active_dali_wi.pl), \n",
            "seq_order": 2,
            "is_optional": false,
            "commands": "```sh \n git clone https://github.com/AAAI-DISIM-UnivAQ/DALI.git \n cd DALI/Examples/advanced \n  bash startmas.sh \n```"
          }
        ],
        "technology": [
          "Linux"
        ]
      },
      {
        "type": "Source",
        "steps": [
          {
            "text": "1.  To download and install SICStus Prolog (it is needed), follow the instructions at https://sicstus.sics.se/download4.html.",
            "note": "Note NCBI may updated",
            "seq_order": 1,
            "is_optional": false,
            "commands": ""
          },
          {
            "text": "2.  Then, you can download DALI and test it by running an example DALI MAS:",
            "note": "",
            "seq_order": 2,
            "is_optional": false,
            "commands": ""
          },
          {
            "text": "3.  Unzip the repository, go to the folder DALI/Examples/basic, and test if DALI works by duble clicking startmas.bat file (this will launch an example DALI MAS).",
            "note": "&nbsp;&nbsp;&nbsp;&nbsp; You will see different windows opening: \n * &nbsp;&nbsp;&nbsp;&nbsp; Prolog LINDA server (active_server_wi.pl) \n * &nbsp;&nbsp;&nbsp;&nbsp; Prolog FIPA client (active_user_wi.pl) \n* &nbsp;&nbsp;&nbsp;&nbsp; 1 instance of DALI metaintepreter for each agent (active_dali_wi.pl)",
            "seq_order": 3,
            "is_optional": false,
            "commands": ""
          }
        ],
        "technology": [
          "Windows"
        ]
      }
    ],
    "readme_instructions": "## Installation **OS X & Linux:** 1. To download and install SICStus Prolog (it is needed), follow the instructions at https://sicstus.sics.se/download4.html. 2. Then, you can download DALI and test it by running an example DALI MAS: ```sh git clone https://github.com/AAAI-DISIM-UnivAQ/DALI.git cd DALI/Examples/advanced bash startmas.sh ``` &nbsp;&nbsp;&nbsp;&nbsp; You will see different windows opening: * &nbsp;&nbsp;&nbsp;&nbsp; Prolog LINDA server (active_server_wi.pl) * &nbsp;&nbsp;&nbsp;&nbsp; Prolog FIPA client (active_user_wi.pl) * &nbsp;&nbsp;&nbsp;&nbsp; 1 instance of DALI metaintepreter for each agent (active_dali_wi.pl) **Windows:** 1. To download and install SICStus Prolog (it is needed), follow the instructions at https://sicstus.sics.se/download4.html. 2. Then, you can download DALI from https://github.com/AAAI-DISIM-UnivAQ/DALI.git. 3. Unzip the repository, go to the folder 'DALI/Examples/basic', and test if DALI works by duble clicking 'startmas.bat' file (this will launch an example DALI MAS). \n\n &nbsp;&nbsp;&nbsp;&nbsp; You will see different windows opening: * &nbsp;&nbsp;&nbsp;&nbsp; Prolog LINDA server (active_server_wi.pl) * &nbsp;&nbsp;&nbsp;&nbsp; Prolog FIPA client (active_user_wi.pl) * &nbsp;&nbsp;&nbsp;&nbsp; 1 instance of DALI metaintepreter for each agent (active_dali_wi.pl)"
  },
  {
    "id": "2",
    "name": "BingqingCheng/cace",
    "plans": [
      {
        "type": "Source",
        "steps": [
          {
            "text": "Please refer to the `setup.py` file for installation instructions.",
            "note": "",
            "seq_order": 1,
            "is_optional": false,
            "commands": ""
          }
        ],
        "technology": [
          ""
        ]
      }
    ],
    "readme_instructions": "## Installation \n Please refer to the `setup.py` file for installation instructions."
  },
  {
    "id": "3",
    "name": "less-and-less-bugs/Trust_TELLER",
    "plans": [
      {
        "type": "Source",
        "steps": [
          {
            "text": "Step 1: Download the dataset folder from onedrive by [data.zip](https://portland-my.sharepoint.com/:u:/g/personal/liuhui3-c_my_cityu_edu_hk/EfApQlFP3PhFjUW4527STo0BALMdP16zs-HPMNgwQVFWsA?e=zoHlW2). Unzip this folder into the project  directory.",
            "note": "You can find four orginal datasets,  pre-processed datasets (i.e., val.jsonl, test.jsonl, train.jsonl in each dataset folder) and the files incuding questions and answers",
            "seq_order": 1,
            "is_optional": false,
            "commands": ""
          },
          {
            "text": "Step 2: Place you OpenAI key into the file named api_key.txt. ",
            "note": "",
            "seq_order": 2,
            "is_optional": false,
            "commands": "```\n openai.api_key = (``) ```"
          }
        ],
        "technology": [
          ""
        ]
      }
    ],
    "readme_instructions": "## Getting Started Step 1: Download the dataset folder from onedrive by [data.zip](https://portland-my.sharepoint.com/:u:/g/personal/liuhui3-c_my_cityu_edu_hk/EfApQlFP3PhFjUW4527STo0BALMdP16zs-HPMNgwQVFWsA?e=zoHlW2). Unzip this folder into the project directory. You can find four orginal datasets, pre-processed datasets (i.e., val.jsonl, test.jsonl, train.jsonl in each dataset folder) and the files incuding questions and answers Step 2: Place you OpenAI key into the file named api_key.txt. ``` openai.api_key = ``"
  },
  {
    "id": "4",
    "name": "utiasASRL/steam_icp",
    "plans": [
      {
        "type": "Container",
        "steps": [
          {
            "text": "Clone this repository and its submodules.",
            "note": "We use docker to install dependencies The recommended way to build the docker image is",
            "seq_order": 1,
            "is_optional": false,
            "commands": "```bash docker build -t steam_icp \n --build-arg USERID=$(id -u) \n --build-arg GROUPID=$(id -g) \n --build-arg USERNAME=$(whoami) \n --build-arg HOMEDIR=${HOME} . ```"
          },
          {
            "text": "When starting a container, remember to mount the code, dataset, and output directories to proper locations in the container:",
            "note": "An example command to start a docker container with the image is",
            "seq_order": 2,
            "is_optional": false,
            "commands": "```bash docker run -it --name steam_icp \n --privileged \n --network=host \n -e DISPLAY=$DISPLAY \n -v /tmp/.X11-unix:/tmp/.X11-unix \n -v ${HOME}:${HOME}:rw \n steam_icp ```"
          },
          {
            "text": "(Inside Container) Go to the root directory of this repository and build STEAM-ICP",
            "note": "",
            "seq_order": 3,
            "is_optional": false,
            "commands": "```bash bash build.sh"
          }
        ],
        "technology": [
          "Docker"
        ]
      }
    ],
    "readme_instructions": "## Installation Clone this repository and its submodules. We use docker to install dependencies The recommended way to build the docker image is ```bash docker build -t steam_icp \n --build-arg USERID=$(id -u) \n --build-arg GROUPID=$(id -g) \n --build-arg USERNAME=$(whoami) \n --build-arg HOMEDIR=${HOME} . ``` When starting a container, remember to mount the code, dataset, and output directories to proper locations in the container. An example command to start a docker container with the image is ```bash docker run -it --name steam_icp \n --privileged \n --network=host \n -e DISPLAY=$DISPLAY \n -v /tmp/.X11-unix:/tmp/.X11-unix \n -v ${HOME}:${HOME}:rw \n steam_icp ``` (Inside Container) Go to the root directory of this repository and build STEAM-ICP ```bash bash build.sh ```"
  },
  {
    "id": "5",
    "name": "PFischbeck/parameter-fitting-experiments",
    "plans": [
      {
        "type": "Source",
        "steps": [
          {
            "text": "- Make sure you have Python, Pip and R installed.",
            "note": "",
            "seq_order": 1,
            "is_optional": false,
            "commands": ""
          },
          {
            "text": "- Checkout this repository",
            "note": "",
            "seq_order": 2,
            "is_optional": false,
            "commands": ""
          },
          {
            "text": "- Install the python dependencies with",
            "note": "",
            "seq_order": 3,
            "is_optional": false,
            "commands": "```pip3 install -r requirements.txt```"
          },
          {
            "text": "- Install the `pygirgs` package at https://github.com/PFischbeck/pygirgs",
            "note": "",
            "seq_order": 4,
            "is_optional": false,
            "commands": ""
          },
          {
            "text": "- Install the R dependencies (used for plots) with",
            "note": "",
            "seq_order": 5,
            "is_optional": false,
            "commands": "```R -e 'install.packages(c(ggplot2, reshape2, plyr, dplyr, scales), repos=https://cloud.r-project.org/)```"
          },
          {
            "text": "- Download the file `konect-data.zip` from [Zenodo](https://doi.org/10.5281/zenodo.10629451) and extract its contents into the folder `input_data/konect`",
            "note": "",
            "seq_order": 6,
            "is_optional": false,
            "commands": ""
          },
          {
            "text": "- Optional: Download the file `output-data.zip` from [Zenodo](https://doi.org/10.5281/zenodo.10629451) and extract its contents into the folder `output_data`. This way, you can access all experiment results without running them yourself.",
            "note": "",
            "seq_order": 7,
            "is_optional": true,
            "commands": ""
          }
        ],
        "technology": [
          ""
        ]
      }
    ],
    "readme_instructions": "# Installation - Make sure you have Python, Pip and R installed. - Checkout this repository - Install the python dependencies with ``` pip3 install -r requirements.txt ``` - Install the `pygirgs` package at https://github.com/PFischbeck/pygirgs - Install the R dependencies (used for plots) with ``` R -e 'install.packages(c(ggplot2, reshape2, plyr, dplyr, scales), repos=https://cloud.r-project.org/)' ``` - Download the file `konect-data.zip` from [Zenodo](https://doi.org/10.5281/zenodo.10629451) and extract its contents into the folder `input_data/konect` - Optional: Download the file `output-data.zip` from [Zenodo](https://doi.org/10.5281/zenodo.10629451) and extract its contents into the folder `output_data`. This way, you can access all experiment results without running them yourself."
  },
  {
    "id": "6",
    "name": "gabbypinto/GET-Tok-Peru",
    "plans": [
      {
        "type": "Source",
        "steps": [
          {
            "text": "pip install -r requirements.txt",
            "note": "*Note: I did not us a virtual environment so the packages in the requirements.txt file are probably not reflective of all the packages used in this project. If some issues pop up please don't hesitate to email me at: gpinto@usc.edu*",
            "seq_order": 1,
            "is_optional": false,
            "commands": ""
          }
        ],
        "technology": [
          ""
        ]
      }
    ],
    "readme_instructions": "## Installation pip install -r requirements.txt *Note: I did not us a virtual environment so the packages in the requirements.txt file are probably not reflective of all the packages used in this project. If some issues pop up please don't hesitate to email me at: gpinto@usc.edu*"
  },
  {
    "id": "7",
    "name": "jonarriza96/gsft",
    "plans": [
      {
        "type": "Source",
        "steps": [
          {
            "text": "### Dependencies \n Initialize git submodules with",
            "note": "",
            "seq_order": 1,
            "is_optional": false,
            "commands": "```git submodule init \n git submodule update```"
          },
          {
            "text": "### Python environment \n Install the specific versions of every package from `requirements.txt` in a new conda environment:",
            "note": "",
            "seq_order": 2,
            "is_optional": false,
            "commands": "```conda create --name gsft python=3.9 \n conda activate gsft \npip install -r requirements.txt```"
          },
          {
            "text": "To ensure that Python paths are properly defined, update the `~/.bashrc` by adding the following lines",
            "note": "",
            "seq_order": 3,
            "is_optional": false,
            "commands": "```export GSFT_PATH=/path_to_gsfc \n export PYTHONPATH=$PYTHONPATH:/$GSFT_PATH```"
          }
        ],
        "technology": [
          ""
        ]
      }
    ],
    "readme_instructions": "## Installation ### Dependencies Initialize git submodules with ``` git submodule init git submodule update ``` ### Python environment Install the specific versions of every package from `requirements.txt` in a new conda environment: ``` conda create --name gsft python=3.9 conda activate gsft pip install -r requirements.txt ``` To ensure that Python paths are properly defined, update the `~/.bashrc` by adding the following lines ``` export GSFT_PATH=/path_to_gsfc export PYTHONPATH=$PYTHONPATH:/$GSFT_PATH ```"
  },
  {
    "id": "8",
    "name": "EricssonResearch/Line-Based-Room-Segmentation-and-EDF",
    "plans": [
      {
        "type": "Source",
        "steps": [
          {
            "text": "The project can be installed by running the following command in your terminal:",
            "note": "",
            "seq_order": 1,
            "is_optional": false,
            "commands": "```bash \n pip install -r requirements.txt```"
          }
        ],
        "technology": [
          ""
        ]
      }
    ],
    "readme_instructions": "## Installation The project can be installed by running the following command in your terminal: ```bash pip install -r requirements.txt ```"
  },
  {
    "id": "9",
    "name": "viralInformatics/VIGA",
    "plans": [
      {
        "type": "Source",
        "steps": [
          {
            "text": "Download VIGA \n Download VIGA with Git from GitHub",
            "note": "or Download ZIP to local",
            "seq_order": 1,
            "is_optional": false,
            "commands": "```git clone https://github.com/viralInformatics/VIGA.git``"
          },
          {
            "text": "1. download taxdmp.zip [Index of /pub/taxonomy (nih.gov)](https://ftp.ncbi.nlm.nih.gov/pub/taxonomy/) and unzip taxdmp.zip and put it in ./db/",
            "note": "",
            "seq_order": 2,
            "is_optional": false,
            "commands": ""
          },
          {
            "text": "2. download `prot.accession2taxid` file from https://ftp.ncbi.nlm.nih.gov/pub/taxonomy/accession2taxid/",
            "note": "",
            "seq_order": 3,
            "is_optional": false,
            "commands": ""
          },
          {
            "text": "3. download `RefSeqVirusProtein` file from",
            "note": "",
            "seq_order": 4,
            "is_optional": false,
            "commands": "wget -c ftp.ncbi.nlm.nih.gov/refseq/release/viral/viral.1.protein.faa.gz \n gzip -d viral.1.protein.faa.gz \n mv viral.1.protein.faa RefSeqVirusProtein"
          },
          {
            "text": "4. download `nr` file from",
            "note": "",
            "seq_order": 5,
            "is_optional": false,
            "commands": "wget -c ftp://ftp.ncbi.nlm.nih.gov/blast/db/FASTA/nr.gz\n or ascp -T  -i  asperaweb_id_dsa.openssh --host=ftp.ncbi.nih.gov --user=anonftp --mode=recv /blast/db/FASTA/nr.gz ./ \n gzip -d nr.gz"
          },
          {
            "text": "5. Use Diamond v2.0.11.149 to create two separate databases as the indexing libraries in the current version are incompatible with each other.",
            "note": "",
            "seq_order": 6,
            "is_optional": false,
            "commands": ""
          },
          {
            "text": "6. In order to set up a reference database for DIAMOND, the makedb command needs to be executed with the following command line:",
            "note": "",
            "seq_order": 7,
            "is_optional": false,
            "commands": "diamond makedb --in YourPath/RefSeqVirusProtein  -d Diamond_RefSeqVirusProtein --taxonmap YourPath/prot.accession2taxid --taxonnodes YourPath/nodes.dmp \n diamond makedb --in nr -d Dimond_nr --taxonmap YourPath/prot.accession2taxid --taxonnodes YourPath/nodes.dmp"
          },
          {
            "text": "Installing Some Software Using Conda",
            "note": "",
            "seq_order": 8,
            "is_optional": false,
            "commands": "conda install fastp=0.12.4 trinity=2.8.5 diamond=2.0.11.149 ragtag=2.1.0 quast=5.0.2"
          },
          {
            "text": "#### Manual Installation of MetaCompass",
            "note": "",
            "seq_order": 9,
            "is_optional": true,
            "commands": ""
          },
          {
            "text": "Python Dependencies \n Base on python 3.6.8",
            "note": "",
            "seq_order": 10,
            "is_optional": false,
            "commands": "```pip install pandas=1.1.5 numpy=1.19.5  matplotlib=3.3.4  biopython=1.79```"
          }
        ],
        "technology": [
          ""
        ]
      }
    ],
    "readme_instructions": "## Installation ### Step1: Download VIGA Download VIGA with Git from GitHub ``` git clone https://github.com/viralInformatics/VIGA.git ``` or Download ZIP to local ### Step 2: Download Database ``` 1. download taxdmp.zip [Index of /pub/taxonomy (nih.gov)](https://ftp.ncbi.nlm.nih.gov/pub/taxonomy/) and unzip taxdmp.zip and put it in ./db/ 2. download prot.accession2taxid file from https://ftp.ncbi.nlm.nih.gov/pub/taxonomy/accession2taxid/ 3. download RefSeqVirusProtein file from wget -c ftp.ncbi.nlm.nih.gov/refseq/release/viral/viral.1.protein.faa.gz gzip -d viral.1.protein.faa.gz mv viral.1.protein.faa RefSeqVirusProtein 4. download nr file from wget -c ftp://ftp.ncbi.nlm.nih.gov/blast/db/FASTA/nr.gz or ascp -T -i asperaweb_id_dsa.openssh --host=ftp.ncbi.nih.gov --user=anonftp --mode=recv /blast/db/FASTA/nr.gz ./ gzip -d nr.gz 5. Use Diamond v2.0.11.149 to create two separate databases as the indexing libraries in the current version are incompatible with each other. 6. In order to set up a reference database for DIAMOND, the makedb command needs to be executed with the following command line: diamond makedb --in YourPath/RefSeqVirusProtein -d Diamond_RefSeqVirusProtein --taxonmap YourPath/prot.accession2taxid --taxonnodes YourPath/nodes.dmp diamond makedb --in nr -d Dimond_nr --taxonmap YourPath/prot.accession2taxid --taxonnodes YourPath/nodes.dmp ``` ### Step 3: Installation of dependent software #### Installing Some Software Using Conda ``` conda install fastp=0.12.4 trinity=2.8.5 diamond=2.0.11.149 ragtag=2.1.0 quast=5.0.2 ``` #### Manual Installation of MetaCompass https://github.com/marbl/MetaCompass ### Step 4: Python Dependencies Base on python 3.6.8 ``` pip install pandas=1.1.5 numpy=1.19.5 matplotlib=3.3.4 biopython=1.79 ```"
  },
  {
    "id": "10",
    "name": "scimemia/NRN-EZ",
    "plans": [
      {
        "type": "Binary",
        "steps": [
          {
            "text": "Installation instructions for Linux (Ubuntu and Pop!_OS): download the Linux zip file and, from the command window, run a bash command for the install.sh file, in the corresponding installation folder.",
            "note": "",
            "seq_order": 1,
            "is_optional": false,
            "commands": ""
          }
        ],
        "technology": [
          "Linux"
        ]
      },
      {
        "type": "Binary",
        "steps": [
          {
            "text": "Installation instructions for Mac OS: download the Mac zip file and copy the NRN-EZ app to the Applications folder.",
            "note": "",
            "seq_order": 1,
            "is_optional": false,
            "commands": ""
          }
        ],
        "technology": [
          "Mac"
        ]
      },
      {
        "type": "Binary",
        "steps": [
          {
            "text": "Installation instructions for Windows: download the Win zip file and run the installation wizard.",
            "note": "",
            "seq_order": 1,
            "is_optional": false,
            "commands": ""
          }
        ],
        "technology": [
          "Windows"
        ]
      }
    ],
    "readme_instructions": "NRN-EZ was built with PyInstaller 3.6, and requires the following languages and libraries: â€¢ Python 3.6.9 and higher (currently up to 3.10) â€¢ PyQt 5.10.1 â€¢ PyQtGraph 0.11.0 Installation instructions for Linux (Ubuntu and Pop!_OS): download the Linux zip file and, from the command window, run a bash command for the install.sh file, in the corresponding installation folder. Installation instructions for Mac OS: download the Mac zip file and copy the NRN-EZ app to the Applications folder. Installation instructions for Windows: download the Win zip file and run the installation wizard."
  },
  {
    "id": "11",
    "name": "uclaml/SPIN",
    "plans": [
      {
        "type": "Source",
        "steps": [
          {
            "text": "1. Create a Python virtual environment with Conda:",
            "note": "",
            "seq_order": 1,
            "is_optional": false,
            "commands": "```conda create -n myenv python=3.10 \n conda activate myenv```"
          },
          {
            "text": "2. Install PyTorch `v2.1.0` with compatible cuda version, following instructions from [PyTorch Installation Page](https://pytorch.org/get-started/locally/). For example with cuda 11:",
            "note": "",
            "seq_order": 2,
            "is_optional": false,
            "commands": "````pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu118```"
          },
          {
            "text": "3. Install the following Python dependencies to run the codes.",
            "note": "",
            "seq_order": 3,
            "is_optional": false,
            "commands": "```python -m pip install . \n python -m pip install flash-attn --no-build-isolation```"
          },
          {
            "text": "4. Login to your huggingface account for downloading models",
            "note": "",
            "seq_order": 4,
            "is_optional": false,
            "commands": "```huggingface-cli login --token ${your_access_token}```"
          }
        ],
        "technology": [
          ""
        ]
      }
    ],
    "readme_instructions": "## Setup The following steps provide the necessary setup to run our codes. 1. Create a Python virtual environment with Conda: ``` conda create -n myenv python=3.10 conda activate myenv ``` 2. Install PyTorch `v2.1.0` with compatible cuda version, following instructions from [PyTorch Installation Page](https://pytorch.org/get-started/locally/). For example with cuda 11: ``` pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu118 ``` 3. Install the following Python dependencies to run the codes. ``` python -m pip install . python -m pip install flash-attn --no-build-isolation ``` 4. Login to your huggingface account for downloading models ``` huggingface-cli login --token ${your_access_token} ```"
  },
  {
    "id": "12",
    "name": "ncbi/GeneGPT",
    "plans": [
      {
        "type": "Source",
        "steps": [
          {
            "text": "Please first install the required packages by:",
            "note": "The code has been tested with Python 3.9.13.",
            "seq_order": 1,
            "is_optional": false,
            "commands": "```bash pip install -r requirements.txt```"
          },
          {
            "text": "Replace the placeholder with your key in `config.py`:",
            "note": "You also need an OpenAI API key to run GeneGPT with Codex.",
            "seq_order": 2,
            "is_optional": false,
            "commands": "```cat config.py \n API_KEY = 'YOUR_OPENAI_API_KEY'```"
          }
        ],
        "technology": [
          ""
        ]
      }
    ],
    "readme_instructions": "## Requirements The code has been tested with Python 3.9.13. Please first install the required packages by: ```bash pip install -r requirements.txt ``` You also need an OpenAI API key to run GeneGPT with Codex. Replace the placeholder with your key in `config.py`: ```bash $ cat config.py API_KEY = 'YOUR_OPENAI_API_KEY' ```"
  },
  {
    "id": "13",
    "name": "arplaboratory/learning-to-fly",
    "plans": [
      {
        "type": "Container",
        "steps": [
          {
            "text": "First, install Docker on your machine. Then move to the original directory `learning_to_fly` and build the Docker image:",
            "note": "",
            "seq_order": 1,
            "is_optional": false,
            "commands": "```docker build -t arpllab/learning_to_fly .```"
          },
          {
            "text": "If desired you can also build the container for building the firmware:",
            "note": "",
            "seq_order": 2,
            "is_optional": true,
            "commands": "```docker build -t arpllab/learning_to_fly_build_firmware -f Dockerfile_build_firmware .```"
          },
          {
            "text": "After that you can run it using e.g.:",
            "note": "This will open the port `8000` for the UI of the training program and run it inside the container.",
            "seq_order": 3,
            "is_optional": false,
            "commands": "```docker run -it --rm -p 8000:8000 arpllab/learning_to_fly```"
          },
          {
            "text": "Navigate to [https://0.0.0.0:8000](https://0.0.0.0:8000) with your browser, and you should see something like in the screenshot above (after starting the training).",
            "note": "The training UI configuration does not log data by default. If you want to inspect the training data run: ```docker run -it --rm -p 6006:6006 arpllab/learning_to_fly training_headless```",
            "seq_order": 4,
            "is_optional": false,
            "commands": ""
          },
          {
            "text": "Navigate to [https://0.0.0.0:6006](https://0.0.0.0:6006) with your browser to investigate the Tensorboard logs.",
            "note": "If you would like to benchmark the training speed you can use: ```docker run -it --rm arpllab/learning_to_fly training_benchmark``` \n This is the fastest configuration, without logging, UI, checkpointing etc.",
            "seq_order": 5,
            "is_optional": false,
            "commands": "```docker run -it --rm -p 8000:8000 arpllab/learning_to_fly```"
          }
        ],
        "technology": [
          "Docker"
        ]
      },
      {
        "type": "Source",
        "steps": [
          {
            "text": "Clone this repository:",
            "note": "",
            "seq_order": 1,
            "is_optional": false,
            "commands": "```git clone https://github.com/arplaboratory/learning-to-fly learning_to_fly \n cd learning_to_fly```"
          },
          {
            "text": "Then instantiate the `RLtools` submodule:",
            "note": "",
            "seq_order": 2,
            "is_optional": false,
            "commands": "```git submodule update --init -- external/rl_tools \n cd external/rl_tools```"
          },
          {
            "text": "Then instantiate some dependencies of `RLtools` (for conveniences like checkpointing, Tensorboard logging, testing, etc.):",
            "note": "",
            "seq_order": 3,
            "is_optional": false,
            "commands": "```git submodule update --init -- external/cli11 external/highfive external/json/ external/tensorboard tests/lib/googletest/```"
          },
          {
            "text": "#### Install dependencies on Ubuntu",
            "note": "As an alternative to openblas you can also install [Intel MKL](https://www.intel.com/content/www/us/en/developer/tools/oneapi/onemkl-download.html) which in our experience is significantly faster than OpenBLAS.",
            "seq_order": 4,
            "is_optional": false,
            "commands": "```sudo apt update && sudo apt install libhdf5-dev libopenblas-dev protobuf-compiler libprotobuf-dev libboost-all-dev```"
          }
        ],
        "technology": [
          "Linux"
        ]
      },
      {
        "type": "Source",
        "steps": [
          {
            "text": "#### Install dependencies on macOS",
            "note": "Please make sure that `brew` links the libraries correctly. If not you might have to link e.g. `protobuf` manually using `brew link protobuf`.",
            "seq_order": 1,
            "is_optional": false,
            "commands": "```brew install hdf5 protobuf boost```"
          },
          {
            "text": "Going back to the main directory (`learning_to_fly`), we can now configure the build of the code:",
            "note": "- Ubuntu + OpenBLAS: `cmake .. -DCMAKE_BUILD_TYPE=Release -DRL_TOOLS_BACKEND_ENABLE_OPENBLAS:BOOL=ON` \n - Ubuntu + MKL: `cmake .. -DCMAKE_BUILD_TYPE=Release -DRL_TOOLS_BACKEND_ENABLE_MKL:BOOL=ON` \n - macOS (tested on Sonoma): `cmake .. -DCMAKE_BUILD_TYPE=Release`",
            "seq_order": 2,
            "is_optional": false,
            "commands": "cd ../../ \n mkdir build \n cd build"
          },
          {
            "text": "Finally, we can build the targets:",
            "note": "",
            "seq_order": 3,
            "is_optional": false,
            "commands": "cmake --build . -j8"
          },
          {
            "text": "After successfully building the targets, we can run the code (in the original directory `learning_to_fly`):",
            "note": "While this is running, you should be able to see training metrics using Tensorboard",
            "seq_order": 4,
            "is_optional": false,
            "commands": "cd .. \n ./build/src/training_headless"
          },
          {
            "text": "If not already installed:",
            "note": "",
            "seq_order": 5,
            "is_optional": true,
            "commands": "```python3 -m pip install tensorboard```"
          },
          {
            "text": "Then from the original directory `learning_to_fly`:",
            "note": "",
            "seq_order": 6,
            "is_optional": true,
            "commands": "tensorboard --logdir=logs"
          },
          {
            "text": "To run the training with the UI, we download the JavaScript dependencies in the form of the two files `three.module.js` and `OrbitControls.js`:",
            "note": "",
            "seq_order": 7,
            "is_optional": false,
            "commands": "cd src/ui \n ./get_dependencies.sh"
          },
          {
            "text": "After that we can execute the UI binary from the root folder:",
            "note": "",
            "seq_order": 8,
            "is_optional": false,
            "commands": "```cd ../.. \n ./build/src/ui 0.0.0.0 8000```"
          },
          {
            "text": "Now you should be able to navigate to [http://0.0.0.0:8000](http://0.0.0.0:8000) in your browser and start the training.",
            "note": "To run the benchmark (with UI, checkpointing and Tensorboard logging turned off):",
            "seq_order": 9,
            "is_optional": false,
            "commands": "```sudo nice -n -20 ./build/src/training_benchmark```"
          }
        ],
        "technology": [
          "Mac"
        ]
      }
    ],
    "readme_instructions": "### Docker installation (isolated) With the following instructions you can also easily build the Docker image yourself. If you want to run the code on bare metal jump [Native installation](#Native-installation). First, install Docker on your machine. Then move to the original directory `learning_to_fly` and build the Docker image: ``` docker build -t arpllab/learning_to_fly . ``` If desired you can also build the container for building the firmware: ``` docker build -t arpllab/learning_to_fly_build_firmware -f Dockerfile_build_firmware . ``` After that you can run it using e.g.: ``` docker run -it --rm -p 8000:8000 arpllab/learning_to_fly ``` This will open the port `8000` for the UI of the training program and run it inside the container. Navigate to [https://0.0.0.0:8000](https://0.0.0.0:8000) with your browser, and you should see something like in the screenshot above (after starting the training). The training UI configuration does not log data by default. If you want to inspect the training data run: ``` docker run -it --rm -p 6006:6006 arpllab/learning_to_fly training_headless ``` Navigate to [https://0.0.0.0:6006](https://0.0.0.0:6006) with your browser to investigate the Tensorboard logs. If you would like to benchmark the training speed you can use: ``` docker run -it --rm arpllab/learning_to_fly training_benchmark ``` This is the fastest configuration, without logging, UI, checkpointing etc. ### Native installation Clone this repository: ``` git clone https://github.com/arplaboratory/learning-to-fly learning_to_fly cd learning_to_fly ``` Then instantiate the `RLtools` submodule: ``` git submodule update --init -- external/rl_tools cd external/rl_tools ``` Then instantiate some dependencies of `RLtools` (for conveniences like checkpointing, Tensorboard logging, testing, etc.): ``` git submodule update --init -- external/cli11 external/highfive external/json/ external/tensorboard tests/lib/googletest/ ``` #### Install dependencies on Ubuntu ``` sudo apt update && sudo apt install libhdf5-dev libopenblas-dev protobuf-compiler libprotobuf-dev libboost-all-dev ``` As an alternative to openblas you can also install [Intel MKL](https://www.intel.com/content/www/us/en/developer/tools/oneapi/onemkl-download.html) which in our experience is significantly faster than OpenBLAS. #### Install dependencies on macOS ``` brew install hdf5 protobuf boost ``` Please make sure that `brew` links the libraries correctly. If not you might have to link e.g. `protobuf` manually using `brew link protobuf`. Going back to the main directory (`learning_to_fly`), we can now configure the build of the code: ``` cd ../../ mkdir build cd build ``` - Ubuntu + OpenBLAS: `cmake .. -DCMAKE_BUILD_TYPE=Release -DRL_TOOLS_BACKEND_ENABLE_OPENBLAS:BOOL=ON` - Ubuntu + MKL: `cmake .. -DCMAKE_BUILD_TYPE=Release -DRL_TOOLS_BACKEND_ENABLE_MKL:BOOL=ON` - macOS (tested on Sonoma): `cmake .. -DCMAKE_BUILD_TYPE=Release` Finally, we can build the targets: ``` cmake --build . -j8 ``` After successfully building the targets, we can run the code (in the original directory `learning_to_fly`): ``` cd .. ./build/src/training_headless ``` While this is running, you should be able to see training metrics using Tensorboard If not already installed: ``` python3 -m pip install tensorboard ``` Then from the original directory `learning_to_fly`: ``` tensorboard --logdir=logs ``` To run the training with the UI, we download the JavaScript dependencies in the form of the two files `three.module.js` and `OrbitControls.js`: ``` cd src/ui ./get_dependencies.sh ``` After that we can execute the UI binary from the root folder: ``` cd ../../ ./build/src/ui 0.0.0.0 8000 ``` Now you should be able to navigate to [http://0.0.0.0:8000](http://0.0.0.0:8000) in your browser and start the training. To run the benchmark (with UI, checkpointing and Tensorboard logging turned off): ``` sudo nice -n -20 ./build/src/training_benchmark ```"
  },
  {
    "id": "14",
    "name": "LargeWorldModel/LWM",
    "plans": [
      {
        "type": "Source",
        "steps": [
          {
            "text": "Install the requirements with:",
            "note": "",
            "seq_order": 1,
            "is_optional": false,
            "commands": "```conda create -n lwm python=3.10 \n pip install -U j`ax[cuda12_pip]==0.4.23` -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html \n pip install -r requirements.txt```"
          },
          {
            "text": "or set up TPU VM with:",
            "note": "",
            "seq_order": 2,
            "is_optional": true,
            "commands": "```sh tpu_vm_setup.sh```"
          }
        ],
        "technology": [
          ""
        ]
      }
    ],
    "readme_instructions": "## Setup This codebase is supported on Ubuntu and has not been tested on Windows or macOS. We recommend using TPUs for training and inference, although it is also possible to use GPUs. On TPU, the code is highly optimized with Jax's Pallas and can achieve high MFUs with RingAttention at very large context sizes. On GPU, the code is based on XLA and is not as optimized as it is for TPU. Install the requirements with: ``` conda create -n lwm python=3.10 pip install -U `jax[cuda12_pip]==0.4.23` -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html pip install -r requirements.txt ``` or set up TPU VM with: ``` sh tpu_vm_setup.sh ```"
  },
  {
    "id": "15",
    "name": "microsoft/UFO",
    "plans": [
      {
        "type": "Source",
        "steps": [
          {
            "text": "### ðŸ› ï¸ Step 1: Installation",
            "note": "UFO requires **Python >= 3.10** running on **Windows OS >= 10**. It can be installed by running the following command:",
            "seq_order": 1,
            "is_optional": false,
            "commands": "```bash # [optional to create conda environment] # conda create -n ufo python=3.10 # conda activate ufo # clone the repository git clone https://github.com/microsoft/UFO.git cd UFO # install the requirements pip install -r requirements.txt ```"
          }
        ],
        "technology": [
          ""
        ]
      }
    ],
    "readme_instructions": "## âœ¨ Getting Started ### ðŸ› ï¸ Step 1: Installation UFO requires **Python >= 3.10** running on **Windows OS >= 10**. It can be installed by running the following command: ```bash # [optional to create conda environment] # conda create -n ufo python=3.10 # conda activate ufo # clone the repository git clone https://github.com/microsoft/UFO.git cd UFO # install the requirements pip install -r requirements.txt ```"
  },
  {
    "id": "16",
    "name": "YOLO-World",
    "plans": [
      {
        "type": "Source",
        "steps": [
          {
            "text": "#### Clone Project",
            "note": "YOLO-World is developed based on `torch==1.11.0` `mmyolo==0.6.0` and `mmdetection==3.0.0`.",
            "seq_order": 1,
            "is_optional": false,
            "commands": "```git clone --recursive https://github.com/AILab-CVC/YOLO-World.git```"
          },
          {
            "text": "#### Install",
            "note": "",
            "seq_order": 2,
            "is_optional": false,
            "commands": "```bash pip install torch wheel -q \n pip install -e .```"
          }
        ],
        "technology": [
          ""
        ]
      }
    ],
    "readme_instructions": "### 1. Installation YOLO-World is developed based on `torch==1.11.0` `mmyolo==0.6.0` and `mmdetection==3.0.0`. #### Clone Project ```bash git clone --recursive https://github.com/AILab-CVC/YOLO-World.git ``` #### Install ```bash pip install torch wheel -q pip install -e . ```"
  },
  {
    "id": "17",
    "name": "tensorflow",
    "plans": [
      {
        "type": "Packagemanager",
        "steps": [
          {
            "text": " To install the current release, which includes support for [CUDA-enabled GPU cards](https://www.tensorflow.org/install/gpu) *(Ubuntu and Windows)*:",
            "note": "",
            "seq_order": 1,
            "is_optional": false,
            "commands": "```$ pip install tensorflow```"
          },
          {
            "text": "A smaller CPU-only package is also available:",
            "note": "To update TensorFlow to the latest version, add `--upgrade` flag to the above commands.",
            "seq_order": 2,
            "is_optional": true,
            "commands": "```$ pip install tensorflow-cpu```"
          }
        ],
        "technology": [
          "Linux",
          "Windows"
        ]
      },
      {
        "type": "Packagemanager",
        "steps": [
          {
            "text": " Other devices (DirectX and MacOS-metal) are supported using [Device plugins](https://www.tensorflow.org/install/gpu_plugins#available_devices).",
            "note": "",
            "seq_order": 1,
            "is_optional": false,
            "commands": ""
          }
        ],
        "technology": [
          "Mac"
        ]
      },
      {
        "type": "Binary",
        "steps": [
          {
            "text": "*Nightly binaries are available for testing using the [tf-nightly](https://pypi.python.org/pypi/tf-nightly) and [tf-nightly-cpu](https://pypi.python.org/pypi/tf-nightly-cpu) packages on PyPi.*",
            "note": "",
            "seq_order": 1,
            "is_optional": false,
            "commands": ""
          }
        ],
        "technology": [
          ""
        ]
      }
    ],
    "readme_instructions": "## Install See the [TensorFlow install guide](https://www.tensorflow.org/install) for the [pip package](https://www.tensorflow.org/install/pip), to [enable GPU support](https://www.tensorflow.org/install/gpu), use a [Docker container](https://www.tensorflow.org/install/docker), and [build from source](https://www.tensorflow.org/install/source). To install the current release, which includes support for [CUDA-enabled GPU cards](https://www.tensorflow.org/install/gpu) *(Ubuntu and Windows)*: ``` $ pip install tensorflow ``` Other devices (DirectX and MacOS-metal) are supported using [Device plugins](https://www.tensorflow.org/install/gpu_plugins#available_devices). A smaller CPU-only package is also available: ``` $ pip install tensorflow-cpu ``` To update TensorFlow to the latest version, add `--upgrade` flag to the above commands. *Nightly binaries are available for testing using the [tf-nightly](https://pypi.python.org/pypi/tf-nightly) and [tf-nightly-cpu](https://pypi.python.org/pypi/tf-nightly-cpu) packages on PyPi.*"
  },
  {
    "id": "18",
    "name": "transformers",
    "plans": [
      {
        "type": "Packagemanager",
        "steps": [
          {
            "text": "First, create a virtual environment with the version of Python you're going to use and activate it.",
            "note": "This repository is tested on Python 3.8+, Flax 0.4.1+, PyTorch 1.11+, and TensorFlow 2.6+. You should install ðŸ¤— Transformers in a [virtual environment](https://docs.python.org/3/library/venv.html). If you're unfamiliar with Python virtual environments, check out the [user guide](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/).",
            "seq_order": 1,
            "is_optional": false,
            "commands": ""
          },
          {
            "text": "Then, you will need to install at least one of Flax, PyTorch, or TensorFlow.",
            "note": "Please refer to [TensorFlow installation page](https://www.tensorflow.org/install/), [PyTorch installation page](https://pytorch.org/get-started/locally/#start-locally) and/or [Flax](https://github.com/google/flax#quick-install) and [Jax](https://github.com/google/jax#installation) installation pages regarding the specific installation command for your platform.",
            "seq_order": 2,
            "is_optional": false,
            "commands": ""
          },
          {
            "text": "When one of those backends has been installed, ðŸ¤— Transformers can be installed using pip as follows:",
            "note": "",
            "seq_order": 3,
            "is_optional": false,
            "commands": "``pip install transformers``"
          },
          {
            "text": "If you'd like to play with the examples or need the bleeding edge of the code and can't wait for a new release, you must [install the library from source](https://huggingface.co/docs/transformers/installation#installing-from-source).",
            "note": "",
            "seq_order": 4,
            "is_optional": true,
            "commands": ""
          }
        ],
        "technology": [
          "pip"
        ]
      },
      {
        "type": "Packagemanager",
        "steps": [
          {
            "text": "ðŸ¤— Transformers can be installed using conda as follows:",
            "note": "> **_NOTE:_** Installing `transformers` from the `huggingface` channel is deprecated.",
            "seq_order": 1,
            "is_optional": false,
            "commands": "```conda install conda-forge::transformers```"
          },
          {
            "text": "Follow the installation pages of Flax, PyTorch or TensorFlow to see how to install them with conda.",
            "note": "> **_NOTE:_**  On Windows, you may be prompted to activate Developer Mode in order to benefit from caching. If this is not an option for you, please let us know in [this issue](https://github.com/huggingface/huggingface_hub/issues/1062).",
            "seq_order": 2,
            "is_optional": false,
            "commands": ""
          }
        ],
        "technology": [
          "conda"
        ]
      }
    ],
    "readme_instructions": "## Installation ### With pip This repository is tested on Python 3.8+, Flax 0.4.1+, PyTorch 1.11+, and TensorFlow 2.6+. You should install ðŸ¤— Transformers in a [virtual environment](https://docs.python.org/3/library/venv.html). If you're unfamiliar with Python virtual environments, check out the [user guide](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/). First, create a virtual environment with the version of Python you're going to use and activate it. Then, you will need to install at least one of Flax, PyTorch, or TensorFlow. Please refer to [TensorFlow installation page](https://www.tensorflow.org/install/), [PyTorch installation page](https://pytorch.org/get-started/locally/#start-locally) and/or [Flax](https://github.com/google/flax#quick-install) and [Jax](https://github.com/google/jax#installation) installation pages regarding the specific installation command for your platform. When one of those backends has been installed, ðŸ¤— Transformers can be installed using pip as follows: ```bash pip install transformers ``` If you'd like to play with the examples or need the bleeding edge of the code and can't wait for a new release, you must [install the library from source](https://huggingface.co/docs/transformers/installation#installing-from-source). ### With conda ðŸ¤— Transformers can be installed using conda as follows: ```shell script conda install conda-forge::transformers ``` > **_NOTE:_** Installing `transformers` from the `huggingface` channel is deprecated. Follow the installation pages of Flax, PyTorch or TensorFlow to see how to install them with conda. > **_NOTE:_** On Windows, you may be prompted to activate Developer Mode in order to benefit from caching. If this is not an option for you, please let us know in [this issue](https://github.com/huggingface/huggingface_hub/issues/1062)."
  },
  {
    "id": "19",
    "name": "divelab/DIG",
    "plans": [
      {
        "type": "Packagemanager",
        "steps": [
          {
            "text": "1. Install [PyTorch](https://pytorch.org/get-started/locally/) (>=1.10.0)",
            "note": "The key dependencies of DIG: Dive into Graphs are PyTorch (>=1.10.0), PyTorch Geometric (>=2.0.0), and RDKit.",
            "seq_order": 1,
            "is_optional": false,
            "commands": "```shell script $ python -c `import torch; print(torch.__version__)` >>> 1.10.0 ```"
          },
          {
            "text": "2. Install [PyG](https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html#) (>=2.0.0)",
            "note": "",
            "seq_order": 2,
            "is_optional": false,
            "commands": "```shell script $ python -c `import torch_geometric; print(torch_geometric.__version__)` >>> 2.0.0 ```"
          },
          {
            "text": "3. Install DIG: Dive into Graphs.",
            "note": "After installation, you can check the version. You have successfully installed DIG: Dive into Graphs if no error occurs. ``` shell script $ python >>> from dig.version import __version__ >>> print(__version__) ```",
            "seq_order": 3,
            "is_optional": false,
            "commands": "```shell script pip install dive-into-graphs```"
          }
        ],
        "technology": [
          "pip"
        ]
      },
      {
        "type": "Source",
        "steps": [
          {
            "text": "If you want to try the latest features that have not been released yet, you can install dig from source.",
            "note": "",
            "seq_order": 1,
            "is_optional": false,
            "commands": "```shell script git clone https://github.com/divelab/DIG.git cd DIG pip install . ```"
          }
        ],
        "technology": [
          ""
        ]
      }
    ],
    "readme_instructions": "# Installation ### Install from pip The key dependencies of DIG: Dive into Graphs are PyTorch (>=1.10.0), PyTorch Geometric (>=2.0.0), and RDKit. 1. Install [PyTorch](https://pytorch.org/get-started/locally/) (>=1.10.0) ```shell script $ python -c `import torch`; print(torch.__version__)` >>> 1.10.0 ``` 2. Install [PyG](https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html#) (>=2.0.0) ```shell script $ python -c `import torch_geometric; print(torch_geometric.__version__)` >>> 2.0.0 ``` 3. Install DIG: Dive into Graphs. ```shell script pip install dive-into-graphs ``` After installation, you can check the version. You have successfully installed DIG: Dive into Graphs if no error occurs. ``` shell script $ python >>> from dig.version import __version__ >>> print(__version__) ``` ### Install from source If you want to try the latest features that have not been released yet, you can install dig from source. ```shell script git clone https://github.com/divelab/DIG.git cd DIG pip install . ```"
  },
  {
    "id": "20",
    "name": "langchain-ai/langchain",
    "plans": [
      {
        "type": "Source",
        "steps": [
          {
            "text": "With pip:",
            "note": "",
            "seq_order": 1,
            "is_optional": false,
            "commands": "```bash pip install langchain```"
          }
        ],
        "technology": [
          "pip"
        ]
      },
      {
        "type": "Source",
        "steps": [
          {
            "text": "With pip:",
            "note": "",
            "seq_order": 1,
            "is_optional": false,
            "commands": "```bash conda install langchain -c conda-forge```"
          }
        ],
        "technology": [
          "conda"
        ]
      }
    ],
    "readme_instructions": "## Quick Install With pip: ```bash pip install langchain ``` With conda: ```bash conda install langchain -c conda-forge ``` "
  },
  {
    "id": "21",
    "name": "ungetym/blender-camera-generator",
    "plans": [
      {
        "type": "Source",
        "steps": [
          {
            "text": "1. Copy the `CamGen_v2` folder into the Blender [add-on folder](https://docs.blender.org/manual/en/latest/advanced/blender_directory_layout.html#platform-dependent-paths) that is right for your operating system, e.g. for Blender 4.0 under Linux ~/.config/blender/4.0/scripts/addons/",
            "note": "",
            "seq_order": 1,
            "is_optional": false,
            "commands": ""
          },
          {
            "text": "2. Open Blender and navigate to `Edit > Preferences > Add-ons`",
            "note": "",
            "seq_order": 2,
            "is_optional": false,
            "commands": ""
          },
          {
            "text": "3. Find and activate `Generic: Camera_Generator_v2` the list of available Add-ons. **You will need to press *refresh* in the Add-ons panel if you do not see the Camera_Generator option.**",
            "note": "",
            "seq_order": 3,
            "is_optional": false,
            "commands": ""
          },
          {
            "text": "4. [Optional] To enable experimental lens analysis operations and plotting of the results, additional packages have to be installed for Blender's bundled Python version.",
            "note": "",
            "seq_order": 4,
            "is_optional": true,
            "commands": "'`$BLENDERPATH/$VERSION/python/bin/python3.10 -m pip install matplotlib PyQt5`'"
          }
        ],
        "technology": [
          ""
        ]
      }
    ],
    "readme_instructions": "### Installation 1. Copy the `CamGen_v2` folder into the Blender [add-on folder](https://docs.blender.org/manual/en/latest/advanced/blender_directory_layout.html#platform-dependent-paths) that is right for your operating system, e.g. for Blender 4.0 under Linux ~/.config/blender/4.0/scripts/addons/ 2. Open Blender and navigate to `Edit > Preferences > Add-ons` 3. Find and activate `Generic: Camera_Generator_v2` the list of available Add-ons. **You will need to press *refresh* in the Add-ons panel if you do not see the Camera_Generator option.** 4. [Optional] To enable experimental lens analysis operations and plotting of the results, additional packages have to be installed for Blender's bundled Python version. '`$BLENDERPATH/$VERSION/python/bin/python3.10 -m pip install matplotlib PyQt5."
  },
  {
    "id": "22",
    "name": "facebookresearch/DiT",
    "plans": [
      {
        "type": "Source",
        "steps": [
          {
            "text": "First, download and set up the repo:",
            "note": "We provide an [`environment.yml`](environment.yml) file that can be used to create a Conda environment.",
            "seq_order": 1,
            "is_optional": false,
            "commands": "````bash git clone https://github.com/facebookresearch/DiT.git cd DiT ```"
          },
          {
            "text": "if you only want to run pre-trained models locally on CPU, you can remove the `cudatoolkit` and `pytorch-cuda` requirements from the file.",
            "note": "",
            "seq_order": 2,
            "is_optional": true,
            "commands": "```bash conda env create -f environment.yml \n conda activate DiT```"
          }
        ],
        "technology": [
          ""
        ]
      }
    ],
    "readme_instructions": "## Setup First, download and set up the repo: ```bash git clone https://github.com/facebookresearch/DiT.git cd DiT ``` We provide an [`environment.yml`](environment.yml) file that can be used to create a Conda environment. If you only want to run pre-trained models locally on CPU, you can remove the `cudatoolkit` and `pytorch-cuda` requirements from the file. ```bash conda env create -f environment.yml conda activate DiT ```"
  },
  {
    "id": "23",
    "name": "ml-stat-Sustech/TorchCP",
    "plans": [
      {
        "type": "Packagemanager",
        "steps": [
          {
            "text": "To install TorchCP, simply run",
            "note": "TorchCP is developed with Python 3.9 and PyTorch 2.0.1.",
            "seq_order": 1,
            "is_optional": false,
            "commands": "```pip install torchcp```"
          }
        ],
        "technology": [
          "pip"
        ]
      },
      {
        "type": "Packagemanager",
        "steps": [
          {
            "text": "To install from TestPyPI server, run",
            "note": "",
            "seq_order": 2,
            "is_optional": false,
            "commands": "```pip install --index-url https://test.pypi.org/simple/ --no-deps torchcp```"
          }
        ],
        "technology": [
          "TestPyPI server"
        ]
      }
    ],
    "readme_instructions": "## Installation TorchCP is developed with Python 3.9 and PyTorch 2.0.1. To install TorchCP, simply run ``` pip install torchcp ``` To install from TestPyPI server, run ``` pip install --index-url https://test.pypi.org/simple/ --no-deps torchcp ```"
  },
  {
    "id": "24",
    "name": "NUS-HPC-AI-Lab/Neural-Network-Diffusion",
    "plans": [
      {
        "type": "Source",
        "steps": [
          {
            "text": "1. Clone the repository:",
            "note": "",
            "seq_order": 1,
            "is_optional": false,
            "commands": "```git clone https://github.com/NUS-HPC-AI-Lab/Neural-Network-Diffusion.git```"
          },
          {
            "text": "2. Create a new Conda environment and activate it:",
            "note": "",
            "seq_order": 2,
            "is_optional": false,
            "commands": "```conda env create -f environment.yml \n conda activate pdiff```"
          },
          {
            "text": "or install necessary package by:",
            "note": "",
            "seq_order": 3,
            "is_optional": true,
            "commands": "```pip install -r requirements.txt```"
          }
        ],
        "technology": [
          ""
        ]
      }
    ],
    "readme_instructions": "## Installation 1. Clone the repository: ``` git clone https://github.com/NUS-HPC-AI-Lab/Neural-Network-Diffusion.git ``` 2. Create a new Conda environment and activate it: ``` conda env create -f environment.yml conda activate pdiff ``` or install necessary package by: ``` pip install -r requirements.txt ```"
  },
  {
    "id": "25",
    "name": "sanjay-810/AYDIV2",
    "plans": [
      {
        "type": "Container",
        "steps": [
          {
            "text": "1.  Prepare for the running environment.",
            "note": "You can use the docker image provided by [`OpenPCDet`](https://github.com/open-mmlab/OpenPCDet). Our experiments are based on the docker provided by Voxel-R-CNN and we use NVIDIA Tesla V100 to train our Aydiv.",
            "seq_order": 1,
            "is_optional": false,
            "commands": ""
          }
        ],
        "technology": [
          "Docker"
        ]
      }
    ],
    "readme_instructions": "### **Installation** 1. Prepare for the running environment. You can use the docker image provided by [`OpenPCDet`](https://github.com/open-mmlab/OpenPCDet). Our experiments are based on the docker provided by Voxel-R-CNN and we use NVIDIA Tesla V100 to train our Aydiv."
  },
  {
    "id": "26",
    "name": "FasterDecoding/BitDelta",
    "plans": [
      {
        "type": "Source",
        "steps": [
          {
            "text": "1. Clone the repo and navigate to BitDelta:",
            "note": "",
            "seq_order": 1,
            "is_optional": false,
            "commands": "`git clone https://github.com/FasterDecoding/BitDelta \n cd BitDelta"
          },
          {
            "text": "2. Set up environment:",
            "note": "",
            "seq_order": 2,
            "is_optional": false,
            "commands": "```bash conda create -yn bitdelta python=3.9 \n conda activate bitdelta \n pip install -e .```"
          }
        ],
        "technology": [
          ""
        ]
      }
    ],
    "readme_instructions": "## Install 1. Clone the repo and navigate to BitDelta: ``` git clone https://github.com/FasterDecoding/BitDelta cd BitDelta ``` 2. Set up environment: ```bash conda create -yn bitdelta python=3.9 conda activate bitdelta pip install -e . ```"
  },
  {
    "id": "27",
    "name": "Emanual20/ZHEM",
    "plans": [
      {
        "type": "Source",
        "steps": [
          {
            "text": "First, install the requirement packages declared by `requirements.txt`. ",
            "note": "",
            "seq_order": 1,
            "is_optional": false,
            "commands": ""
          },
          {
            "text": "Modify [settings/example.json](./settings/example.json) on your desired processing procedure.",
            "note": "The configuration file may contains many options. For example, if you want to use regular expressions to match some dirty tokens, you should add your own regular expressions into `re_list` of `rm_re_rules`.",
            "seq_order": 2,
            "is_optional": false,
            "commands": ""
          },
          {
            "text": "Copy raw data to input_path in `settings/example.json`. And make sure the output_path not exists, otherwise the output path will be overwritten.",
            "note": "",
            "seq_order": 3,
            "is_optional": false,
            "commands": "```{commandline} \n pip install -r requirements.txt \n cp <raw_data> /path/to/input_path```"
          },
          {
            "text": "Then the processing pipeline will generate an `debug_report.json` into /path/to/report_path defined in `settings/example.json`.",
            "note": "The report displayed filter ratio of each filter rules, as well as match ratio, time cost and match cases of each regular expression cleaner rules and so on.",
            "seq_order": 4,
            "is_optional": false,
            "commands": "```{commandline} \n python main.py --conf settings/example.json```"
          },
          {
            "text": "After running the processing pipeline, the cleaned data will be merged as a `.jsonl` file, while there is just one record in a line.",
            "note": " The record is in the form of `json`, with the field `text`, representing the refined text.",
            "seq_order": 5,
            "is_optional": false,
            "commands": ""
          }
        ],
        "technology": [
          ""
        ]
      }
    ],
    "readme_instructions": "## Quick Start First, install the requirement packages declared by `requirements.txt`. Modify [settings/example.json](./settings/example.json) on your desired processing procedure. The configuration file may contains many options. For example, if you want to use regular expressions to match some dirty tokens, you should add your own regular expressions into `re_list` of `rm_re_rules`. Copy raw data to input_path in `settings/example.json`. And make sure the output_path not exists, otherwise the output path will be overwritten. ```{commandline} pip install -r requirements.txt cp <raw_data> /path/to/input_path ``` Then the processing pipeline will generate an `debug_report.json` into /path/to/report_path defined in `settings/example.json`. The report displayed filter ratio of each filter rules, as well as match ratio, time cost and match cases of each regular expression cleaner rules and so on. ```{commandline} python main.py --conf settings/example.json ``` After running the processing pipeline, the cleaned data will be merged as a `.jsonl` file, while there is just one record in a line. The record is in the form of `json`, with the field `text`, representing the refined text."
  },
  {
    "id": "28",
    "name": "FloatAI/humaneval-xl",
    "plans": [
      {
        "type": "Source",
        "steps": [
          {
            "text": "Check out and install this repository:",
            "note": "",
            "seq_order": 1,
            "is_optional": false,
            "commands": "```git clone git@github.com:FloatAI/humaneval-xl.git \n cd mxeval\n pip install -e mxeval```"
          }
        ],
        "technology": [
          ""
        ]
      }
    ],
    "readme_instructions": "### Installation Check out and install this repository: ``` git clone git@github.com:FloatAI/humaneval-xl.git cd mxeval pip install -e mxeval ```"
  },
  {
    "id": "29",
    "name": "M2219/DCVSMNet",
    "plans": [
      {
        "type": "Source",
        "steps": [
          {
            "text": "### Create a virtual environment and activate it.",
            "note": "",
            "seq_order": 1,
            "is_optional": false,
            "commands": "```conda create -n DCVSMNet python=3.8 \n conda activate DCVSMNet```"
          },
          {
            "text": "### Dependencies",
            "note": "",
            "seq_order": 2,
            "is_optional": false,
            "commands": "``` conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch -c nvidia pip install opencv-python pip install scikit-image pip install tensorboard pip install matplotlib pip install tqdm pip install timm==0.5.4 ```"
          }
        ],
        "technology": [
          ""
        ]
      }
    ],
    "readme_instructions": "## Install ### Create a virtual environment and activate it. ``` conda create -n DCVSMNet python=3.8 conda activate DCVSMNet ``` ### Dependencies ``` conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch -c nvidia pip install opencv-python pip install scikit-image pip install tensorboard pip install matplotlib pip install tqdm pip install timm==0.5.4 ```"
  },
  {
    "id": "30",
    "name": "sail-sg/GDPO",
    "plans": [
      {
        "type": "Source",
        "steps": [
          {
            "text": "## Installing dependence",
            "note": "If there are still issues, please refer to DiGress and add other dependencies as necessary.",
            "seq_order": 1,
            "is_optional": false,
            "commands": "```conda create --GDPO --file spec-list.txt \n conda activate GDPO pip install requrements.txt```"
          },
          {
            "text": "In the following steps, make sure you have activated the GDPO environment.",
            "note": "",
            "seq_order": 2,
            "is_optional": false,
            "commands": "```conda activate GDPO```"
          }
        ],
        "technology": [
          ""
        ]
      }
    ],
    "readme_instructions": "## Installing dependence ``` conda create --GDPO --file spec-list.txt conda activate GDPO pip install requrements.txt ``` If there are still issues, please refer to DiGress and add other dependencies as necessary. In the following steps, make sure you have activated the GDPO environment. ``` conda activate GDPO ```"
  },
  {
    "id": "31",
    "name": "josemanuel22/ISL",
    "plans": [
      {
        "type": "Packagemanager",
        "steps": [
          {
            "text": "To install ISL, simply use Julia's package manager.",
            "note": "The module is not registered so you need to clone the repository and follow the following steps:",
            "seq_order": 1,
            "is_optional": false,
            "commands": "````julia> push!(LOAD_PATH,pwd()) # You are in the ISL Repository \n julia> using ISL````"
          },
          {
            "text": "To reproduce the enviroment for compiling the repository:",
            "note": "If you want to use any utility subrepository like GAN or DeepAR, make sure it's within your path.",
            "seq_order": 2,
            "is_optional": false,
            "commands": "```(@v1.9) pkg>  activate pathToRepository/ISL```"
          }
        ],
        "technology": [
          "Julia"
        ]
      }
    ],
    "readme_instructions": "## How to install To install ISL, simply use Julia's package manager. The module is not registered so you need to clone the repository and follow the following steps: ```` julia> push!(LOAD_PATH,pwd()) # You are in the ISL Repository julia> using ISL ```` To reproduce the enviroment for compiling the repository: ```` (@v1.9) pkg> activate pathToRepository/ISL ```` If you want to use any utility subrepository like GAN or DeepAR, make sure it's within your path."
  },
  {
    "id": "32",
    "name": "Hendrik-code/spineps",
    "plans": [
      {
        "type": "Source",
        "steps": [
          {
            "text": "### Setup Venv \n The prder of the following instructions is important! 1. Use Conda or Pip to create a venv for python 3.11, we are using conda for this example:",
            "note": "This installation assumes you know your way around conda and virtual environments.",
            "seq_order": 1,
            "is_optional": false,
            "commands": "```bash conda create --name spineps python=3.11 \n conda activate spineps \n conda install pip```"
          },
          {
            "text": "2. Go to <a href=`https://pytorch.org/get-started/locally/`>https://pytorch.org/get-started/locally/</a> and install a correct pytorch version for your machine in your venv",
            "note": "",
            "seq_order": 2,
            "is_optional": false,
            "commands": "```bash pip install torch wheel -q \n pip install -e .```"
          },
          {
            "text": "3. Confirm that your pytorch package is working! Try calling these commands:",
            "note": "",
            "seq_order": 3,
            "is_optional": false,
            "commands": "```bash nvidia-smi```"
          },
          {
            "text": "This should show your GPU and it's usage.",
            "note": "This should throw no errors and return True",
            "seq_order": 4,
            "is_optional": true,
            "commands": "```python -c `import torch; print(torch.cuda.is_available())````"
          }
        ],
        "technology": [
          "Linux"
        ]
      }
    ],
    "readme_instructions": "## Installation (Ubuntu) This installation assumes you know your way around conda and virtual environments. ### Setup Venv The order of the following instructions is important! 1. Use Conda or Pip to create a venv for python 3.11, we are using conda for this example: ```bash conda create --name spineps python=3.11 conda activate spineps conda install pip ``` 2. Go to <a href=`https://pytorch.org/get-started/locally/`>https://pytorch.org/get-started/locally/</a> and install a correct pytorch version for your machine in your venv 3. Confirm that your pytorch package is working! Try calling these commands: ```bash nvidia-smi ``` This should show your GPU and it's usage. ```bash python -c `import torch; print(torch.cuda.is_available())` ``` This should throw no errors and return True"
  },
  {
    "id": "33",
    "name": "nand1155/CausNet",
    "plans": [
      {
        "type": "Source",
        "steps": [
          {
            "text": "You can install the development version from GitHub with:",
            "note": "",
            "seq_order": 1,
            "is_optional": false,
            "commands": "``` r require(`devtools`) \n install_github(`https://github.com/nand1155/CausNet`)```"
          }
        ],
        "technology": [
          ""
        ]
      }
    ],
    "readme_instructions": "## Installation You can install the development version from GitHub with: ``` r require(`devtools`) install_github(`https://github.com/nand1155/CausNet`)"
  }
]