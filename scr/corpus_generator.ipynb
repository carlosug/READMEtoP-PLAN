{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python notebook is designed to:\n",
    "# 1. Create a list of GitHub repositories from the awesome list found in the README.md file at https://github.com/jamesmurdza/awesome-ai-devtools\n",
    "# 2. For each GitHub repository in the list, read the README file and extract the installation instructions (e.g., sections titled \"How to install\", code blocks containing \"pip install\", \"git clone .git\", etc.)\n",
    "# 3. Generate a JSON-LD file with the following fields for each repository:\n",
    "#    - field1: URL (the GitHub link of the repository)\n",
    "#    - field2: Text (the extracted installation instructions)\n",
    "#    - field3: Tokens (the individual tokens of the text in field2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWESOME LIST OF AI DEVTOOLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each url found in the repos_urls, extract the installation instructions found in each readme file. For instance, the first url is https://github.com/silvanmelchior/IncognitoPilot; you need to find the readme.md file and extract the installation instructions found in the readme.md. In this case you should extract the line of comments and code text in here https://github.com/silvanmelchior/IncognitoPilot/blob/main/README.md#package-installation-gpt-via-openai-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import json\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def fetch_raw_markdown(url):\n",
    "    response = requests.get(url)\n",
    "    return response.text if response.status_code == 200 else None\n",
    "\n",
    "def extract_github_urls(markdown_content):\n",
    "    pattern = re.compile(r'https://github\\.com/[a-zA-Z0-9_-]+/[a-zA-Z0-9_-]+')\n",
    "    return pattern.findall(markdown_content)\n",
    "\n",
    "def fetch_readme_content(repo_url):\n",
    "    readme_url = repo_url.replace(\"github.com\", \"raw.githubusercontent.com\") + \"/main/README.md\"\n",
    "    response = requests.get(readme_url)\n",
    "    return response.text if response.status_code == 200 else None\n",
    "\n",
    "# def extract_installation_instructions(readme_content):\n",
    "#     keywords = [\"installation\", \"setup\", \"install\", \"how to\", \"getting started\", \"quick start\"]\n",
    "#     pattern = re.compile(\"|\".join(keywords), re.IGNORECASE)\n",
    "#     sections = re.split(r'#+ ', readme_content)\n",
    "#     installation_sections = [section for section in sections if pattern.search(section)]\n",
    "#     return installation_sections\n",
    "\n",
    "def extract_installation_instructions(readme_content):\n",
    "    sections = re.split(r'#+ ', readme_content)\n",
    "    installation_sections = [section for section in sections if re.search(r'installation|setup|install|how to|getting started|quick start', section, re.IGNORECASE)]\n",
    "    return installation_sections\n",
    "\n",
    "def differentiate_comments(installation_text):\n",
    "    code_comments = re.findall(r'```.*?```', installation_text, re.DOTALL)\n",
    "    text_comments = re.sub(r'```.*?```', '', installation_text, flags=re.DOTALL)\n",
    "    return text_comments, code_comments\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# from nltk.corpus import stopwords\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "   \n",
    "# def tokenize_and_lemmatize(text):\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "#     tokens = [lemmatizer.lemmatize(word) for word in word_tokenize(text) if word not in stop_words]\n",
    "#     return tokens\n",
    "\n",
    "\n",
    "# Create an heuristic classifier to cluster the repo_url by complexity of the installation instructions: \n",
    "# complexity = 0 if the installation instructions contain in token and text: \"pip install\", \"package manager install\",\n",
    "# complexity = 1 if the installation instructions contain: \"container\", \"docker container\", \"docker componse up\"\n",
    "# complexity = 2 if the installation instructions contain: \"from source\", \"git clone\", \".git\"\n",
    "# append the heuristic classifier to the repos_data\n",
    "\n",
    "\n",
    "# Main execution\n",
    "awesome_list_url = \"https://raw.githubusercontent.com/jamesmurdza/awesome-ai-devtools/main/README.md\"\n",
    "markdown_content = fetch_raw_markdown(awesome_list_url)\n",
    "repos_data = []\n",
    "\n",
    "if markdown_content:\n",
    "    repos_urls = extract_github_urls(markdown_content)\n",
    "    for repo_url in repos_urls:\n",
    "        readme_content = fetch_readme_content(repo_url)\n",
    "        if readme_content:\n",
    "            installation_instructions = extract_installation_instructions(readme_content)\n",
    "            instructions_text = \" \".join(installation_instructions)\n",
    "            tokens = tokenize_text(instructions_text)\n",
    "\n",
    "            # Heuristic classifier\n",
    "            complexity = -1  # Default complexity\n",
    "            if any(word in tokens for word in [\"pip install\", \"package manager install\"]):\n",
    "                complexity = 0\n",
    "            elif any(word in instructions_text for word in [\"container\", \"docker container\", \"docker compose up\"]):\n",
    "                complexity = 1\n",
    "            elif any(word in instructions_text for word in [\"from source\", \"git clone\", \".git\"]):\n",
    "                complexity = 2\n",
    "\n",
    "            repos_data.append({\n",
    "                \"url\": repo_url,\n",
    "                \"text\": instructions_text,\n",
    "                \"tokens\": tokens,\n",
    "                \"level complexity\": complexity\n",
    "            })\n",
    "else:\n",
    "    print(\"Failed to fetch the markdown content of the awesome list.\")\n",
    "\n",
    "# Output to a JSON file\n",
    "with open('data/corpus-awesome_list.json', 'w') as outfile:\n",
    "    json.dump(repos_data, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ccugutrillague/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>level complexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://github.com/silvanmelchior/IncognitoPilot</td>\n",
       "      <td>:package: Installation (GPT via OpenAI API)\\n\\...</td>\n",
       "      <td>[:, package, :, Installation, (, GPT, via, Ope...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://github.com/smallcloudai/refact</td>\n",
       "      <td>Running Refact Self-Hosted in a Docker Contain...</td>\n",
       "      <td>[Running, Refact, Self-Hosted, in, a, Docker, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://github.com/rubberduck-ai/rubberduck-vs...</td>\n",
       "      <td>Quick Install\\n\\nYou can install Rubberduck fr...</td>\n",
       "      <td>[Quick, Install, You, can, install, Rubberduck...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0   https://github.com/silvanmelchior/IncognitoPilot   \n",
       "1             https://github.com/smallcloudai/refact   \n",
       "2  https://github.com/rubberduck-ai/rubberduck-vs...   \n",
       "\n",
       "                                                text  \\\n",
       "0  :package: Installation (GPT via OpenAI API)\\n\\...   \n",
       "1  Running Refact Self-Hosted in a Docker Contain...   \n",
       "2  Quick Install\\n\\nYou can install Rubberduck fr...   \n",
       "\n",
       "                                              tokens  level complexity  \n",
       "0  [:, package, :, Installation, (, GPT, via, Ope...                 1  \n",
       "1  [Running, Refact, Self-Hosted, in, a, Docker, ...                 1  \n",
       "2  [Quick, Install, You, can, install, Rubberduck...                -1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "columns_long_list = ['url', 'text', 'tokens', 'level complexity']\n",
    "columns_short_list = ['url', 'text', 'tokens']\n",
    "df = pd.read_json('data/corpus-awesome_list.json')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "level complexity\n",
       " 2    8\n",
       "-1    5\n",
       " 1    2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['level complexity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "level complexity\n",
       "-1    5\n",
       " 1    2\n",
       " 2    8\n",
       "Name: tokens, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['level complexity'])['tokens'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>token_len</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>level complexity</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">-1</th>\n",
       "      <th>0.50</th>\n",
       "      <td>107.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.70</th>\n",
       "      <td>519.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.80</th>\n",
       "      <td>671.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>768.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.95</th>\n",
       "      <td>816.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>0.50</th>\n",
       "      <td>746.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.70</th>\n",
       "      <td>895.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.80</th>\n",
       "      <td>969.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>1043.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.95</th>\n",
       "      <td>1080.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2</th>\n",
       "      <th>0.50</th>\n",
       "      <td>277.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.70</th>\n",
       "      <td>625.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.80</th>\n",
       "      <td>1001.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>1416.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.95</th>\n",
       "      <td>1635.90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       token_len\n",
       "level complexity                \n",
       "-1               0.50     107.00\n",
       "                 0.70     519.80\n",
       "                 0.80     671.40\n",
       "                 0.90     768.20\n",
       "                 0.95     816.60\n",
       " 1               0.50     746.50\n",
       "                 0.70     895.10\n",
       "                 0.80     969.40\n",
       "                 0.90    1043.70\n",
       "                 0.95    1080.85\n",
       " 2               0.50     277.00\n",
       "                 0.70     625.10\n",
       "                 0.80    1001.00\n",
       "                 0.90    1416.80\n",
       "                 0.95    1635.90"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['token_len'] = df.tokens.apply(lambda x: len(x))\n",
    "query_len_summary = df.groupby('level complexity')['token_len'].quantile([.5, .7, .8, .9, .95])\n",
    "display(pd.DataFrame(query_len_summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>level complexity</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">-1</th>\n",
       "      <th>0.50</th>\n",
       "      <td>107.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.70</th>\n",
       "      <td>519.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.80</th>\n",
       "      <td>671.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>768.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.95</th>\n",
       "      <td>816.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>0.50</th>\n",
       "      <td>746.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.70</th>\n",
       "      <td>895.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.80</th>\n",
       "      <td>969.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>1043.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.95</th>\n",
       "      <td>1080.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2</th>\n",
       "      <th>0.50</th>\n",
       "      <td>277.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.70</th>\n",
       "      <td>625.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.80</th>\n",
       "      <td>1001.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>1416.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.95</th>\n",
       "      <td>1635.90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          text\n",
       "level complexity              \n",
       "-1               0.50   107.00\n",
       "                 0.70   519.80\n",
       "                 0.80   671.40\n",
       "                 0.90   768.20\n",
       "                 0.95   816.60\n",
       " 1               0.50   746.50\n",
       "                 0.70   895.10\n",
       "                 0.80   969.40\n",
       "                 0.90  1043.70\n",
       "                 0.95  1080.85\n",
       " 2               0.50   277.00\n",
       "                 0.70   625.10\n",
       "                 0.80  1001.00\n",
       "                 0.90  1416.80\n",
       "                 0.95  1635.90"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['text'] = df.tokens.apply(lambda x: len(x))\n",
    "query_len_summary = df.groupby('level complexity')['text'].quantile([.5, .7, .8, .9, .95])\n",
    "display(pd.DataFrame(query_len_summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Token  Frequency\n",
      "0     :        373\n",
      "1     `        338\n",
      "2     .        268\n",
      "3   the        242\n",
      "4     *        227\n",
      "5     |        186\n",
      "6     ,        180\n",
      "7     (        169\n",
      "8     )        169\n",
      "9     -        148\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Assuming df is your DataFrame and 'field3' is the column with tokens\n",
    "# Step 1: Aggregate Tokens\n",
    "all_tokens = sum(df['tokens'].tolist(), [])\n",
    "\n",
    "# Step 2: Count Frequencies\n",
    "token_counts = Counter(all_tokens)\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Summarize Most Frequent Tokens\n",
    "most_common_tokens = token_counts.most_common(10)  # Adjust the number to get more or fewer tokens\n",
    "\n",
    "# Convert the most common tokens to a DataFrame for a nicer display\n",
    "summary_df = pd.DataFrame(most_common_tokens, columns=['Token', 'Frequency'])\n",
    "\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "Empty DataFrame\n",
      "Columns: [Token, Frequency]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Define the specific tokens you're interested in\n",
    "import re\n",
    "specific_tokens = [token for token in summary_df['Token'] if re.search(r'docker', token, re.IGNORECASE)]\n",
    "print(specific_tokens)\n",
    "# Filter the DataFrame for rows where the 'Token' column contains any of the specific tokens\n",
    "filtered_df = summary_df[summary_df['Token'].isin(specific_tokens)]\n",
    "\n",
    "print(filtered_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PaperWithCodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests nltk pandas paperswithcode-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "   \n",
    "def fetch_repositories():\n",
    "    response = requests.get(\"https://paperswithcode.com/api/v1/repositories/\")\n",
    "    return response.json()['results'] if response.status_code == 200 else []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_readme_content(repo_url):\n",
    "    readme_url = repo_url\n",
    "    response = requests.get(readme_url)\n",
    "    return response.text if response.status_code == 200 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_installation_instructions(readme_content):\n",
    "    sections = re.split(r'#+ ', readme_content)\n",
    "    installation_sections = [section for section in sections if re.search(r'installation|setup|install|how to|getting started|quick start', section, re.IGNORECASE)]\n",
    "    return installation_sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def differentiate_comments(installation_text):\n",
    "    code_comments = re.findall(r'```.*?```', installation_text, re.DOTALL)\n",
    "    text_comments = re.sub(r'```.*?```', '', installation_text, flags=re.DOTALL)\n",
    "    return text_comments, code_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ccugutrillague/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ccugutrillague/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ccugutrillague/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "   \n",
    "def tokenize_and_lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in word_tokenize(text) if word not in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "   \n",
    "def output_to_files(data):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_json(\"data/corpus-paperwithcodes.json\", orient=\"records\", lines=True)\n",
    "    df.to_csv(\"data/corpus-paperwithcodes.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_repository(repo):\n",
    "    data = {}\n",
    "    readme_content = fetch_readme_content(repo['url'])\n",
    "    if readme_content:\n",
    "        installation_sections = extract_installation_instructions(readme_content)\n",
    "        for section in installation_sections:\n",
    "            text_comments, code_comments = differentiate_comments(section)\n",
    "            tokens = tokenize_and_lemmatize(section)\n",
    "            data = {\n",
    "                \"url\": repo['url'],\n",
    "                \"readme_url\": repo['url'],\n",
    "                \"text\": section,\n",
    "                \"comments\": text_comments,\n",
    "                \"code-comments\": \" \".join(code_comments),\n",
    "                \"sentence\": sent_tokenize(section),\n",
    "                \"tokens\": tokens\n",
    "            }\n",
    "    return data\n",
    "\n",
    "def main():\n",
    "    repos = fetch_repositories()\n",
    "    processed_data = []\n",
    "    for repo in repos:\n",
    "        repo_data = process_repository(repo)\n",
    "        if repo_data:\n",
    "            processed_data.append(repo_data)\n",
    "    \n",
    "    # Output to files\n",
    "    output_to_files(processed_data)\n",
    "\n",
    "def output_to_files(data):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_json(\"data/corpus-paperwithcodes.json\", orient=\"records\", lines=True)\n",
    "    df.to_csv(\"data/corpus-paperwithcodes.csv\", index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from paperswithcode import PapersWithCodeClient\n",
    "\n",
    "client = PapersWithCodeClient()\n",
    "papers = client.repository_list()\n",
    "print(papers.next_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper = papers.results[4]\n",
    "paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Github -repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRepostitoryTopics(url, GitHub_Token, not_found):\n",
    "    header = {'Authorization': 'Bearer ' + GitHub_Token}\n",
    "    reposUrl = f\"https://api.github.com/repos/{url}\"\n",
    "    reposr = requests.get(reposUrl, headers = header)\n",
    "    reposj = reposr.json()\n",
    "    try:\n",
    "        topics = reposj[\"topics\"]\n",
    "        return (not_found, topics)\n",
    "    except:\n",
    "        return (not_found + 1, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_dict = {}\n",
    "GitHub_Token = \"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\"\n",
    "listTopics = ['LLM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_repositories_by_topic(topic):\n",
    "#     header = {'Authorization': 'Bearer ' + GitHub_Token}\n",
    "#     reposUrl = f\"https://api.github.com/search/repositories?q=topic:{topic}&per_page=50\"\n",
    "#     reposr = requests.get(reposUrl, headers = header)\n",
    "#     reposj = reposr.json()\n",
    "#     return reposj[\"items\"]\n",
    "\n",
    "def get_repositories_by_topic(topic):\n",
    "    header = {'Authorization': 'Bearer ' + GitHub_Token}\n",
    "    reposUrl = f\"https://api.github.com/search/repositories?q=topic:{topic}&per_page=50\"\n",
    "    reposr = requests.get(reposUrl, headers=header)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if reposr.status_code == 200:\n",
    "        reposj = reposr.json()\n",
    "        # Check if 'items' key exists in the response\n",
    "        if \"items\" in reposj:\n",
    "            return reposj[\"items\"]\n",
    "        else:\n",
    "            print(f\"'items' key not found in response. Response JSON: {reposj}\")\n",
    "            return []\n",
    "    else:\n",
    "        print(f\"GitHub API request failed with status code: {reposr.status_code}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_with_topics = {}\n",
    "for topic in listTopics:\n",
    "    repositories_json = get_repositories_by_topic(topic)\n",
    "    for repository in repositories_json:\n",
    "        topics = repository[\"topics\"]\n",
    "        combined_topics_string = '\\t'.join(topics)\n",
    "        if(not \"LLM\" in combined_topics_string):\n",
    "            urls_with_topics[repository[\"html_url\"]] = topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'https://github.com/ollama/ollama': ['go', 'golang', 'llama', 'llama2', 'llm', 'llms', 'mistral', 'ollama'], 'https://github.com/geekan/MetaGPT': ['agent', 'gpt', 'hacktoberfest', 'llm', 'metagpt', 'multi-agent'], 'https://github.com/run-llama/llama_index': ['agents', 'application', 'data', 'fine-tuning', 'framework', 'llamaindex', 'llm', 'rag', 'vector-database'], 'https://github.com/QuivrHQ/quivr': ['ai', 'api', 'chatbot', 'chatgpt', 'database', 'docker', 'frontend', 'html', 'javascript', 'llm', 'openai', 'postgresql', 'privacy', 'rag', 'react', 'rest-api', 'security', 'typescript', 'vector', 'ycombinator'], 'https://github.com/milvus-io/milvus': ['anns', 'cloud-native', 'distributed', 'embedding-database', 'embedding-similarity', 'embedding-store', 'faiss', 'golang', 'hnsw', 'image-search', 'llm', 'nearest-neighbor-search', 'tensor-database', 'vector-database', 'vector-search', 'vector-similarity', 'vector-store'], 'https://github.com/JushBJJ/Mr.-Ranedeer-AI-Tutor': ['ai', 'education', 'gpt-4', 'llm'], 'https://github.com/mlabonne/llm-course': ['course', 'large-language-models', 'llm', 'machine-learning', 'roadmap'], 'https://github.com/chatchat-space/Langchain-Chatchat': ['chatbot', 'chatchat', 'chatglm', 'chatglm-6b', 'chatglm2-6b', 'chatgpt', 'embedding', 'faiss', 'fastchat', 'gpt', 'knowledge-base', 'langchain', 'langchain-chatchat', 'langchain-chatglm', 'llama', 'llm', 'milvus', 'pgvector', 'streamlit', 'text2vec'], 'https://github.com/zhayujie/chatgpt-on-wechat': ['ai', 'chatglm-4', 'chatgpt', 'dingtalk', 'feishu-bot', 'gemini', 'glm', 'gpt-4', 'linkai', 'llm', 'openai', 'python3', 'qwen', 'rag', 'wechat', 'wechat-bot', 'wenxinyiyan', 'xunfei-spark'], 'https://github.com/FlowiseAI/Flowise': ['artificial-intelligence', 'chatgpt', 'javascript', 'large-language-models', 'llm', 'low-code', 'no-code', 'react', 'typescript'], 'https://github.com/mindsdb/mindsdb': ['ai', 'ai-agents', 'artificial-intelligence', 'auto-gpt', 'chatbot', 'database', 'forecasting', 'gpt', 'gpt4all', 'hacktoberfest', 'huggingface', 'llm', 'machine-learning', 'ml', 'mongodb', 'mysql', 'postgres', 'semantic-search', 'timeseries'], 'https://github.com/microsoft/unilm': ['beit', 'beit-3', 'deepnet', 'document-ai', 'foundation-models', 'kosmos', 'kosmos-1', 'layoutlm', 'layoutxlm', 'llm', 'minilm', 'mllm', 'multimodal', 'nlp', 'pre-trained-model', 'textdiffuser', 'trocr', 'unilm', 'xlm-e'], 'https://github.com/microsoft/semantic-kernel': ['ai', 'artificial-intelligence', 'llm', 'openai', 'sdk'], 'https://github.com/mudler/LocalAI': ['ai', 'alpaca', 'api', 'api-rest', 'bloom', 'containers', 'coqui', 'falcon', 'gpt-neox', 'gpt4all', 'guanaco', 'kubernetes', 'llama', 'llm', 'mamba', 'musicgen', 'rwkv', 'stable-diffusion', 'tts', 'vicuna'], 'https://github.com/ymcui/Chinese-LLaMA-Alpaca': ['alpaca', 'alpaca-2', 'large-language-models', 'llama', 'llama-2', 'llm', 'lora', 'nlp', 'plm', 'pre-trained-language-models', 'quantization'], 'https://github.com/mlc-ai/mlc-llm': ['language-model', 'llm', 'machine-learning-compilation', 'tvm'], 'https://github.com/langgenius/dify': ['ai', 'backend-as-a-service', 'gpt', 'gpt-4', 'langchain', 'llama2', 'llm', 'openai', 'orchestration', 'python', 'rag'], 'https://github.com/THUDM/ChatGLM2-6B': ['chatglm', 'chatglm-6b', 'large-language-models', 'llm'], 'https://github.com/vllm-project/vllm': ['gpt', 'inference', 'llama', 'llm', 'llm-serving', 'llmops', 'mlops', 'model-serving', 'pytorch', 'transformer'], 'https://github.com/TransformerOptimus/SuperAGI': ['agents', 'agi', 'ai', 'artificial-general-intelligence', 'artificial-intelligence', 'autonomous-agents', 'gpt-4', 'hacktoberfest', 'llm', 'llmops', 'nextjs', 'openai', 'pinecone', 'python', 'superagi'], 'https://github.com/cocktailpeanut/dalai': ['ai', 'llama', 'llm'], 'https://github.com/huggingface/peft': ['adapter', 'diffusion', 'llm', 'lora', 'parameter-efficient-learning', 'python', 'pytorch', 'transformers'], 'https://github.com/botpress/botpress': ['agent', 'ai', 'botpress', 'chatbot', 'chatgpt', 'gpt', 'gpt-4', 'langchain', 'llm', 'nlp', 'openai', 'prompt'], 'https://github.com/hiyouga/LLaMA-Factory': ['agent', 'baichuan', 'chatglm', 'fine-tuning', 'generative-ai', 'gpt', 'instruction-tuning', 'language-model', 'large-language-models', 'llama', 'llm', 'lora', 'mistral', 'mixture-of-experts', 'peft', 'qlora', 'quantization', 'qwen', 'rlhf', 'transformers'], 'https://github.com/PaddlePaddle/PaddleNLP': ['bert', 'compression', 'distributed-training', 'document-intelligence', 'embedding', 'ernie', 'information-extraction', 'llama', 'llm', 'neural-search', 'nlp', 'paddlenlp', 'pretrained-models', 'question-answering', 'search-engine', 'semantic-analysis', 'sentiment-analysis', 'transformers', 'uie'], 'https://github.com/ludwig-ai/ludwig': ['computer-vision', 'data-centric', 'data-science', 'deep', 'deep-learning', 'deeplearning', 'fine-tuning', 'learning', 'llama', 'llama2', 'llm', 'llm-training', 'machine-learning', 'machinelearning', 'mistral', 'ml', 'natural-language', 'natural-language-processing', 'neural-network', 'pytorch'], 'https://github.com/getumbrel/llama-gpt': ['ai', 'chatgpt', 'code-llama', 'codellama', 'gpt', 'gpt-4', 'gpt4all', 'llama', 'llama-2', 'llama-cpp', 'llama2', 'llamacpp', 'llm', 'localai', 'openai', 'self-hosted'], 'https://github.com/eosphoros-ai/DB-GPT': ['agents', 'bgi', 'database', 'gpt', 'gpt-4', 'langchain', 'llm', 'private', 'rag', 'security', 'vicuna'], 'https://github.com/gventuri/pandas-ai': ['ai', 'csv', 'data', 'data-analysis', 'data-science', 'gpt-3', 'gpt-4', 'llm', 'pandas', 'sql'], 'https://github.com/h2oai/h2ogpt': ['ai', 'chatgpt', 'embeddings', 'generative', 'gpt', 'gpt4all', 'llama2', 'llm', 'mixtral', 'pdf', 'private', 'privategpt', 'vectorstore'], 'https://github.com/labring/FastGPT': ['gpt', 'gpt-35-turbo', 'gpt-4', 'gpt35', 'llm', 'nextjs', 'openai', 'rag', 'react'], 'https://github.com/eugeneyan/open-llms': ['commercial', 'large-language-models', 'llm', 'llms'], 'https://github.com/ShishirPatil/gorilla': ['api', 'api-documentation', 'chatgpt', 'claude-api', 'gpt-4-api', 'llm', 'openai-api', 'openai-functions'], 'https://github.com/QwenLM/Qwen': ['chinese', 'flash-attention', 'large-language-models', 'llm', 'natural-language-processing', 'pretrained-models'], 'https://github.com/rasbt/LLMs-from-scratch': ['chatgpt', 'gpt', 'large-language-models', 'llm', 'python', 'pytorch'], 'https://github.com/mlc-ai/web-llm': ['chatgpt', 'deep-learning', 'language-model', 'llm', 'tvm', 'webgpu', 'webml'], 'https://github.com/nebuly-ai/nebuly': ['ai', 'analytics', 'artificial-intelligence', 'deeplearning', 'large-language-models', 'llm'], 'https://github.com/LlamaFamily/Llama2-Chinese': ['finetune', 'llama', 'llama2', 'llm', 'lora', 'pretrain'], 'https://github.com/bentoml/OpenLLM': ['ai', 'bentoml', 'falcon', 'fine-tuning', 'llama', 'llama2', 'llm', 'llm-inference', 'llm-ops', 'llm-serving', 'llmops', 'mistral', 'ml', 'mlops', 'model-inference', 'mpt', 'open-source-llm', 'openllm', 'stablelm', 'vicuna'], 'https://github.com/cpacker/MemGPT': ['chat', 'chatbot', 'gpt', 'gpt-4', 'llm', 'llm-agent'], 'https://github.com/stas00/ml-engineering': ['ai', 'bash', 'large-language-models', 'llm', 'machine-learning', 'machine-learning-engineering', 'make', 'mlops', 'python', 'pytorch', 'scalability', 'slurm', 'transformers'], 'https://github.com/RUCAIBox/LLMSurvey': ['chain-of-thought', 'chatgpt', 'in-context-learning', 'instruction-tuning', 'large-language-models', 'llm', 'llms', 'natural-language-processing', 'pre-trained-language-models', 'pre-training', 'rlhf'], 'https://github.com/embedchain/embedchain': ['ai', 'application', 'chatbots', 'chatgpt', 'embeddings', 'llm', 'python', 'rag', 'vector-database'], 'https://github.com/activeloopai/deeplake': ['ai', 'computer-vision', 'cv', 'data-science', 'data-version-control', 'datalake', 'datasets', 'deep-learning', 'image-processing', 'langchain', 'large-language-models', 'llm', 'machine-learning', 'ml', 'mlops', 'python', 'pytorch', 'tensorflow', 'vector-database', 'vector-search'], 'https://github.com/microsoft/TypeChat': ['ai', 'llm', 'natural-language', 'types'], 'https://github.com/mistralai/mistral-src': ['llm', 'llm-inference', 'mistralai'], 'https://github.com/facebookresearch/llama-recipes': ['ai', 'finetuning', 'langchain', 'llama', 'llama2', 'llm', 'machine-learning', 'python', 'pytorch', 'vllm'], 'https://github.com/microsoft/promptflow': ['ai', 'ai-application-development', 'ai-applications', 'chatgpt', 'gpt', 'llm', 'prompt', 'prompt-engineering'], 'https://github.com/TheR1D/shell_gpt': ['chatgpt', 'cheat-sheet', 'cli', 'commands', 'gpt-3', 'gpt-4', 'linux', 'llm', 'openai', 'productivity', 'python', 'shell', 'terminal'], 'https://github.com/continuedev/continue': ['ai', 'chatgpt', 'copilot', 'developer-tools', 'intellij', 'jetbrains', 'llm', 'open-source', 'openai', 'pycharm', 'software-development', 'visual-studio-code', 'vscode']}\n"
     ]
    }
   ],
   "source": [
    "print(urls_with_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ccugutrillague/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ccugutrillague/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ccugutrillague/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import nltk\n",
    "import json\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def fetch_readme_content(repo_url):\n",
    "    readme_url = repo_url.replace(\"github.com\", \"raw.githubusercontent.com\") + \"/main/README.md\"\n",
    "    response = requests.get(readme_url)\n",
    "    return response.text if response.status_code == 200 else None\n",
    "# todo: split text into sentences; tokenize the text; lemmatize and lowercase all tokens; remove stop words (preprocessing)\n",
    "\n",
    "def extract_installation_instructions(readme_content):\n",
    "    keywords = [\"installation\", \"setup\", \"install\", \"how to\", \"getting started\", \"quick start\"]\n",
    "    pattern = re.compile(\"|\".join(keywords), re.IGNORECASE)\n",
    "    sections = re.split(r'#+ ', readme_content)\n",
    "    installation_sections = [section for section in sections if pattern.search(section)]\n",
    "    return installation_sections\n",
    "\n",
    "# def tokenize_text(text):\n",
    "#     return word_tokenize(text)\n",
    "def process_text(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    processed_sentences = []\n",
    "    processed_tokens = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        tokens = word_tokenize(sentence)\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens if token.lower() not in stop_words]\n",
    "        \n",
    "        processed_sentences.append(' '.join(lemmatized_tokens))  # Join tokens to form the processed sentence\n",
    "        processed_tokens.extend(lemmatized_tokens)  # Extend the list of processed tokens\n",
    "    \n",
    "    return processed_tokens, processed_sentences\n",
    "\n",
    "def classify_complexity(text):\n",
    "    complexity = -1  # Default complexity\n",
    "    if any(word in text for word in [\"pip install\", \"package manager install\"]):\n",
    "        complexity = 0\n",
    "    elif any(word in text for word in [\"container\", \"docker container\", \"docker compose up\"]):\n",
    "        complexity = 1\n",
    "    elif any(word in text for word in [\"from source\", \"git clone\", \".git\"]):\n",
    "        complexity = 2\n",
    "    return complexity\n",
    "\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.mixture import GaussianMixture\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# import numpy as np\n",
    "\n",
    "# def cluster_data(texts, topics, sentence_lengths, code_counts):\n",
    "#     # Vectorize the texts and topics\n",
    "#     vectorizer = TfidfVectorizer()\n",
    "#     text_features = vectorizer.fit_transform(texts + topics).toarray()\n",
    "    \n",
    "#     # Combine all features\n",
    "#     features = np.hstack((text_features, np.array(sentence_lengths).reshape(-1, 1), np.array(code_counts).reshape(-1, 1)))\n",
    "    \n",
    "#     # Optional: Apply PCA for dimensionality reduction\n",
    "#     pca = PCA(n_components=0.95)  # Keep 95% of variance\n",
    "#     reduced_features = pca.fit_transform(features)\n",
    "    \n",
    "#     # Cluster using Gaussian Mixture Model\n",
    "#     gmm = GaussianMixture(n_components=3)  # We want to cluster into 3 groups\n",
    "#     gmm.fit(reduced_features)\n",
    "#     cluster_labels = gmm.predict(reduced_features)\n",
    "#     probabilities = gmm.predict_proba(reduced_features)\n",
    "    \n",
    "#     return cluster_labels, probabilities\n",
    "\n",
    "# Iterate through the URLs and perform the tasks\n",
    "output_data = []\n",
    "\n",
    "for url, topics in urls_with_topics.items():\n",
    "    readme_url = url.replace(\"github.com\", \"raw.githubusercontent.com\") + \"/main/README.md\"  # Define readme_url for each repository\n",
    "    readme_content = fetch_readme_content(url)\n",
    "    if readme_content:\n",
    "        installation_instructions = extract_installation_instructions(readme_content)\n",
    "        for instruction in installation_instructions:\n",
    "            tokens = process_text(instruction)\n",
    "            sentence = process_text(instruction)\n",
    "            complexity = classify_complexity(instruction)\n",
    "            # cluster_labels, probabilities = cluster_data([instruction], topics, [len(sentence)], [len(tokens)])\n",
    "            output_data.append({\n",
    "                \"url\": url,\n",
    "                \"readme_url\": readme_url,\n",
    "                \"topic\": topics,\n",
    "                \"text\": instruction,\n",
    "                'sentence': sentence,\n",
    "                \"token\": tokens,\n",
    "                # \"cluster_labels\": cluster_labels.tolist(),\n",
    "                # \"probabilities\": probabilities.tolist()\n",
    "                \"level of complexity\": complexity\n",
    "            })\n",
    "\n",
    "# Output to a JSON file\n",
    "with open('data/corpus-github.json', 'w') as outfile:\n",
    "    json.dump(output_data, outfile, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
