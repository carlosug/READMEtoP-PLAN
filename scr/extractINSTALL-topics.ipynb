{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A jupter notebook that crawl Readme.md files from paperwithcode using python\n",
    "# 1 Script to get the text from installation instructions of a readme file available in paperwithcode. For each file:\n",
    "# 2. It extracts the content (code comments and text comments) from the section \"Installation\" from the readme. Please extract the part of text that relates or match with the following: |installation|setup|install|how|Getting started/quick start|\n",
    "# 3. Create a dictionary with all the links and text per repository and output a csv file containing: url, text, tokenization of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "\n",
    "def getRepostitoryTopics(url, GitHub_Token, not_found):\n",
    "    header = {'Authorization': 'Bearer ' + GitHub_Token}\n",
    "    reposUrl = f\"https://api.github.com/repos/{url}\"\n",
    "    reposr = requests.get(reposUrl, headers = header)\n",
    "    reposj = reposr.json()\n",
    "    try:\n",
    "        topics = reposj[\"topics\"]\n",
    "        return (not_found, topics)\n",
    "    except:\n",
    "        return (not_found + 1, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_dict = {}\n",
    "GitHub_Token = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "listTopics = ['LLM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repositories_by_topic(topic):\n",
    "    header = {'Authorization': 'Bearer ' + GitHub_Token}\n",
    "    reposUrl = f\"https://api.github.com/search/repositories?q=topic:{topic}&per_page=50\"\n",
    "    reposr = requests.get(reposUrl, headers = header)\n",
    "    reposj = reposr.json()\n",
    "    return reposj[\"items\"]\n",
    "\n",
    "# def get_repositories_by_topic(topic):\n",
    "#     header = {'Authorization': 'Bearer ' + GitHub_Token}\n",
    "#     reposUrl = f\"https://api.github.com/search/repositories?q=topic:{topic}&per_page=50\"\n",
    "#     reposr = requests.get(reposUrl, headers=header)\n",
    "    \n",
    "#     # Check if the request was successful\n",
    "#     if reposr.status_code == 200:\n",
    "#         reposj = reposr.json()\n",
    "#         # Check if 'items' key exists in the response\n",
    "#         if \"items\" in reposj:\n",
    "#             return reposj[\"items\"]\n",
    "#         else:\n",
    "#             print(f\"'items' key not found in response. Response JSON: {reposj}\")\n",
    "#             return []\n",
    "#     else:\n",
    "#         print(f\"GitHub API request failed with status code: {reposr.status_code}\")\n",
    "#         return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_with_topics = {}\n",
    "for topic in listTopics:\n",
    "    repositories_json = get_repositories_by_topic(topic)\n",
    "    for repository in repositories_json:\n",
    "        topics = repository[\"topics\"]\n",
    "        combined_topics_string = '\\t'.join(topics)\n",
    "        if(not \"LLM\" in combined_topics_string):\n",
    "            urls_with_topics[repository[\"html_url\"]] = topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'https://github.com/ollama/ollama': ['go', 'golang', 'llama', 'llama2', 'llm', 'llms', 'mistral', 'ollama'], 'https://github.com/geekan/MetaGPT': ['agent', 'gpt', 'hacktoberfest', 'llm', 'metagpt', 'multi-agent'], 'https://github.com/run-llama/llama_index': ['agents', 'application', 'data', 'fine-tuning', 'framework', 'llamaindex', 'llm', 'rag', 'vector-database'], 'https://github.com/QuivrHQ/quivr': ['ai', 'api', 'chatbot', 'chatgpt', 'database', 'docker', 'frontend', 'html', 'javascript', 'llm', 'openai', 'postgresql', 'privacy', 'rag', 'react', 'rest-api', 'security', 'typescript', 'vector', 'ycombinator'], 'https://github.com/milvus-io/milvus': ['anns', 'cloud-native', 'distributed', 'embedding-database', 'embedding-similarity', 'embedding-store', 'faiss', 'golang', 'hnsw', 'image-search', 'llm', 'nearest-neighbor-search', 'tensor-database', 'vector-database', 'vector-search', 'vector-similarity', 'vector-store'], 'https://github.com/JushBJJ/Mr.-Ranedeer-AI-Tutor': ['ai', 'education', 'gpt-4', 'llm'], 'https://github.com/mlabonne/llm-course': ['course', 'large-language-models', 'llm', 'machine-learning', 'roadmap'], 'https://github.com/chatchat-space/Langchain-Chatchat': ['chatbot', 'chatchat', 'chatglm', 'chatglm-6b', 'chatglm2-6b', 'chatgpt', 'embedding', 'faiss', 'fastchat', 'gpt', 'knowledge-base', 'langchain', 'langchain-chatchat', 'langchain-chatglm', 'llama', 'llm', 'milvus', 'pgvector', 'streamlit', 'text2vec'], 'https://github.com/zhayujie/chatgpt-on-wechat': ['ai', 'chatglm-4', 'chatgpt', 'dingtalk', 'feishu-bot', 'gemini', 'glm', 'gpt-4', 'linkai', 'llm', 'openai', 'python3', 'qwen', 'rag', 'wechat', 'wechat-bot', 'wenxinyiyan', 'xunfei-spark'], 'https://github.com/FlowiseAI/Flowise': ['artificial-intelligence', 'chatgpt', 'javascript', 'large-language-models', 'llm', 'low-code', 'no-code', 'react', 'typescript'], 'https://github.com/mindsdb/mindsdb': ['ai', 'ai-agents', 'artificial-intelligence', 'auto-gpt', 'chatbot', 'database', 'forecasting', 'gpt', 'gpt4all', 'hacktoberfest', 'huggingface', 'llm', 'machine-learning', 'ml', 'mongodb', 'mysql', 'postgres', 'semantic-search', 'timeseries'], 'https://github.com/microsoft/unilm': ['beit', 'beit-3', 'deepnet', 'document-ai', 'foundation-models', 'kosmos', 'kosmos-1', 'layoutlm', 'layoutxlm', 'llm', 'minilm', 'mllm', 'multimodal', 'nlp', 'pre-trained-model', 'textdiffuser', 'trocr', 'unilm', 'xlm-e'], 'https://github.com/microsoft/semantic-kernel': ['ai', 'artificial-intelligence', 'llm', 'openai', 'sdk'], 'https://github.com/mudler/LocalAI': ['ai', 'alpaca', 'api', 'api-rest', 'bloom', 'containers', 'coqui', 'falcon', 'gpt-neox', 'gpt4all', 'guanaco', 'kubernetes', 'llama', 'llm', 'mamba', 'musicgen', 'rwkv', 'stable-diffusion', 'tts', 'vicuna'], 'https://github.com/ymcui/Chinese-LLaMA-Alpaca': ['alpaca', 'alpaca-2', 'large-language-models', 'llama', 'llama-2', 'llm', 'lora', 'nlp', 'plm', 'pre-trained-language-models', 'quantization'], 'https://github.com/mlc-ai/mlc-llm': ['language-model', 'llm', 'machine-learning-compilation', 'tvm'], 'https://github.com/langgenius/dify': ['ai', 'backend-as-a-service', 'gpt', 'gpt-4', 'langchain', 'llama2', 'llm', 'openai', 'orchestration', 'python', 'rag'], 'https://github.com/THUDM/ChatGLM2-6B': ['chatglm', 'chatglm-6b', 'large-language-models', 'llm'], 'https://github.com/vllm-project/vllm': ['gpt', 'inference', 'llama', 'llm', 'llm-serving', 'llmops', 'mlops', 'model-serving', 'pytorch', 'transformer'], 'https://github.com/TransformerOptimus/SuperAGI': ['agents', 'agi', 'ai', 'artificial-general-intelligence', 'artificial-intelligence', 'autonomous-agents', 'gpt-4', 'hacktoberfest', 'llm', 'llmops', 'nextjs', 'openai', 'pinecone', 'python', 'superagi'], 'https://github.com/cocktailpeanut/dalai': ['ai', 'llama', 'llm'], 'https://github.com/huggingface/peft': ['adapter', 'diffusion', 'llm', 'lora', 'parameter-efficient-learning', 'python', 'pytorch', 'transformers'], 'https://github.com/botpress/botpress': ['agent', 'ai', 'botpress', 'chatbot', 'chatgpt', 'gpt', 'gpt-4', 'langchain', 'llm', 'nlp', 'openai', 'prompt'], 'https://github.com/hiyouga/LLaMA-Factory': ['agent', 'baichuan', 'chatglm', 'fine-tuning', 'generative-ai', 'gpt', 'instruction-tuning', 'language-model', 'large-language-models', 'llama', 'llm', 'lora', 'mistral', 'mixture-of-experts', 'peft', 'qlora', 'quantization', 'qwen', 'rlhf', 'transformers'], 'https://github.com/PaddlePaddle/PaddleNLP': ['bert', 'compression', 'distributed-training', 'document-intelligence', 'embedding', 'ernie', 'information-extraction', 'llama', 'llm', 'neural-search', 'nlp', 'paddlenlp', 'pretrained-models', 'question-answering', 'search-engine', 'semantic-analysis', 'sentiment-analysis', 'transformers', 'uie'], 'https://github.com/ludwig-ai/ludwig': ['computer-vision', 'data-centric', 'data-science', 'deep', 'deep-learning', 'deeplearning', 'fine-tuning', 'learning', 'llama', 'llama2', 'llm', 'llm-training', 'machine-learning', 'machinelearning', 'mistral', 'ml', 'natural-language', 'natural-language-processing', 'neural-network', 'pytorch'], 'https://github.com/getumbrel/llama-gpt': ['ai', 'chatgpt', 'code-llama', 'codellama', 'gpt', 'gpt-4', 'gpt4all', 'llama', 'llama-2', 'llama-cpp', 'llama2', 'llamacpp', 'llm', 'localai', 'openai', 'self-hosted'], 'https://github.com/eosphoros-ai/DB-GPT': ['agents', 'bgi', 'database', 'gpt', 'gpt-4', 'langchain', 'llm', 'private', 'rag', 'security', 'vicuna'], 'https://github.com/gventuri/pandas-ai': ['ai', 'csv', 'data', 'data-analysis', 'data-science', 'gpt-3', 'gpt-4', 'llm', 'pandas', 'sql'], 'https://github.com/h2oai/h2ogpt': ['ai', 'chatgpt', 'embeddings', 'generative', 'gpt', 'gpt4all', 'llama2', 'llm', 'mixtral', 'pdf', 'private', 'privategpt', 'vectorstore'], 'https://github.com/labring/FastGPT': ['gpt', 'gpt-35-turbo', 'gpt-4', 'gpt35', 'llm', 'nextjs', 'openai', 'rag', 'react'], 'https://github.com/eugeneyan/open-llms': ['commercial', 'large-language-models', 'llm', 'llms'], 'https://github.com/ShishirPatil/gorilla': ['api', 'api-documentation', 'chatgpt', 'claude-api', 'gpt-4-api', 'llm', 'openai-api', 'openai-functions'], 'https://github.com/QwenLM/Qwen': ['chinese', 'flash-attention', 'large-language-models', 'llm', 'natural-language-processing', 'pretrained-models'], 'https://github.com/rasbt/LLMs-from-scratch': ['chatgpt', 'gpt', 'large-language-models', 'llm', 'python', 'pytorch'], 'https://github.com/mlc-ai/web-llm': ['chatgpt', 'deep-learning', 'language-model', 'llm', 'tvm', 'webgpu', 'webml'], 'https://github.com/nebuly-ai/nebuly': ['ai', 'analytics', 'artificial-intelligence', 'deeplearning', 'large-language-models', 'llm'], 'https://github.com/LlamaFamily/Llama2-Chinese': ['finetune', 'llama', 'llama2', 'llm', 'lora', 'pretrain'], 'https://github.com/bentoml/OpenLLM': ['ai', 'bentoml', 'falcon', 'fine-tuning', 'llama', 'llama2', 'llm', 'llm-inference', 'llm-ops', 'llm-serving', 'llmops', 'mistral', 'ml', 'mlops', 'model-inference', 'mpt', 'open-source-llm', 'openllm', 'stablelm', 'vicuna'], 'https://github.com/cpacker/MemGPT': ['chat', 'chatbot', 'gpt', 'gpt-4', 'llm', 'llm-agent'], 'https://github.com/stas00/ml-engineering': ['ai', 'bash', 'large-language-models', 'llm', 'machine-learning', 'machine-learning-engineering', 'make', 'mlops', 'python', 'pytorch', 'scalability', 'slurm', 'transformers'], 'https://github.com/RUCAIBox/LLMSurvey': ['chain-of-thought', 'chatgpt', 'in-context-learning', 'instruction-tuning', 'large-language-models', 'llm', 'llms', 'natural-language-processing', 'pre-trained-language-models', 'pre-training', 'rlhf'], 'https://github.com/embedchain/embedchain': ['ai', 'application', 'chatbots', 'chatgpt', 'embeddings', 'llm', 'python', 'rag', 'vector-database'], 'https://github.com/activeloopai/deeplake': ['ai', 'computer-vision', 'cv', 'data-science', 'data-version-control', 'datalake', 'datasets', 'deep-learning', 'image-processing', 'langchain', 'large-language-models', 'llm', 'machine-learning', 'ml', 'mlops', 'python', 'pytorch', 'tensorflow', 'vector-database', 'vector-search'], 'https://github.com/microsoft/TypeChat': ['ai', 'llm', 'natural-language', 'types'], 'https://github.com/mistralai/mistral-src': ['llm', 'llm-inference', 'mistralai'], 'https://github.com/facebookresearch/llama-recipes': ['ai', 'finetuning', 'langchain', 'llama', 'llama2', 'llm', 'machine-learning', 'python', 'pytorch', 'vllm'], 'https://github.com/microsoft/promptflow': ['ai', 'ai-application-development', 'ai-applications', 'chatgpt', 'gpt', 'llm', 'prompt', 'prompt-engineering'], 'https://github.com/TheR1D/shell_gpt': ['chatgpt', 'cheat-sheet', 'cli', 'commands', 'gpt-3', 'gpt-4', 'linux', 'llm', 'openai', 'productivity', 'python', 'shell', 'terminal'], 'https://github.com/continuedev/continue': ['ai', 'chatgpt', 'copilot', 'developer-tools', 'intellij', 'jetbrains', 'llm', 'open-source', 'openai', 'pycharm', 'software-development', 'visual-studio-code', 'vscode']}\n"
     ]
    }
   ],
   "source": [
    "print(urls_with_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ccugutrillague/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ccugutrillague/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ccugutrillague/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import nltk\n",
    "import json\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def fetch_readme_content(repo_url):\n",
    "    readme_url = repo_url.replace(\"github.com\", \"raw.githubusercontent.com\") + \"/main/README.md\"\n",
    "    response = requests.get(readme_url)\n",
    "    return response.text if response.status_code == 200 else None\n",
    "# todo: split text into sentences; tokenize the text; lemmatize and lowercase all tokens; remove stop words (preprocessing)\n",
    "\n",
    "def extract_installation_instructions(readme_content):\n",
    "    keywords = [\"installation\", \"setup\", \"install\", \"how to\", \"getting started\", \"quick start\"]\n",
    "    pattern = re.compile(\"|\".join(keywords), re.IGNORECASE)\n",
    "    sections = re.split(r'#+ ', readme_content)\n",
    "    installation_sections = [section for section in sections if pattern.search(section)]\n",
    "    return installation_sections\n",
    "\n",
    "# def tokenize_text(text):\n",
    "#     return word_tokenize(text)\n",
    "def process_text(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    processed_sentences = []\n",
    "    processed_tokens = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        tokens = word_tokenize(sentence)\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens if token.lower() not in stop_words]\n",
    "        \n",
    "        processed_sentences.append(' '.join(lemmatized_tokens))  # Join tokens to form the processed sentence\n",
    "        processed_tokens.extend(lemmatized_tokens)  # Extend the list of processed tokens\n",
    "    \n",
    "    return processed_tokens, processed_sentences\n",
    "\n",
    "def classify_complexity(text):\n",
    "    complexity = -1  # Default complexity\n",
    "    if any(word in text for word in [\"pip install\", \"package manager install\"]):\n",
    "        complexity = 0\n",
    "    elif any(word in text for word in [\"container\", \"docker container\", \"docker compose up\"]):\n",
    "        complexity = 1\n",
    "    elif any(word in text for word in [\"from source\", \"git clone\", \".git\"]):\n",
    "        complexity = 2\n",
    "    return complexity\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# def cluster_data(texts, topics, sentence_lengths, code_counts):\n",
    "#     # Vectorize the texts and topics\n",
    "#     vectorizer = TfidfVectorizer()\n",
    "#     text_features = vectorizer.fit_transform(texts + topics).toarray()\n",
    "    \n",
    "#     # Combine all features\n",
    "#     features = np.hstack((text_features, np.array(sentence_lengths).reshape(-1, 1), np.array(code_counts).reshape(-1, 1)))\n",
    "    \n",
    "#     # Optional: Apply PCA for dimensionality reduction\n",
    "#     pca = PCA(n_components=0.95)  # Keep 95% of variance\n",
    "#     reduced_features = pca.fit_transform(features)\n",
    "    \n",
    "#     # Cluster using Gaussian Mixture Model\n",
    "#     gmm = GaussianMixture(n_components=3)  # We want to cluster into 3 groups\n",
    "#     gmm.fit(reduced_features)\n",
    "#     cluster_labels = gmm.predict(reduced_features)\n",
    "#     probabilities = gmm.predict_proba(reduced_features)\n",
    "    \n",
    "#     return cluster_labels, probabilities\n",
    "\n",
    "# Iterate through the URLs and perform the tasks\n",
    "output_data = []\n",
    "\n",
    "for url, topics in urls_with_topics.items():\n",
    "    readme_url = url.replace(\"github.com\", \"raw.githubusercontent.com\") + \"/main/README.md\"  # Define readme_url for each repository\n",
    "    readme_content = fetch_readme_content(url)\n",
    "    if readme_content:\n",
    "        installation_instructions = extract_installation_instructions(readme_content)\n",
    "        for instruction in installation_instructions:\n",
    "            tokens = process_text(instruction)\n",
    "            sentence = process_text(instruction)\n",
    "            complexity = classify_complexity(instruction)\n",
    "            # cluster_labels, probabilities = cluster_data([instruction], topics, [len(sentence)], [len(tokens)])\n",
    "            output_data.append({\n",
    "                \"url\": url,\n",
    "                \"readme_url\": readme_url,\n",
    "                \"topic\": topics,\n",
    "                \"text\": instruction,\n",
    "                'sentence': sentence,\n",
    "                \"token\": tokens,\n",
    "                # \"cluster_labels\": cluster_labels.tolist(),\n",
    "                # \"probabilities\": probabilities.tolist()\n",
    "                \"level of complexity\": complexity\n",
    "            })\n",
    "\n",
    "# Output to a JSON file\n",
    "with open('data/corpus-topic3.json', 'w') as outfile:\n",
    "    json.dump(output_data, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting paperswithcode-client\n",
      "  Downloading paperswithcode_client-0.3.1-py3-none-any.whl (24 kB)\n",
      "Collecting tea-client==0.0.7 (from paperswithcode-client)\n",
      "  Downloading tea_client-0.0.7-py3-none-any.whl (11 kB)\n",
      "Collecting tea-console==0.0.6 (from paperswithcode-client)\n",
      "  Downloading tea_console-0.0.6-py3-none-any.whl (12 kB)\n",
      "Collecting typer==0.3.2 (from paperswithcode-client)\n",
      "  Downloading typer-0.3.2-py3-none-any.whl (21 kB)\n",
      "Collecting httpx~=0.14.2 (from httpx[http2]~=0.14.2->tea-client==0.0.7->paperswithcode-client)\n",
      "  Downloading httpx-0.14.3-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic~=1.6.1 (from tea-client==0.0.7->paperswithcode-client)\n",
      "  Downloading pydantic-1.6.2-py36.py37.py38-none-any.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.3/99.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tea~=0.1.2 (from tea-client==0.0.7->paperswithcode-client)\n",
      "  Downloading tea-0.1.7-py3-none-any.whl (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pytz~=2021.1 (from tea-console==0.0.6->paperswithcode-client)\n",
      "  Downloading pytz-2021.3-py2.py3-none-any.whl (503 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m503.5/503.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting rich~=9.11.0 (from tea-console==0.0.6->paperswithcode-client)\n",
      "  Downloading rich-9.11.1-py3-none-any.whl (195 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.6/195.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tzlocal~=2.1 (from tea-console==0.0.6->paperswithcode-client)\n",
      "  Downloading tzlocal-2.1-py2.py3-none-any.whl (16 kB)\n",
      "Collecting click<7.2.0,>=7.1.1 (from typer==0.3.2->paperswithcode-client)\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.8/82.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi in /Users/ccugutrillague/anaconda3/lib/python3.11/site-packages (from httpx~=0.14.2->httpx[http2]~=0.14.2->tea-client==0.0.7->paperswithcode-client) (2023.11.17)\n",
      "Requirement already satisfied: sniffio in /Users/ccugutrillague/anaconda3/lib/python3.11/site-packages (from httpx~=0.14.2->httpx[http2]~=0.14.2->tea-client==0.0.7->paperswithcode-client) (1.3.0)\n",
      "Collecting chardet==3.* (from httpx~=0.14.2->httpx[http2]~=0.14.2->tea-client==0.0.7->paperswithcode-client)\n",
      "  Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "Collecting rfc3986<2,>=1.3 (from rfc3986[idna2008]<2,>=1.3->httpx~=0.14.2->httpx[http2]~=0.14.2->tea-client==0.0.7->paperswithcode-client)\n",
      "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
      "Collecting httpcore==0.10.* (from httpx~=0.14.2->httpx[http2]~=0.14.2->tea-client==0.0.7->paperswithcode-client)\n",
      "  Downloading httpcore-0.10.2-py3-none-any.whl (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting h11<0.10,>=0.8 (from httpcore==0.10.*->httpx~=0.14.2->httpx[http2]~=0.14.2->tea-client==0.0.7->paperswithcode-client)\n",
      "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting h2==3.* (from httpx[http2]~=0.14.2->tea-client==0.0.7->paperswithcode-client)\n",
      "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting hyperframe<6,>=5.2.0 (from h2==3.*->httpx[http2]~=0.14.2->tea-client==0.0.7->paperswithcode-client)\n",
      "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting hpack<4,>=3.0 (from h2==3.*->httpx[http2]~=0.14.2->tea-client==0.0.7->paperswithcode-client)\n",
      "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.0 in /Users/ccugutrillague/anaconda3/lib/python3.11/site-packages (from rich~=9.11.0->tea-console==0.0.6->paperswithcode-client) (0.4.6)\n",
      "Collecting commonmark<0.10.0,>=0.9.0 (from rich~=9.11.0->tea-console==0.0.6->paperswithcode-client)\n",
      "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /Users/ccugutrillague/anaconda3/lib/python3.11/site-packages (from rich~=9.11.0->tea-console==0.0.6->paperswithcode-client) (2.17.2)\n",
      "Collecting typing-extensions<4.0.0,>=3.7.4 (from rich~=9.11.0->tea-console==0.0.6->paperswithcode-client)\n",
      "  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
      "Collecting psutil~=5.8.0 (from tea~=0.1.2->tea-client==0.0.7->paperswithcode-client)\n",
      "  Downloading psutil-5.8.0.tar.gz (470 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m470.9/470.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of tea to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tea~=0.1.2 (from tea-client==0.0.7->paperswithcode-client)\n",
      "  Downloading tea-0.1.6-py3-none-any.whl (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading tea-0.1.5-py3-none-any.whl (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading tea-0.1.4-py3-none-any.whl (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.4/41.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: idna in /Users/ccugutrillague/anaconda3/lib/python3.11/site-packages (from rfc3986[idna2008]<2,>=1.3->httpx~=0.14.2->httpx[http2]~=0.14.2->tea-client==0.0.7->paperswithcode-client) (3.4)\n",
      "Building wheels for collected packages: psutil\n",
      "  Building wheel for psutil (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for psutil: filename=psutil-5.8.0-cp311-cp311-macosx_11_0_arm64.whl size=234465 sha256=ba7a9857b541121762e3b988fa6d9373e3d9c441b0fff07e972ea555be64c390\n",
      "  Stored in directory: /Users/ccugutrillague/Library/Caches/pip/wheels/42/f0/f8/87fba1d4d16d4a77492a8b9e047bd4da74daf2312115d01fb0\n",
      "Successfully built psutil\n",
      "Installing collected packages: typing-extensions, rfc3986, pytz, hyperframe, hpack, h11, commonmark, chardet, tzlocal, rich, pydantic, psutil, httpcore, h2, click, typer, tea, httpx, tea-console, tea-client, paperswithcode-client\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.8.0\n",
      "    Uninstalling typing_extensions-4.8.0:\n",
      "      Successfully uninstalled typing_extensions-4.8.0\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2023.3.post1\n",
      "    Uninstalling pytz-2023.3.post1:\n",
      "      Successfully uninstalled pytz-2023.3.post1\n",
      "  Attempting uninstall: hyperframe\n",
      "    Found existing installation: hyperframe 6.0.1\n",
      "    Uninstalling hyperframe-6.0.1:\n",
      "      Successfully uninstalled hyperframe-6.0.1\n",
      "  Attempting uninstall: hpack\n",
      "    Found existing installation: hpack 4.0.0\n",
      "    Uninstalling hpack-4.0.0:\n",
      "      Successfully uninstalled hpack-4.0.0\n",
      "  Attempting uninstall: h11\n",
      "    Found existing installation: h11 0.14.0\n",
      "    Uninstalling h11-0.14.0:\n",
      "      Successfully uninstalled h11-0.14.0\n",
      "  Attempting uninstall: chardet\n",
      "    Found existing installation: chardet 5.2.0\n",
      "    Uninstalling chardet-5.2.0:\n",
      "      Successfully uninstalled chardet-5.2.0\n",
      "  Attempting uninstall: rich\n",
      "    Found existing installation: rich 13.7.0\n",
      "    Uninstalling rich-13.7.0:\n",
      "      Successfully uninstalled rich-13.7.0\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.10.13\n",
      "    Uninstalling pydantic-1.10.13:\n",
      "      Successfully uninstalled pydantic-1.10.13\n",
      "  Attempting uninstall: psutil\n",
      "    Found existing installation: psutil 5.9.5\n",
      "    Uninstalling psutil-5.9.5:\n",
      "      Successfully uninstalled psutil-5.9.5\n",
      "  Attempting uninstall: httpcore\n",
      "    Found existing installation: httpcore 0.17.3\n",
      "    Uninstalling httpcore-0.17.3:\n",
      "      Successfully uninstalled httpcore-0.17.3\n",
      "  Attempting uninstall: h2\n",
      "    Found existing installation: h2 4.1.0\n",
      "    Uninstalling h2-4.1.0:\n",
      "      Successfully uninstalled h2-4.1.0\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 8.1.7\n",
      "    Uninstalling click-8.1.7:\n",
      "      Successfully uninstalled click-8.1.7\n",
      "  Attempting uninstall: typer\n",
      "    Found existing installation: typer 0.9.0\n",
      "    Uninstalling typer-0.9.0:\n",
      "      Successfully uninstalled typer-0.9.0\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.24.1\n",
      "    Uninstalling httpx-0.24.1:\n",
      "      Successfully uninstalled httpx-0.24.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "conda-repo-cli 1.0.75 requires requests_mock, which is not installed.\n",
      "dask 2023.6.0 requires click>=8.0, but you have click 7.1.2 which is incompatible.\n",
      "linkml-runtime 1.6.0 requires pydantic<3.0.0,>=1.10.2, but you have pydantic 1.6.2 which is incompatible.\n",
      "mlem 0.4.14 requires pydantic<2,>=1.9.0, but you have pydantic 1.6.2 which is incompatible.\n",
      "readmeai 0.4.49 requires click<9.0.0,>=8.1.7, but you have click 7.1.2 which is incompatible.\n",
      "readmeai 0.4.49 requires h2<5.0.0,>=4.1.0, but you have h2 3.2.0 which is incompatible.\n",
      "readmeai 0.4.49 requires httpx<0.25.0,>=0.24.1, but you have httpx 0.14.3 which is incompatible.\n",
      "readmeai 0.4.49 requires openai<0.28.0,>=0.27.8, but you have openai 0.28.1 which is incompatible.\n",
      "readmeai 0.4.49 requires pydantic<2.0.0,>=1.10.9, but you have pydantic 1.6.2 which is incompatible.\n",
      "readmeai 0.4.49 requires responses<0.24.0,>=0.23.1, but you have responses 0.18.0 which is incompatible.\n",
      "prefixmaps 0.1.7 requires click>=8.1.3, but you have click 7.1.2 which is incompatible.\n",
      "prefixmaps 0.1.7 requires pydantic<2.0.0,>=1.8.2, but you have pydantic 1.6.2 which is incompatible.\n",
      "confection 0.1.4 requires pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4, but you have pydantic 1.6.2 which is incompatible.\n",
      "gto 1.5.0 requires pydantic!=2.0.0,<3,>=1.9.0, but you have pydantic 1.6.2 which is incompatible.\n",
      "gto 1.5.0 requires typer>=0.4.1, but you have typer 0.3.2 which is incompatible.\n",
      "celery 5.3.6 requires click<9.0,>=8.1.2, but you have click 7.1.2 which is incompatible.\n",
      "conda-repo-cli 1.0.75 requires clyent==1.2.1, but you have clyent 1.2.2 which is incompatible.\n",
      "spacy 3.7.2 requires pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4, but you have pydantic 1.6.2 which is incompatible.\n",
      "trapi-predict-kit 0.2.2 requires pydantic>=1.9, but you have pydantic 1.6.2 which is incompatible.\n",
      "qdrant-client 1.6.9 requires pydantic>=1.10.8, but you have pydantic 1.6.2 which is incompatible.\n",
      "thinc 8.2.2 requires pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4, but you have pydantic 1.6.2 which is incompatible.\n",
      "weasel 0.3.4 requires pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4, but you have pydantic 1.6.2 which is incompatible.\n",
      "argilla 1.22.0 requires httpx<=0.25,>=0.15, but you have httpx 0.14.3 which is incompatible.\n",
      "argilla 1.22.0 requires pydantic>=1.10.7, but you have pydantic 1.6.2 which is incompatible.\n",
      "argilla 1.22.0 requires typer<0.10.0,>=0.6.0, but you have typer 0.3.2 which is incompatible.\n",
      "conda-index 0.3.0 requires click>=8, but you have click 7.1.2 which is incompatible.\n",
      "tox 4.11.4 requires chardet>=5.2, but you have chardet 3.0.4 which is incompatible.\n",
      "dvc 3.31.2 requires rich>=12, but you have rich 9.11.1 which is incompatible.\n",
      "reasoner-pydantic 4.1.5 requires pydantic<2,>=1.8, but you have pydantic 1.6.2 which is incompatible.\n",
      "black 0.0 requires click>=8.0.0, but you have click 7.1.2 which is incompatible.\n",
      "sqlalchemy 2.0.23 requires typing-extensions>=4.2.0, but you have typing-extensions 3.10.0.2 which is incompatible.\n",
      "python-lsp-black 1.2.1 requires black>=22.3.0, but you have black 0.0 which is incompatible.\n",
      "prefixcommons 0.1.12 requires click<9.0.0,>=8.1.3, but you have click 7.1.2 which is incompatible.\n",
      "flask 2.2.2 requires click>=8.0, but you have click 7.1.2 which is incompatible.\n",
      "fastapi 0.104.1 requires pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4, but you have pydantic 1.6.2 which is incompatible.\n",
      "fastapi 0.104.1 requires typing-extensions>=4.8.0, but you have typing-extensions 3.10.0.2 which is incompatible.\n",
      "distributed 2023.6.0 requires click>=8.0, but you have click 7.1.2 which is incompatible.\n",
      "flufl-lock 7.1.1 requires psutil>=5.9.0, but you have psutil 5.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed chardet-3.0.4 click-7.1.2 commonmark-0.9.1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 httpcore-0.10.2 httpx-0.14.3 hyperframe-5.2.0 paperswithcode-client-0.3.1 psutil-5.8.0 pydantic-1.6.2 pytz-2021.3 rfc3986-1.5.0 rich-9.11.1 tea-0.1.4 tea-client-0.0.7 tea-console-0.0.6 typer-0.3.2 typing-extensions-3.10.0.2 tzlocal-2.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install paperswithcode-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ccugutrillague/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ccugutrillague/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ccugutrillague/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docker: Cannot connect to the Docker daemon at unix:///Users/ccugutrillague/.docker/run/docker.sock. Is the docker daemon running?.\n",
      "See 'docker run --help'.\n",
      "\n",
      "docker: Cannot connect to the Docker daemon at unix:///Users/ccugutrillague/.docker/run/docker.sock. Is the docker daemon running?.\n",
      "See 'docker run --help'.\n",
      "\n",
      "docker: Cannot connect to the Docker daemon at unix:///Users/ccugutrillague/.docker/run/docker.sock. Is the docker daemon running?.\n",
      "See 'docker run --help'.\n",
      "\n",
      "docker: Cannot connect to the Docker daemon at unix:///Users/ccugutrillague/.docker/run/docker.sock. Is the docker daemon running?.\n",
      "See 'docker run --help'.\n",
      "\n",
      "docker: Cannot connect to the Docker daemon at unix:///Users/ccugutrillague/.docker/run/docker.sock. Is the docker daemon running?.\n",
      "See 'docker run --help'.\n",
      "\n",
      "docker: Cannot connect to the Docker daemon at unix:///Users/ccugutrillague/.docker/run/docker.sock. Is the docker daemon running?.\n",
      "See 'docker run --help'.\n",
      "\n",
      "docker: Cannot connect to the Docker daemon at unix:///Users/ccugutrillague/.docker/run/docker.sock. Is the docker daemon running?.\n",
      "See 'docker run --help'.\n",
      "\n",
      "docker: Cannot connect to the Docker daemon at unix:///Users/ccugutrillague/.docker/run/docker.sock. Is the docker daemon running?.\n",
      "See 'docker run --help'.\n",
      "\n",
      "docker: Cannot connect to the Docker daemon at unix:///Users/ccugutrillague/.docker/run/docker.sock. Is the docker daemon running?.\n",
      "See 'docker run --help'.\n",
      "\n",
      "docker: Cannot connect to the Docker daemon at unix:///Users/ccugutrillague/.docker/run/docker.sock. Is the docker daemon running?.\n",
      "See 'docker run --help'.\n",
      "\n",
      "docker: Cannot connect to the Docker daemon at unix:///Users/ccugutrillague/.docker/run/docker.sock. Is the docker daemon running?.\n",
      "See 'docker run --help'.\n",
      "\n",
      "docker: Cannot connect to the Docker daemon at unix:///Users/ccugutrillague/.docker/run/docker.sock. Is the docker daemon running?.\n",
      "See 'docker run --help'.\n",
      "\n",
      "docker: Cannot connect to the Docker daemon at unix:///Users/ccugutrillague/.docker/run/docker.sock. Is the docker daemon running?.\n",
      "See 'docker run --help'.\n",
      "\n",
      "docker: Cannot connect to the Docker daemon at unix:///Users/ccugutrillague/.docker/run/docker.sock. Is the docker daemon running?.\n",
      "See 'docker run --help'.\n",
      "\n",
      "docker: Cannot connect to the Docker daemon at unix:///Users/ccugutrillague/.docker/run/docker.sock. Is the docker daemon running?.\n",
      "See 'docker run --help'.\n",
      "\n",
      "docker: Cannot connect to the Docker daemon at unix:///Users/ccugutrillague/.docker/run/docker.sock. Is the docker daemon running?.\n",
      "See 'docker run --help'.\n",
      "\n",
      "docker: Cannot connect to the Docker daemon at unix:///Users/ccugutrillague/.docker/run/docker.sock. Is the docker daemon running?.\n",
      "See 'docker run --help'.\n",
      "\n",
      "docker: Cannot connect to the Docker daemon at unix:///Users/ccugutrillague/.docker/run/docker.sock. Is the docker daemon running?.\n",
      "See 'docker run --help'.\n",
      "\n",
      "docker: Cannot connect to the Docker daemon at unix:///Users/ccugutrillague/.docker/run/docker.sock. Is the docker daemon running?.\n",
      "See 'docker run --help'.\n",
      "\n",
      "docker: Cannot connect to the Docker daemon at unix:///Users/ccugutrillague/.docker/run/docker.sock. Is the docker daemon running?.\n",
      "See 'docker run --help'.\n",
      "\n",
      "docker: Cannot connect to the Docker daemon at unix:///Users/ccugutrillague/.docker/run/docker.sock. Is the docker daemon running?.\n",
      "See 'docker run --help'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## with paperswithcode\n",
    "\n",
    "import json\n",
    "import csv\n",
    "import subprocess\n",
    "from paperswithcode import PapersWithCodeClient\n",
    "import requests\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "client = PapersWithCodeClient()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def get_readme_content(repo_url):\n",
    "    readme_url = repo_url.replace(\"github.com\", \"raw.githubusercontent.com\") + \"/main/README.md\"\n",
    "    response = requests.get(readme_url)\n",
    "    return response.text if response.status_code == 200 else None\n",
    "\n",
    "def extract_installation_instruction(readme_content):\n",
    "    # Call SOMEF via Docker to extract installation instructions\n",
    "    command = f\"docker run -i kcapd/somef describe -t 0.8 -o -\"\n",
    "    process = subprocess.Popen(command, shell=True, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    stdout, stderr = process.communicate(input=readme_content.encode())\n",
    "    if process.returncode == 0:\n",
    "        return json.loads(stdout.decode())['installation instructions']\n",
    "    else:\n",
    "        print(stderr.decode())\n",
    "        return None\n",
    "\n",
    "def preprocess_data(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    lemmatized_tokens = [[lemmatizer.lemmatize(token.lower()) for token in sentence if token.lower() not in stop_words] for sentence in tokens]\n",
    "    return sentences, lemmatized_tokens\n",
    "\n",
    "def classify_complexity(text):\n",
    "    # Example complexity classification based on text length\n",
    "    length = len(text)\n",
    "    if length < 100:\n",
    "        return 'Low'\n",
    "    elif length < 500:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'High'\n",
    "\n",
    "output_data = []\n",
    "\n",
    "# Fetch repositories from PapersWithCode\n",
    "repositories = client.repository_list()\n",
    "\n",
    "for repo in repositories.results:\n",
    "    repo_url = repo.url\n",
    "    readme_content = get_readme_content(repo_url)\n",
    "    if readme_content:\n",
    "        installation_instructions = extract_installation_instruction(readme_content)\n",
    "        if installation_instructions:\n",
    "            sentences, tokens = preprocess_data(installation_instructions)\n",
    "            complexity = classify_complexity(installation_instructions)\n",
    "            output_data.append({\n",
    "                \"url\": repo_url,\n",
    "                \"readme_url\": repo_url.replace(\"github.com\", \"raw.githubusercontent.com\") + \"/main/README.md\",\n",
    "                \"text\": installation_instructions,\n",
    "                \"sentence\": sentences,\n",
    "                \"tokens\": tokens,\n",
    "                \"level_complexity\": complexity\n",
    "            })\n",
    "\n",
    "# Output to JSON file\n",
    "with open('output_data.json', 'w') as jsonfile:\n",
    "    json.dump(output_data, jsonfile, indent=4)\n",
    "\n",
    "# Output to CSV file\n",
    "with open('output_data.csv', 'w', newline='') as csvfile:\n",
    "    fieldnames = ['url', 'readme_url', 'text', 'sentence', 'tokens', 'level_complexity']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for data in output_data:\n",
    "        writer.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch repositories. Check if the 'paperswithcode-client' library has been updated.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'repositories' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to fetch repositories. Check if the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpaperswithcode-client\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m library has been updated.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Handle the error or exit\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m repo \u001b[38;5;129;01min\u001b[39;00m \u001b[43mrepositories\u001b[49m\u001b[38;5;241m.\u001b[39mresults:\n\u001b[1;32m     11\u001b[0m     repo_url \u001b[38;5;241m=\u001b[39m repo\u001b[38;5;241m.\u001b[39mrepository_link\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Continue with your logic here\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'repositories' is not defined"
     ]
    }
   ],
   "source": [
    "output_data = []\n",
    "\n",
    "# Fetch repositories from PapersWithCode\n",
    "try:\n",
    "    repositories = client.repositories.list()  # Assuming this is the correct method based on the pattern\n",
    "except AttributeError:\n",
    "    print(\"Failed to fetch repositories. Check if the 'paperswithcode-client' library has been updated.\")\n",
    "    # Handle the error or exit\n",
    "\n",
    "for repo in repositories.results:\n",
    "    repo_url = repo.repository_link\n",
    "    # Continue with your logic here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from paperswithcode import PapersWithCodeClient\n",
    "\n",
    "client = PapersWithCodeClient()\n",
    "papers = client.repository_list()\n",
    "print(papers.next_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper = papers.results[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Repository(url='https://github.com/facebook/react', owner='facebook', name='react', description='The library for web and native user interfaces.', stars=218380, framework='none', is_official=None)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
