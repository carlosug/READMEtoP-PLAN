{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.1.2 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (2.1.2)\n",
      "Requirement already satisfied: tensorboard in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (2.14.0)\n",
      "Requirement already satisfied: filelock in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from torch==2.1.2) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from torch==2.1.2) (4.9.0)\n",
      "Requirement already satisfied: sympy in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from torch==2.1.2) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from torch==2.1.2) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from torch==2.1.2) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from torch==2.1.2) (2023.10.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from tensorboard) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from tensorboard) (1.60.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from tensorboard) (2.28.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from tensorboard) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from tensorboard) (3.5.2)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from tensorboard) (1.24.4)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from tensorboard) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from tensorboard) (2.31.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from tensorboard) (69.0.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from tensorboard) (3.0.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from tensorboard) (0.42.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard) (7.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from sympy->torch==2.1.2) (1.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.17.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard) (3.2.2)\n",
      "Collecting transformers==4.36.2\n",
      "  Using cached transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n",
      "Requirement already satisfied: datasets==2.16.1 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (2.16.1)\n",
      "Collecting accelerate==0.26.1\n",
      "  Using cached accelerate-0.26.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: evaluate==0.4.1 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (0.4.1)\n",
      "Requirement already satisfied: bitsandbytes==0.42.0 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (0.42.0)\n",
      "Requirement already satisfied: filelock in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from transformers==4.36.2) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from transformers==4.36.2) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from transformers==4.36.2) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from transformers==4.36.2) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from transformers==4.36.2) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from transformers==4.36.2) (2023.12.25)\n",
      "Requirement already satisfied: requests in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from transformers==4.36.2) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from transformers==4.36.2) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from transformers==4.36.2) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from transformers==4.36.2) (4.66.2)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from datasets==2.16.1) (15.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from datasets==2.16.1) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from datasets==2.16.1) (0.3.7)\n",
      "Requirement already satisfied: pandas in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from datasets==2.16.1) (2.0.3)\n",
      "Requirement already satisfied: xxhash in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from datasets==2.16.1) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from datasets==2.16.1) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.16.1) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from datasets==2.16.1) (3.9.3)\n",
      "Requirement already satisfied: psutil in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from accelerate==0.26.1) (5.9.8)\n",
      "Requirement already satisfied: torch>=1.10.0 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from accelerate==0.26.1) (2.1.2)\n",
      "Requirement already satisfied: responses<0.19 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from evaluate==0.4.1) (0.18.0)\n",
      "Requirement already satisfied: scipy in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from bitsandbytes==0.42.0) (1.10.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from aiohttp->datasets==2.16.1) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from aiohttp->datasets==2.16.1) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from aiohttp->datasets==2.16.1) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from aiohttp->datasets==2.16.1) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from aiohttp->datasets==2.16.1) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from aiohttp->datasets==2.16.1) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.2) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from requests->transformers==4.36.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from requests->transformers==4.36.2) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from requests->transformers==4.36.2) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from requests->transformers==4.36.2) (2024.2.2)\n",
      "Requirement already satisfied: sympy in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from torch>=1.10.0->accelerate==0.26.1) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from torch>=1.10.0->accelerate==0.26.1) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from torch>=1.10.0->accelerate==0.26.1) (3.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from pandas->datasets==2.16.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from pandas->datasets==2.16.1) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from pandas->datasets==2.16.1) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.16.1) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from jinja2->torch>=1.10.0->accelerate==0.26.1) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from sympy->torch>=1.10.0->accelerate==0.26.1) (1.3.0)\n",
      "Using cached transformers-4.36.2-py3-none-any.whl (8.2 MB)\n",
      "Using cached accelerate-0.26.1-py3-none-any.whl (270 kB)\n",
      "Installing collected packages: accelerate, transformers\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 0.27.2\n",
      "    Uninstalling accelerate-0.27.2:\n",
      "      Successfully uninstalled accelerate-0.27.2\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.37.2\n",
      "    Uninstalling transformers-4.37.2:\n",
      "      Successfully uninstalled transformers-4.37.2\n",
      "Successfully installed accelerate-0.26.1 transformers-4.36.2\n"
     ]
    }
   ],
   "source": [
    "# Install Pytorch & other libraries\n",
    "!pip install \"torch==2.1.2\" tensorboard\n",
    "\n",
    "# Install Hugging Face libraries\n",
    "!pip install  --upgrade \\\n",
    "  \"transformers==4.36.2\" \\\n",
    "  \"datasets==2.16.1\" \\\n",
    "  \"accelerate==0.26.1\" \\\n",
    "  \"evaluate==0.4.1\" \\\n",
    "  \"bitsandbytes==0.42.0\" \\\n",
    "  # \"trl==0.7.10\" # \\\n",
    "  # \"peft==0.7.1\" \\\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install peft & trl from github\n",
    "# !pip install git+https://github.com/huggingface/trl@a3c5b7178ac4f65569975efadc97db2f3749c65e --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (2.1.2)\n",
      "Requirement already satisfied: torchvision in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (0.17.0)\n",
      "Requirement already satisfied: torchaudio in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (2.2.0)\n",
      "Requirement already satisfied: filelock in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: numpy in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from torchvision) (1.24.4)\n",
      "Requirement already satisfied: requests in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from torchvision) (2.31.0)\n",
      "Collecting torch\n",
      "  Using cached torch-2.2.0-cp38-none-macosx_11_0_arm64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from requests->torchvision) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from requests->torchvision) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from requests->torchvision) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from requests->torchvision) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\n",
      "Using cached torch-2.2.0-cp38-none-macosx_11_0_arm64.whl (59.7 MB)\n",
      "Installing collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.1.2\n",
      "    Uninstalling torch-2.1.2:\n",
      "      Successfully uninstalled torch-2.1.2\n",
      "Successfully installed torch-2.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch; \n",
    "# assert torch.cuda.get_device_capability()[0] >= 8, 'Hardware not supported for Flash Attention'\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# CMAKE_ARGS=\"-DLLAMA_METAL=on\"  FORCE_CMAKE=1 pip install llama-cpp-python==0.1.83 --no-cache-dir\n",
    "device = torch.device('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fused Multiplication Successful!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "tensor_float = torch.rand((1, 1), dtype=torch.float32, device=device)\n",
    "try:\n",
    "    result = tensor_float * tensor_float + tensor_float.abs()\n",
    "except Exception as e:\n",
    "    print(\"Exception occurred:\", str(e))\n",
    "else:\n",
    "    print(\"Fused Multiplication Successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ninja in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (1.11.1.1)\n",
      "Requirement already satisfied: packaging in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (23.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install ninja packaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !MAX_JOBS=4 pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: write).\n",
      "Your token has been saved in your configured git credential helpers (cache,osxkeychain).\n",
      "Your token has been saved to /Users/ccugutrillague/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(\n",
    "  token=\"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\", # ADD YOUR TOKEN HERE\n",
    "  add_to_git_credential=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# # Convert dataset to OAI messages\n",
    "# system_message = \"\"\"You are an readme to instructions query translator. Users will ask you questions in English and you will generate a SPARQL query based on the provided SCHEMA.\n",
    "# SCHEMA:\n",
    "# {schema}\"\"\"\n",
    "\n",
    "# def create_conversation(sample):\n",
    "#   return {\n",
    "#     \"messages\": [\n",
    "#       {\"role\": \"system\", \"content\": system_message.format(schema=sample[\"context\"])},\n",
    "#       {\"role\": \"user\", \"content\": sample[\"question\"]},\n",
    "#       {\"role\": \"assistant\", \"content\": sample[\"answer\"]}\n",
    "#     ]\n",
    "#   } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'Based on the software research tools, I want you generate a list of methods and tasks as part of the installation plans based on a strict JSON format like SCHEMA.\\nSCHEMA:\\n-1', 'role': 'system'}, {'content': 'Model Support\\n\\nMLC LLM supports a wide range of model architectures and variants. We have the following prebuilts which you can\\nuse off-the-shelf. Visit [Prebuilt Models](https://llm.mlc.ai/docs/prebuilt_models.html) to see the full list, and [Compile Models via MLC](https://llm.mlc.ai/docs/compilation/compile_models.html) to see how to use models not on this list.\\n\\n<table style=\"width:100%\">\\n  <thead>\\n    <tr>\\n      <th style=\"width:40%\">Architecture</th>\\n      <th style=\"width:60%\">Prebuilt Model Variants</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <td>Llama</td>\\n      <td>Llama-2, Code Llama, Vicuna, WizardLM, WizardMath, OpenOrca Platypus2, FlagAlpha Llama-2 Chinese, georgesung Llama-2 Uncensored</td>\\n    </tr>\\n    <tr>\\n      <td>GPT-NeoX</td>\\n      <td>RedPajama</td>\\n    </tr>\\n    <tr>\\n      <td>GPT-J</td>\\n      <td></td>\\n    </tr>\\n    <tr>\\n      <td>RWKV</td>\\n      <td>RWKV-raven</td>\\n    </tr>\\n    <tr>\\n      <td>MiniGPT</td>\\n      <td></td>\\n    </tr>\\n    <tr>\\n      <td>GPTBigCode</td>\\n      <td>WizardCoder</td>\\n    </tr>\\n    <tr>\\n      <td>ChatGLM</td>\\n      <td></td>\\n    </tr>\\n    <tr>\\n      <td>StableLM</td>\\n      <td></td>\\n    </tr>\\n    <tr>\\n      <td>Mistral</td>\\n      <td></td>\\n    </tr>\\n    <tr>\\n      <td>Phi</td>\\n      <td></td>\\n    </tr>\\n  </tbody>\\n</table>\\n\\n', 'role': 'RESEARCHER'}, {'content': 'https://raw.githubusercontent.com/mlc-ai/mlc-llm/main/README.md', 'role': 'ASSISANT'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 265.03ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 534.78ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "48514"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Convert dataset to OAI messages\n",
    "system_message = \"\"\"Based on the software research tools, I want you generate a list of methods and tasks as part of the installation plans based on a strict JSON format like SCHEMA.\n",
    "SCHEMA:\n",
    "{schema}\"\"\"\n",
    "# \"\"\"\\n# GOAL #: Based on the software tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. The format must in a strict JSON format, like: {\"task_steps\": [ step description of one or more steps ], \"task_nodes\": [{\"task\": \"tool name must be from # TOOL LIST #\", \"arguments\": [ a concise list of arguments for the tool. Either original text, or user-mentioned filename, or tag '<node-j>' (start from 0) to refer to the output of the j-th node. ]}]} \"\"\"\n",
    "#         prompt += \"\"\"\\n\\n# REQUIREMENTS #: \\n1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #; \\n2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes; \\n3. the dependencies among task steps should align with the argument dependencies of the task nodes; \\n4. the tool arguments should be align with the input-type field of # TASK LIST #;\"\"\"\n",
    "#     else:\n",
    "#         prompt = \"\"\"\\n# GOAL #:\\nBased on the above tools, I want you generate task steps and task nodes to solve the # USER REQUEST #. The format must in a strict JSON format, like: {\"task_steps\": [ \"concrete steps, format as Step x: Call xxx tool with xxx: 'xxx' and xxx: 'xxx'\" ], \"task_nodes\": [{\"task\": \"task name must be from # TASK LIST #\", \"arguments\": [ {\"name\": \"parameter name\", \"value\": \"parameter value, either user-specified text or the specific name of the tool whose result is required by this node\"} ]}], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]}\"\"\"\n",
    "#         prompt += \"\"\"\\n\\n# REQUIREMENTS #: \\n1. the generated task steps and task nodes can resolve the given user request # USER REQUEST # perfectly. Task name must be selected from # TASK LIST #; \\n2. the task steps should strictly aligned with the task nodes, and the number of task steps should be same with the task nodes; \\n3. The task links (task_links) should reflect the temporal dependencies among task nodes, i.e. the order in which the APIs are invoked;\"\"\"\n",
    "\n",
    "def create_conversation(sample):\n",
    "  return {\n",
    "    \"messages\": [\n",
    "      {\"role\": \"system\", \"content\": system_message.format(schema=sample[\"level of complexity\"])},\n",
    "      {\"role\": \"RESEARCHER\", \"content\": sample[\"text\"]}, # research softwares\n",
    "      {\"role\": \"ASSISANT\", \"content\": sample[\"readme_url\"]} #steos\n",
    "    ]\n",
    "  }  \n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"carlosug/ReadmeInstallationPlan\", split=\"train\")\n",
    "# dataset = dataset.shuffle().select(range(800))\n",
    "columns = dataset.column_names\n",
    "columns_to_keep = [\"level of complexity\", \"text\", \"readme_url\"]\n",
    "columns_to_remove = set(columns_to_keep).symmetric_difference(columns)\n",
    "dataset = dataset.remove_columns(columns_to_remove)\n",
    "\n",
    "# Convert dataset to OAI messages\n",
    "dataset = dataset.map(create_conversation, remove_columns=dataset.features,batched=False)\n",
    "# split dataset into 10,000 training samples and 2,500 test samples\n",
    "dataset = dataset.train_test_split(test_size=20/99)\n",
    "\n",
    "print(dataset[\"train\"][3][\"messages\"])\n",
    "\n",
    "# save datasets to disk \n",
    "dataset[\"train\"].to_json(\"train_dataset.json\", orient=\"records\")\n",
    "dataset[\"test\"].to_json(\"test_dataset.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: trl in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (0.7.11.dev0)\n",
      "Requirement already satisfied: torch>=1.4.0 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from trl) (2.2.0)\n",
      "Requirement already satisfied: transformers>=4.31.0 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from trl) (4.36.2)\n",
      "Requirement already satisfied: numpy>=1.18.2 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from trl) (1.24.4)\n",
      "Requirement already satisfied: accelerate in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from trl) (0.26.1)\n",
      "Requirement already satisfied: datasets in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from trl) (2.16.1)\n",
      "Requirement already satisfied: tyro>=0.5.11 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from trl) (0.7.2)\n",
      "Requirement already satisfied: filelock in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from torch>=1.4.0->trl) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from torch>=1.4.0->trl) (4.9.0)\n",
      "Requirement already satisfied: sympy in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from torch>=1.4.0->trl) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from torch>=1.4.0->trl) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from torch>=1.4.0->trl) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from torch>=1.4.0->trl) (2023.10.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from transformers>=4.31.0->trl) (0.20.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from transformers>=4.31.0->trl) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from transformers>=4.31.0->trl) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from transformers>=4.31.0->trl) (2023.12.25)\n",
      "Requirement already satisfied: requests in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from transformers>=4.31.0->trl) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from transformers>=4.31.0->trl) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from transformers>=4.31.0->trl) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from transformers>=4.31.0->trl) (4.66.2)\n",
      "Requirement already satisfied: docstring-parser>=0.14.1 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from tyro>=0.5.11->trl) (0.15)\n",
      "Requirement already satisfied: rich>=11.1.0 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from tyro>=0.5.11->trl) (13.7.0)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from tyro>=0.5.11->trl) (1.6.5)\n",
      "Requirement already satisfied: psutil in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from accelerate->trl) (5.9.8)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from datasets->trl) (15.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from datasets->trl) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from datasets->trl) (0.3.7)\n",
      "Requirement already satisfied: pandas in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from datasets->trl) (2.0.3)\n",
      "Requirement already satisfied: xxhash in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from datasets->trl) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from datasets->trl) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from datasets->trl) (3.9.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from aiohttp->datasets->trl) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from aiohttp->datasets->trl) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from aiohttp->datasets->trl) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from aiohttp->datasets->trl) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from aiohttp->datasets->trl) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from aiohttp->datasets->trl) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from requests->transformers>=4.31.0->trl) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from requests->transformers>=4.31.0->trl) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from requests->transformers>=4.31.0->trl) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from requests->transformers>=4.31.0->trl) (2024.2.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.17.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from jinja2->torch>=1.4.0->trl) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from pandas->datasets->trl) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from pandas->datasets->trl) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from pandas->datasets->trl) (2024.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/huggingface/peft@4a1559582281fc3c9283892caea8ccef1d6f5a4f --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (0.7.2.dev0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from peft) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from peft) (23.2)\n",
      "Requirement already satisfied: psutil in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from peft) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from peft) (2.2.0)\n",
      "Requirement already satisfied: transformers in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from peft) (4.36.2)\n",
      "Requirement already satisfied: tqdm in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from peft) (4.66.2)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from peft) (0.26.1)\n",
      "Requirement already satisfied: safetensors in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from peft) (0.4.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from peft) (0.20.3)\n",
      "Requirement already satisfied: filelock in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from huggingface-hub>=0.17.0->peft) (2023.10.0)\n",
      "Requirement already satisfied: requests in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from huggingface-hub>=0.17.0->peft) (4.9.0)\n",
      "Requirement already satisfied: sympy in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from torch>=1.13.0->peft) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from torch>=1.13.0->peft) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from torch>=1.13.0->peft) (3.1.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from transformers->peft) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from transformers->peft) (0.15.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/ccugutrillague/anaconda3/envs/finetune/lib/python3.8/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import setup_chat_format\n",
    "# from trl import get_quantization_config\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U flash-attn # no cuda, cant use attention-2\n",
    "# !pip install -U optimum # no cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face model id\n",
    "model_id = \"facebook/opt-350m\" # or codellama/CodeLlama-7b-hf `mistralai/Mistral-7B-v0.1`\n",
    "# BitsAndBytesConfig int-4 config\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    # attn_implementation=\"flash_attention_2\", # no CUDA\n",
    "    # torch_dtype=torch.bfloat16,\n",
    "    # quantization_config=bnb_config # requires CUDA\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# tokenizer.padding_side = 'right' # to prevent warnings\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/huggingface/peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # set chat template to OAI chatML, remove if you start from a fine-tuned model\n",
    "from trl import setup_chat_format\n",
    "model, tokenizer = setup_chat_format(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "# LoRA config based on QLoRA paper & Sebastian Raschka experiment\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=128,\n",
    "        lora_dropout=0.05,\n",
    "        r=256,\n",
    "        bias=\"none\",\n",
    "        target_modules=\"all-linear\",\n",
    "        task_type=\"CAUSAL_LM\", \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"facebook-7b-text-to-sql\", # directory to save and repository id\n",
    "    num_train_epochs=3,                     # number of training epochs\n",
    "    per_device_train_batch_size=3,          # batch size per device during training\n",
    "    gradient_accumulation_steps=2,          # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n",
    "    # optim=\"adamw_torch_fused\",              # use fused adamw optimizer # no cuda no fuse work\n",
    "    logging_steps=10,                       # log every 10 steps\n",
    "    save_strategy=\"epoch\",                  # save checkpoint every epoch\n",
    "    learning_rate=2e-4,                     # learning rate, based on QLoRA paper\n",
    "    # bf16=True,                              # use bfloat16 precision doesnt work\n",
    "    # tf32=True,                              # use tf32 precision doesnt work\n",
    "    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n",
    "    warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n",
    "    lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n",
    "    push_to_hub=False,                       # push model to hub\n",
    "    report_to=\"tensorboard\",                # report metrics to tensorboard\n",
    ")\n",
    "\n",
    "# model_name = model_checkpoint.split(\"/\")[-1]\n",
    "# args = TrainingArguments(\n",
    "#     #f\"{model_name}-finetuned-{task}\",\n",
    "#     f\"{model_name}-carpentries-restaurant-ner\",\n",
    "#     evaluation_strategy = \"epoch\",\n",
    "#     learning_rate=2e-5,\n",
    "#     per_device_train_batch_size=batch_size,\n",
    "#     per_device_eval_batch_size=batch_size,\n",
    "#     num_train_epochs=3,\n",
    "#     weight_decay=0.01,\n",
    "#     #push_to_hub=True, #You can have your model automatically pushed to HF if you uncomment this and log in.\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You need to pass a `dataset_text_field` or `formatting_func` argument to the SFTTrainer if you want to use the `ConstantLengthDataset`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m PYTORCH_ENABLE_MPS_FALLBACK\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      5\u001b[0m max_seq_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3072\u001b[39m \u001b[38;5;66;03m# max sequence length for model and packing of the dataset\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mSFTTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpacking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43madd_special_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# We template with special tokens\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend_concat_token\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# No need to add additional separator token\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# trainer = SFTTrainer(\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#     model=model,\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#     train_dataset=dataset,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# trainer.train()\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/finetune/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:258\u001b[0m, in \u001b[0;36mSFTTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics, peft_config, dataset_text_field, packing, formatting_func, max_seq_length, infinite, num_of_sequences, chars_per_token, dataset_num_proc, dataset_batch_size, neftune_noise_alpha, model_init_kwargs, dataset_kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m     dataset_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 258\u001b[0m     train_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpacking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_text_field\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mformatting_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_of_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchars_per_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremove_unused_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremove_unused_columns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eval_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    271\u001b[0m     _multiple \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(eval_dataset, \u001b[38;5;28mdict\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/finetune/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:382\u001b[0m, in \u001b[0;36mSFTTrainer._prepare_dataset\u001b[0;34m(self, dataset, tokenizer, packing, dataset_text_field, max_seq_length, formatting_func, num_of_sequences, chars_per_token, remove_unused_columns, append_concat_token, add_special_tokens)\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_non_packed_dataloader(\n\u001b[1;32m    372\u001b[0m         tokenizer,\n\u001b[1;32m    373\u001b[0m         dataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    378\u001b[0m         remove_unused_columns,\n\u001b[1;32m    379\u001b[0m     )\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 382\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_packed_dataloader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_text_field\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_of_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchars_per_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m        \u001b[49m\u001b[43mformatting_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m        \u001b[49m\u001b[43mappend_concat_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/finetune/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:493\u001b[0m, in \u001b[0;36mSFTTrainer._prepare_packed_dataloader\u001b[0;34m(self, tokenizer, dataset, dataset_text_field, max_seq_length, num_of_sequences, chars_per_token, formatting_func, append_concat_token, add_special_tokens)\u001b[0m\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m packed_dataset\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 493\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    494\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to pass a `dataset_text_field` or `formatting_func` argument to the SFTTrainer if you want to use the `ConstantLengthDataset`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    495\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: You need to pass a `dataset_text_field` or `formatting_func` argument to the SFTTrainer if you want to use the `ConstantLengthDataset`."
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 # https://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/9133\n",
    "PYTORCH_ENABLE_MPS_FALLBACK=1\n",
    "\n",
    "max_seq_length = 3072 # max sequence length for model and packing of the dataset\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model.to('cpu'),\n",
    "    args=args,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False,  # We template with special tokens\n",
    "        \"append_concat_token\": False, # No need to add additional separator token\n",
    "    }\n",
    ")\n",
    "\n",
    "# trainer = SFTTrainer(\n",
    "#     model=model,\n",
    "#     train_dataset=dataset,\n",
    "#     dataset_text_field=\"text\",\n",
    "#     max_seq_length=512,\n",
    "# )\n",
    "# trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training, the model will be automatically saved to the hub and the output directory\n",
    "# 9.28 GB, other allocations: 7.40 GB, max allowed: 18.13 GB\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "# Load PEFT model on CPU\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    args.output_dir,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    ")  \n",
    "# Merge LoRA and base model and save\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(args.output_dir,safe_serialization=True, max_shard_size=\"2GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer, pipeline \n",
    "\n",
    "peft_model_id = \"./facebook-7b-text-to-sql\"\n",
    "# peft_model_id = args.output_dir\n",
    "\n",
    "# Load Model with PEFT adapter\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "  peft_model_id,\n",
    "  device_map=\"auto\",\n",
    "  torch_dtype=torch.float16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
    "# load into pipeline\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset \n",
    "from random import randint\n",
    "\n",
    "\n",
    "# Load our test dataset\n",
    "eval_dataset = load_dataset(\"json\", data_files=\"test_dataset.json\", split=\"train\")\n",
    "rand_idx = randint(0, len(eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on sample #modify the json file test\n",
    "prompt = pipe.tokenizer.apply_chat_template(eval_dataset[rand_idx][\"messages\"][:2], tokenize=False, add_generation_prompt=True)\n",
    "outputs = pipe(prompt, max_new_tokens=256, do_sample=False, temperature=0.1, top_k=50, top_p=0.1, eos_token_id=pipe.tokenizer.eos_token_id, pad_token_id=pipe.tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Query:\\n{eval_dataset[rand_idx]['messages'][1]['content']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original Answer:\\n{eval_dataset[rand_idx]['messages'][2]['content']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Generated Answer:\\n{outputs[0]['generated_text'][len(prompt):].strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def evaluate(sample):\n",
    "    prompt = pipe.tokenizer.apply_chat_template(sample[\"messages\"][:2], tokenize=False, add_generation_prompt=True)\n",
    "    outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95, eos_token_id=pipe.tokenizer.eos_token_id, pad_token_id=pipe.tokenizer.pad_token_id)\n",
    "    predicted_answer = outputs[0]['generated_text'][len(prompt):].strip()\n",
    "    if predicted_answer == sample[\"messages\"][2][\"content\"]:\n",
    "        return 1 \n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "success_rate = []\n",
    "number_of_eval_samples = 19 # 1000\n",
    "# iterate over eval dataset and predict\n",
    "for s in tqdm(eval_dataset.shuffle().select(range(number_of_eval_samples))):\n",
    "    success_rate.append(evaluate(s))\n",
    "\n",
    "# compute accuracy\n",
    "accuracy = sum(success_rate)/len(success_rate)\n",
    "\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
