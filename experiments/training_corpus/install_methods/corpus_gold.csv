Link,type,text,method_1,method_2, method_3,level_difficulty
https://raw.githubusercontent.com/apple/ml-mgie/main/README.md,package,"## Requirements ``` /n conda create -n mgie python=3.10 -y \n conda activate mgie \n conda update -n base -c defaults conda setuptools -y \n conda install -c conda-forge git git-lfs ffmpeg vim htop ninja gpustat -y \n conda clean -a -y \n pip install -U pip cmake cython==0.29.36 pydantic==1.10 numpy \n pip install -U gdown pydrive2 wget jupyter jupyterlab jupyterthemes ipython \n pip install -U sentencepiece transformers diffusers tokenizers datasets gradio==3.37 accelerate evaluate git+https://github.com/openai/CLIP.git \n pip install -U https://download.pytorch.org/whl/cu113/torch-1.12.0%2Bcu113-cp310-cp310-linux_x86_64.whl https://download.pytorch.org/whl/cu113/torchvision-0.13.0%2Bcu113-cp310-cp310-linux_x86_64.whl https://download.pytorch.org/whl/cu113/torchaudio-0.12.0%2Bcu113-cp310-cp310-linux_x86_64.whl \n pip install -U deepspeed \n  \n # git clone this repo cd ml-mgie \n git submodule update --init --recursive \n cd LLaVA \n pip install -e . \n pip install -U https://download.pytorch.org/whl/cu113/torch-1.12.0%2Bcu113-cp310-cp310-linux_x86_64.whl https://download.pytorch.org/whl/cu113/torchvision-0.13.0%2Bcu113-cp310-cp310-linux_x86_64.whl https://download.pytorch.org/whl/cu113/torchaudio-0.12.0%2Bcu113-cp310-cp310-linux_x86_64.whl \n pip install -U ninja flash-attn==1.0.2 \ pip install -U pydrive2 gdown wget",## Requirements```conda create -n mgie python=3.10 -y / conda activate mgie/ conda update -n base -c defaults conda setuptools -y /conda install -c conda-forge git git-lfs ffmpeg vim htop ninja gpustat -y/ conda clean -a -y, "<add>", hard, "</add>"
https://raw.githubusercontent.com/AILab-CVC/YOLO-World/master/README.md,package, ### 1. Installation YOLO-World is developed based on `torch==1.11.0` `mmyolo==0.6.0` and `mmdetection==3.0.0`. ```bash python setup.py build develop```,,2,<add>, test
https://raw.githubusercontent.com/uclaml/SPIN/main/README.md, library,## Setup /n The following steps provide the necessary setup to run our codes. 1. Create a Python virtual environment with Conda: \n ``` \n conda create -n myenv python=3.10 \n conda activate myenv \n 2. Install PyTorch `v2.1.0` with compatible cuda version, following instructions from [PyTorch Installation Page](https://pytorch.org/get-started/locally/). For example with cuda 11: \n ``` \n pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu118 \n ``` \n 3. Install the following Python dependencies to run the codes. \n ``` \n python -m pip install . \n python -m pip install flash-attn --no-build-isolation \n ``` \n 4. Login to your huggingface account for downloading models \n ``` \n huggingface-cli login --token ${your_access_token} \n ```,no parameters,"Complex", no
https://raw.githubusercontent.com/Doubiiu/DynamiCrafter/main/README.md, package, ## Setup /n ### Install Environment via Anaconda (Recommended) /n ```bash /n conda create -n dynamicrafter python=3.8.5 /n conda activate dynamicrafter /n pip install -r requirements.txt /n  ```,, "source", "moderate", test
https://raw.githubusercontent.com/time-series-foundation-models/lag-llama/main/README.md, package, <add>,<add>,<add>, <add>, <add>
https://raw.githubusercontent.com/arplaboratory/learning-to-fly/master/README.MD, package, ## Instructions to run the code /n ### Docker (isolated) \n We provide a pre-built Docker image with a simple web interface that can be executed using a single command (given that Docker is already installed on your machine): \n ``` \n docker run -it --rm -p 8000:8000 arpllab/learning_to_fly \n ``` \n After the container is running, navigate to [https://0.0.0.0:8000](https://0.0.0.0:8000) and you should see something like (after starting the training): /n Note that to make this Docker image compatible with a broad range of CPUs some optimizations have been turned off. For full speed we recommend a [Native installation](#Native-installation), from source; from container, testing, <add>