{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## - LIST OF STEPS PROMPT\n",
    "\n",
    "#### Baseline Zero Shot\n",
    "```py\n",
    "prompt = \"\"\" Provide a detailed list of the Steps in the given Research Software which refers to \"[software-name]\" plan, including the total count of all the Step? \"\"\"\n",
    "```\n",
    "\n",
    "#### One Shot Learning:\n",
    "\n",
    "The prompt additionally contains an example instance of a research software installation plan (consisting of a description of the initial step and the end) and the corresponding plan (which ends with a tag, referred to as the plan-end tag, that denotes the end of the plan). The prompt is formatted in natural language (or **JSON/RDF format**?)\n",
    "\n",
    "**JSON**\n",
    "\n",
    "```py\n",
    "prompt = \"\"\"\\nGiven a TEXT, I want you generate task steps and plan type of installation.\\\n",
    "    The format must in a strict JSON format : {[{\"plan_type\": \"detect the method of installation\", \"task_steps\": [a ordered list of steps to install], \"commands\": [ a concise list of commands for the tool.  \\\n",
    "    If you do not find software installation steps in the TEXT, return \"none\".\\\n",
    "    Annotate the following TEXT  \"\"\"\n",
    "        prompt += \"\"\"\\n\\n# REQUIREMENTS #: \\n1. the generated task steps and plan types allign with # RESEARCH SOFTWARE TOOL # perfectly. Task name must be detected from the README file; \\n2. the task steps should strictly aligned with the plan type, and the number of task steps should be same with the readmes;  \\n4.  RESEARCH SOFTWARE TOOL command should be align with the input-type field of # TASK LIST #;\"\"\"\n",
    "```\n",
    "\n",
    "**ONTOLOGY**\n",
    "\n",
    "```py\n",
    "\n",
    "prompt = \"\"\" Here is the complete definition of the procedural RDF ontology in TTL format which defines Plans and Steps and relationships between each other:\n",
    "\n",
    "@prefix p-plan: <http://purl.org/net/p-plan#> .\n",
    "@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n",
    "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
    "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
    "@prefix prov: <http://www.w3.org/ns/prov#> .\n",
    "@prefix bpmn: <http://www.w3.org/ns/bpmn#> .\n",
    "\n",
    "[.....]\n",
    "\n",
    "\n",
    "Using the provided ontology, please create specific instances and data about plans representing the steps and  for the [-] software installation procedure. Include relationships such as 'isStepOfPlan' to model the hierarchy of main steps and their substeps. Additionally, create the RDF graph that represents the defined individuals and their relationships.  Refer to the RDF graph as Context, please provide an Answer to the Question below.\"\"\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gpt4all openai -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example for validation ground_true**\n",
    "\n",
    "```json\n",
    "            {\n",
    "                \"id\": \"6\",\n",
    "                \"name\": \"PFischbeck/parameter-fitting-experiments\",\n",
    "                \"url\": \"https://raw.githubusercontent.com/PFischbeck/parameter-fitting-experiments/main/Readme.md\",\n",
    "                \"n_plans\": 2,\n",
    "                \"plan_nodes\": [\n",
    "                    {\n",
    "                        \"type\": \"Source\",\n",
    "                        \"plan_step\": [\n",
    "                            \"Step 1: Make sure you have Python, Pip and R installed.\",\n",
    "                            \"Step 2: Checkout this repository.\",\n",
    "                            \"Step 3: Install the python dependencies with.\",\n",
    "                            \"Step 4: Install the pygirgs package at https://github.com/PFischbeck/pygirgs.\",\n",
    "                            \"Step 5: Install the R dependencies (used for plots) with\",\n",
    "                            \"Step 6: Download the file at https://doi.org/10.5281/zenodo.10629451 and extract the content\",\n",
    "                            \"Step optional: Download the file output-data.zip from Zenodo and extract its contents into the folder output_data. This way, you can access all experiment results without running them yourself.\"\n",
    "                        ],\n",
    "                        \"operating_system\": []\n",
    "                    }\n",
    "                ],\n",
    "                \"readme_instructions\": \"\",\n",
    "                \"skip_content\": \"\"\n",
    "            },\n",
    "``````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import openai\n",
    "import config\n",
    "api_key = config.API_KEY\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. - TYPE OF PLANS PROMPT\n",
    "\n",
    "#### Baseline Zero Shot\n",
    "```py\n",
    "prompt_TEXT = \"\"\" Given a INSTALL_TEXT, detect the class of plan for the installation of a software.\"\"\"\n",
    "prompt_URL =  \"\"\" Given a INSTALL_TEXT, detect the class of plan for the installation of a software.\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**GEMMA: Gemini-based open model**\n",
    "\n",
    "ref:\n",
    "[https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf](https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a URL (no labels), Natural language response\n",
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api-inference.huggingface.co/v1\",\n",
    "    api_key= api_key # your key hf\n",
    ")\n",
    "MODEL = \"google/gemma-7b-it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_code_completion(messages, max_tokens=512, model=MODEL):\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=model,\n",
    "        max_tokens=max_tokens,\n",
    "        # stop=[\n",
    "        #     \"<step>\"\n",
    "        # ],\n",
    "        # frequency_penalty=1,\n",
    "        # presence_penalty=1,\n",
    "        # top_p=0.7,\n",
    "        # n=10,\n",
    "        # temperature=1,\n",
    "    )\n",
    "\n",
    "    return chat_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = config.API_KEY\n",
    "SYSTEM_PROMPT = \"You are a smart and intelligent Named Entity Recognition (NER) system. I will provide you some task to identify entities in a text\"\n",
    "USER_PROMPT_1 = \"Detect the TYPE of plan for the installation of a software in the README_INSTALLATION.\"\n",
    "ASSISTANT_PROMPT_1 = \"Sure, I'm ready to help you with your NER task. Please provide me with the necessary information\"\n",
    "GUIDELINES_PROMPT = (\n",
    "    \"Entity Definition:\\n\"\n",
    "    \"1. README_INSTALLATION: a excerpt of the text found in a readme file of a research software tool\"\n",
    "    \"2. TYPE: represents the concept of a installation method type as a plan, which is composed of steps, which must be executed in a given order. A installation method is an instance of the Plan concept. There are four type of plans: binary, package manager, source and container. A research software readme installation instruction could refer to one or multiple methods\\n\"\n",
    "    \"3. PLAN_STEP: represents a list of planned action(s) as part of a Plan to be executed by an Agent. These are a list of indivisible sequence of actions that must executed without interruption. Step within a Plan could be linked to one specific executable operation, or refer to a group of operations. A Step then could invoke more than one action. Each sentence in a readme is an instance of the Step concept.\\n\"\n",
    "    \"4. TECHNOLOGY: repesents the concept of a particular operating sytem or package management correspoding to a particular set of PLAN_STEPs in a specific TYPE.\\n\"\n",
    "    \"5. N_PLANS: count the number of unique plan TYPE\"\n",
    "    \"\\n\"\n",
    "    \"Output Format:\\n\"\n",
    "    \"{{'TYPE': [name of the TYPE plans present], 'PLAN_STEP': [list of steps], 'TECHNOLOGY': [name of the technology used for that specific TYPE], 'N_PLANS': [number of unique plan TYPE]}}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api-inference.huggingface.co/v1\",\n",
    "    api_key= api_key # your key hf\n",
    ")\n",
    "MODEL = \"google/gemma-7b-it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = openai.OpenAI()\n",
    "def get_completion(prompt, model=MODEL):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=1\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion for Readme 1:\n",
      "```json\n",
      "{\n",
      "  \"from_source\": [\n",
      "    \"Build from source\",\n",
      "    \"See the [TensorFlow install guide](/install) for the [pip package](/install/pip) and follow the instructions for building from source.\"\n",
      "  ],\n",
      "  \"container\": [\n",
      "    \"Use a [Docker container](/install/docker) to enable GPU support\",\n",
      "    \"See the [TensorFlow install guide](/install) for the [pip package\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api-inference.huggingface.co/v1\",\n",
    "    api_key= api_key # your key hf\n",
    ")\n",
    "MODEL = \"google/gemma-7b-it\"\n",
    "\n",
    "# client = openai.OpenAI()\n",
    "def get_completion(prompt, model=MODEL):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=1\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "readme = f\"\"\"\n",
    "# Install See the [TensorFlow install guide](https://www.tensorflow.org/install) for the [pip package](https://www.tensorflow.org/install/pip), to [enable GPU support](https://www.tensorflow.org/install/gpu), use a [Docker container](https://www.tensorflow.org/install/docker), and [build from source](https://www.tensorflow.org/install/source). To install the current release, which includes support for [CUDA-enabled GPU cards](https://www.tensorflow.org/install/gpu) *(Ubuntu and Windows)*: ``` $ pip install tensorflow ``` Other devices (DirectX and MacOS-metal) are supported using [Device plugins](https://www.tensorflow.org/install/gpu_plugins#available_devices). A smaller CPU-only package is also available: ``` $ pip install tensorflow-cpu ``` To update TensorFlow to the latest version, add `--upgrade` flag to the above commands. *Nightly binaries are available for testing using the [tf-nightly](https://pypi.python.org/pypi/tf-nightly) and [tf-nightly-cpu](https://pypi.python.org/pypi/tf-nightly-cpu) packages on PyPi.\n",
    "\"\"\"\n",
    "definitions = f\"\"\"\n",
    "1. readme: a excerpt of the text found in a readme file of a research software in Github \\\n",
    "2. Installation Method: indicates a **PLAN** which contains instructions as **STEPS** for installing a research software that must be performed in a precise order and under specific conditions. In the context of the study, there are four general different methods as PLAN_TYPE: 0.source, 1.packagemanager, 2.container and 3.binary. \\\n",
    "3. PLAN_TYPE: represents the concept of an installation method type as a plan, which is composed of steps, which must be executed in a given order. An installation method is an instance of the PLAN concept. A research software readme installation instruction could refer to one or multiple methods as PLAN_TYPE\". Here the definition of each TYPE: \\\n",
    "      0.source: Installing a software \"from source\" means installing a software without using automatic tools e.g. container or package manager.\\\n",
    "      1.Packagemanager: Installing a software \"from pip\" means installing the software along with its dependencies indexed in official package managers.\\\n",
    "      2.container: provide a way of packaging research software and their dependencies inside lightweight, standalone containers. \\\n",
    "      3.binary: a binary files contains the entire codebase and associated resources needed to run the software, eliminating the need for compilation or building the software from source.\n",
    "4. STEP: a list of planned activities as part of a `Plan` to be executed in a specific order. Each atomic activity is an instance of the Step concept. A Step within a Plan could comprise one activity, or a group of activities. We define a Step as the readme's author annotated.\n",
    "\n",
    "    \"\\n\"\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Given the following \\\"\\\"\\\"{readme}\\\"\\\"\\\", extract the installation instructions for each method (from source, container) or Operative System (Windows, Linux). For each installation method return a list, where each element of the list is an instruction, in sequential order. The response should be a JSON list. Do not add code commands in the list. Be concise.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(\"Completion for Readme 1:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Generate LLMs response JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responses saved to step-responses.json\n"
     ]
    }
   ],
   "source": [
    "# STEP\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api-inference.huggingface.co/v1\",\n",
    "    api_key=api_key\n",
    ")\n",
    "MODEL = \"google/gemma-7b-it\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_completion(prompt, model=MODEL):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        temperature=0.1\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def extract_installation_instructions_from_json(json_file):\n",
    "    with open(json_file, 'r') as file:\n",
    "        readmes = json.load(file)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for readme in readmes:\n",
    "        readme_instructions = readme.get('readme_instructions', '')\n",
    "        \n",
    "        # Use the get_completion function to get the installation instructions\n",
    "        prompt = f\"\"\" \n",
    "        You will be provided with a Readme. It contains a sequence of instructions per installation method to install a software \\\n",
    "        There are 4 types of methods which are source, packagemanager, container and binary. \\\n",
    "        Perform the following actions. For each Readme:\n",
    "         1. Extract the installation instructions for each method  \\\n",
    "         2. For each installation method return a list, where each element of the list is an instruction, in sequential order \\\n",
    "         2. The output should be in JSON format. \n",
    "        Do not add code commands in the list. Be concise. \\\n",
    "\n",
    "        Readme:\n",
    "        ```{readme_instructions}``` \n",
    "        \"\"\"\n",
    "        prompt += \"\"\"\\n# The format must in a strict JSON format, like: {\"plan\": [{\"type\": \"installation method in the readme\" ], \"steps\": [{\"text\": \"description of the step per installation instruction\", \"n_steps\": [\"count of total steps\"]}]} \"\"\"\n",
    "        \n",
    "        response = get_completion(prompt)\n",
    "        \n",
    "        # Append the extracted instructions to the results list\n",
    "        results.append([readme['id'],response])\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "json_file_path = '../scr/output.json'\n",
    "result_list = extract_installation_instructions_from_json(json_file_path)\n",
    "\n",
    "# # Print the results\n",
    "# for result in result_list:\n",
    "#     for key, value in result.items():\n",
    "#         print(f\"{key}:\")\n",
    "#         print(value)\n",
    "#         print()\n",
    "\n",
    "\n",
    "\n",
    "with open(\"step-responses.json\", \"w\") as json_file:\n",
    "    json.dump(result_list, json_file, indent=2)\n",
    "\n",
    "print(\"Responses saved to step-responses.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLAN\n",
    "\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api-inference.huggingface.co/v1\",\n",
    "    api_key=api_key\n",
    ")\n",
    "MODEL = \"google/gemma-7b-it\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_completion(prompt, model=MODEL):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        temperature=2\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def extract_installation_instructions_from_json(json_file):\n",
    "    with open(json_file, 'r') as file:\n",
    "        readmes = json.load(file)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for readme in readmes:\n",
    "        readme_instructions = readme.get('readme_instructions', '')\n",
    "        readme_plans = readme.get('plans', [])\n",
    "\n",
    "        readme_types = []\n",
    "        for plan in readme_plans:\n",
    "            # Extracting the 'type' field from each plan\n",
    "            readme_type = plan.get('type', '')\n",
    "            readme_types.append(readme_type)\n",
    "            \n",
    "        \n",
    "        # Use the get_completion function to get the installation instructions\n",
    "        prompt = f\"\"\" \n",
    "        Given a  <\\\"\\\"\\\"{readme_instructions}\\\"\\\"\\\", detect the TYPE of PLAN for the installation of a research software. \\\n",
    "        There are 4 TYPE of PLAN: source, packagemanager, container and binary.\n",
    "        \"\"\"\n",
    "        # prompt += \"\"\"labels: binary, source, packagemanager, container.\"\"\"\n",
    "        # prompt += \"\"\"Classify the following \\\"\\\"\\\"{readme_instructions}\\\"\\\"\\\" in one or multiple installation methods \\\n",
    "        # There are 4 possible installation methods which are: source, packagemanager, container and binary. \\\n",
    "        # A readme can describe a single, or multiple types of installation. \\\n",
    "        # \"\"\"\n",
    "        response = get_completion(prompt)\n",
    "        \n",
    "        # Append the extracted instructions to the results list\n",
    "        results.append({\"id\":readme['id'],\"response\":response, \"answers\":readme_types, \"prompt\":prompt})\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Assuming your JSON file has an array of readmes\n",
    "json_file_path = '../scr/output.json'\n",
    "result_list = extract_installation_instructions_from_json(json_file_path)\n",
    "\n",
    "# # Print the results\n",
    "# for result in result_list:\n",
    "#     for key, value in result.items():\n",
    "#         print(f\"{key}:\")\n",
    "#         print(value)\n",
    "#         print()\n",
    "\n",
    "\n",
    "\n",
    "with open(\"plan-responses.json\", \"w\") as json_file:\n",
    "    json.dump(result_list, json_file, indent=2)\n",
    "\n",
    "print(\"LLM responses saved to plan-responses.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Other experiments below:>>>>>>>*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': 'Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBehave as an expert labeler. \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124m          Given a README, detect the class of plan for the installation of a software in the text of the. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Definition:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m 1. README_INSTALLATION: a excerpt of the text found in a readme file of a research software tool \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m 2. TYPE: represents the concept of a installation method type as a plan, which is composed of steps, which must be executed in a given order. A installation method is an instance of the Plan concept. There are four type of plans: binary, package manager, source and container. A research software readme installation instruction could refer to one or multiple methods\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m 3. PLAN_STEP: represents a list of planned action(s) as part of a Plan to be executed by an Agent. These are a list of indivisible sequence of actions that must executed without interruption. Step within a Plan could be linked to one specific executable operation, or refer to a group of operations. A Step then could invoke more than one action. Each sentence in a readme is an instance of the Step concept.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m 4. TECHNOLOGY: repesents the concept of a particular operating sytem or package management correspoding to a particular set of PLAN_STEPs in a specific TYPE.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m 5. N_PLANS: count the number of unique plan TYPE \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m README = # Installation - Make sure you have Python, Pip and R installed. - Checkout this repository - Install the python dependencies with ```pip3 install -r requirements.txt``` - Install the `pygirgs` package at https://github.com/PFischbeck/pygirgs \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m - Install the R dependencies (used for plots) with  ```R -e install.packages(c(ggplot2, reshape2, plyr, dplyr, scales), repos=https://cloud.r-project.org/) Download the file `konect-data.zip` from [Zenodo](https://doi.org/10.5281/zenodo.10629451) and extract its contents into the folder `input_data/konect` Optional: Download the file `output-data.zip` from [Zenodo](https://doi.org/10.5281/zenodo.10629451) and extract its contents into the folder `output_data`. This way, you can access all experiment results without running them yourself. The Output Format in JSON file:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m{{\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTYPE\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: [name of the TYPE plans present], \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPLAN_STEP\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: [list of steps], \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTECHNOLOGY\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: [name of the technology used for that specific TYPE], \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN_PLANS\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: [number of unique plan TYPE]}} \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m         \u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m      5\u001b[0m ]\n\u001b[0;32m----> 7\u001b[0m chat_completion \u001b[38;5;241m=\u001b[39m \u001b[43mget_code_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(chat_completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent)\n",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m, in \u001b[0;36mget_code_completion\u001b[0;34m(messages, max_tokens, model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_code_completion\u001b[39m(messages, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, model\u001b[38;5;241m=\u001b[39mMODEL):\n\u001b[0;32m----> 2\u001b[0m     chat_completion \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# stop=[\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#     \"<step>\"\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# ],\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# frequency_penalty=1,\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# presence_penalty=1,\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# top_p=0.7,\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# n=10,\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# temperature=1,\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chat_completion\n",
      "File \u001b[0;32m~/mambaforge/envs/coderefinery/lib/python3.11/site-packages/openai/_utils/_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/coderefinery/lib/python3.11/site-packages/openai/resources/chat/completions.py:663\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    612\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    613\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    661\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    662\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 663\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/coderefinery/lib/python3.11/site-packages/openai/_base_client.py:1200\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1187\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1188\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1195\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1196\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1197\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1198\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1199\u001b[0m     )\n\u001b[0;32m-> 1200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/mambaforge/envs/coderefinery/lib/python3.11/site-packages/openai/_base_client.py:889\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    882\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    887\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    888\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/coderefinery/lib/python3.11/site-packages/openai/_base_client.py:965\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m    964\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/mambaforge/envs/coderefinery/lib/python3.11/site-packages/openai/_base_client.py:1013\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1013\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/coderefinery/lib/python3.11/site-packages/openai/_base_client.py:965\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m    964\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/mambaforge/envs/coderefinery/lib/python3.11/site-packages/openai/_base_client.py:1013\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1013\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/coderefinery/lib/python3.11/site-packages/openai/_base_client.py:980\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    977\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    979\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 980\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m    983\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    984\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    987\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    988\u001b[0m )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': 'Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate'}"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "        {\"role\": \"user\", \"content\": \"Behave as an expert labeler. \\\n",
    "          Given a README, detect the class of plan for the installation of a software in the text of the. \\n Definition:\\n 1. README_INSTALLATION: a excerpt of the text found in a readme file of a research software tool \\n 2. TYPE: represents the concept of a installation method type as a plan, which is composed of steps, which must be executed in a given order. A installation method is an instance of the Plan concept. There are four type of plans: binary, package manager, source and container. A research software readme installation instruction could refer to one or multiple methods\\n 3. PLAN_STEP: represents a list of planned action(s) as part of a Plan to be executed by an Agent. These are a list of indivisible sequence of actions that must executed without interruption. Step within a Plan could be linked to one specific executable operation, or refer to a group of operations. A Step then could invoke more than one action. Each sentence in a readme is an instance of the Step concept.\\n 4. TECHNOLOGY: repesents the concept of a particular operating sytem or package management correspoding to a particular set of PLAN_STEPs in a specific TYPE.\\n 5. N_PLANS: count the number of unique plan TYPE \\n README = # Installation - Make sure you have Python, Pip and R installed. - Checkout this repository - Install the python dependencies with ```pip3 install -r requirements.txt``` - Install the `pygirgs` package at https://github.com/PFischbeck/pygirgs \\ - Install the R dependencies (used for plots) with  ```R -e install.packages(c(ggplot2, reshape2, plyr, dplyr, scales), repos=https://cloud.r-project.org/) Download the file `konect-data.zip` from [Zenodo](https://doi.org/10.5281/zenodo.10629451) and extract its contents into the folder `input_data/konect` Optional: Download the file `output-data.zip` from [Zenodo](https://doi.org/10.5281/zenodo.10629451) and extract its contents into the folder `output_data`. This way, you can access all experiment results without running them yourself. The Output Format in JSON file:\\n {{'TYPE': [name of the TYPE plans present], 'PLAN_STEP': [list of steps], 'TECHNOLOGY': [name of the technology used for that specific TYPE], 'N_PLANS': [number of unique plan TYPE]}} \\\n",
    "         \"},\n",
    "]\n",
    "\n",
    "chat_completion = get_code_completion(messages)\n",
    "            \n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Mistral orca-2 Model**\n",
    "\n",
    "ref: [mistral-7b-openorca.gguf2.Q4_0.gguf](mistral-7b-openorca.gguf2.Q4_0.gguf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One Shot Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chain of Thought Prompting**\n",
    "\n",
    "\n",
    "In addition to the four experiments above, we look at a fifth experiment using a state tracking chain of thought prompting technique in a natural language one shot setting. Within this configuration, we provide an annotated example where each action is annotated with the state prior to the action, the reason for why the action is applicable in the prior state, and the resulting state after applying the action. After the example, a meta-explanation about plan correctness is provided. The LLM is then asked to return a response making the same state tracking and justification annotations that were included in the example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**GEMMA: Gemini-based open model**\n",
    "\n",
    "ref:\n",
    "[https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf](https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**- Baseline Zero Shot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_code_completion' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 9\u001b[0m\n\u001b[1;32m      2\u001b[0m messages\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m      3\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBehave as an expert labeler. \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m          Given a URL, I want you generate a list of steps for the plan of installation. Please indicate the number of the steps. URL = https://raw.githubusercontent.com/PFischbeck/parameter-fitting-experiments/main/Readme.md\u001b[39m\u001b[38;5;124m\"\u001b[39m},        \n\u001b[1;32m      5\u001b[0m     ]\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# stream=True,\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# max_tokens=500\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m chat_completion \u001b[38;5;241m=\u001b[39m \u001b[43mget_code_completion\u001b[49m(messages)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(chat_completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_code_completion' is not defined"
     ]
    }
   ],
   "source": [
    "# Given a URL, Natural language response\n",
    "messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Behave as an expert labeler. \\\n",
    "          Given a URL, I want you generate a list of steps for the plan of installation. Please indicate the number of the steps. URL = https://raw.githubusercontent.com/PFischbeck/parameter-fitting-experiments/main/Readme.md\"},        \n",
    "    ]\n",
    "    # stream=True,\n",
    "    # max_tokens=500\n",
    "\n",
    "chat_completion = get_code_completion(messages)\n",
    "            \n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"PLAN_STEP\": [\"Step 1: Ensure Python, Pip and R are installed.\", \"Step 2: Checkout this repository.\", \"Step 3: Install Python dependencies (e.g. `pip3 install -r requirements.txt`)\", \"Step 4: Install the `pygirgs` package.\", \"Step 5: Install R dependencies (used for plots) with  ```R -e install.packages(c(ggplot2, reshape2, plyr, dplyr, scales), repos=https://cloud.r-project.org/)`, \"Step 6: Download and extract `konect-data.zip` into `input_data/konect` folder.\", \"Optional: Download and extract `output-data.zip` into `output_data` folder.],\n",
      "  \"NUM_STEP\": 7\n",
      "}\n",
      "```\n",
      "\n",
      "**Number of steps:** 7\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "The README_TEXT describes an installation plan with a total of 7 steps. Each step is clearly defined and described. The steps include ensuring required software dependencies are installed, checking out the repository, installing Python and R dependencies, downloading and extracting data files, and optional steps for downloading additional data.<eos>"
     ]
    }
   ],
   "source": [
    "# given a URL, JSON object response\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    model=\"google/gemma-7b-it\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Behave as an expert labeler. \\\n",
    "          Given a README_TEXT, \\\n",
    "          I want you generate a list of steps for the plan of installation. \\\n",
    "          Please indicate the number of the steps. The RESPONSE Format for your response should be in JSON file as following: \\\n",
    "         \\n {{'PLAN_STEP': [`Step 1`: give a number of the step. Each step separated with commas], {{`NUM_STEP`: [count of unique steps]}}}} \\n \\\n",
    "         Definition:\\n 1. README_INSTALLATION: a excerpt of the text found in a readme file of a research software tool \\n 2.PLAN_STEP: represents a list of planned action(s) in sequential orden as part of a installation plan. These are a list of indivisible sequence of actions that must executed without interruption. Step within a Plan could be linked to one specific executable operation, or refer to a group of operations. A Step then could invoke more than one action. \\n \\\n",
    "         Here is the README_TEXT = # Installation - Make sure you have Python, Pip and R installed. - Checkout this repository - Install the python dependencies with ```pip3 install -r requirements.txt``` - Install the `pygirgs` package at https://github.com/PFischbeck/pygirgs \\ - Install the R dependencies (used for plots) with  ```R -e install.packages(c(ggplot2, reshape2, plyr, dplyr, scales), repos=https://cloud.r-project.org/) Download the file `konect-data.zip` from [Zenodo](https://doi.org/10.5281/zenodo.10629451) and extract its contents into the folder `input_data/konect` Optional: Download the file `output-data.zip` from [Zenodo](https://doi.org/10.5281/zenodo.10629451) and extract its contents into the folder `output_data`. This way, you can access all experiment results without running them yourself.\"},\n",
    "    ],\n",
    "    stream=True,\n",
    "    max_tokens=500,\n",
    "    response_format={\"type\": \"json_object\"},\n",
    ")\n",
    "\n",
    "for message in chat_completion:\n",
    "    print(message.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # given a URL, JSON object response\n",
    "# from openai import OpenAI\n",
    "# client = OpenAI(\n",
    "#     base_url=\"https://api-inference.huggingface.co/v1\",\n",
    "#     api_key= api_key\n",
    "\n",
    "example_json = {\n",
    "  \"plan_nodes\": [\n",
    "                    {\n",
    "                        \"num_steps\": \"# total count number of unique steps\",\n",
    "                        \"plan_step\": [\n",
    "                            \"`# Step 1:` a string to represent the concept of a step in a method or plan. Each atomic activity is an instance of the Step. This step indicates the initial step in a plan\",\n",
    "                            \"# `Step 2:` a string to represent the concept of a step in a procedure or plan. It associates a step with the next step that must be performed\",\n",
    "                            \"# `Step n:`a string to represent the concept of a step in a procedure or plan. It associates a plan with its last step. This indicates the final step in a plan.\"\n",
    "                        ]\n",
    "                    }\n",
    "                ]\n",
    "}\n",
    "\n",
    "\n",
    "prompt= \"Given a , I want you generate a valid JSON output with the list of steps for the plan of installation. \\\n",
    "README_TEXT = # Installation - Make sure you have Python, Pip and R installed. - Checkout this repository - Install the python dependencies with ```pip3 install -r requirements.txt``` - Install the `pygirgs` package at https://github.com/PFischbeck/pygirgs \\ - Install the R dependencies (used for plots) with  ```R -e install.packages(c(ggplot2, reshape2, plyr, dplyr, scales), repos=https://cloud.r-project.org/) Download the file `konect-data.zip` from [Zenodo](https://doi.org/10.5281/zenodo.10629451) and extract its contents into the folder `input_data/konect` Optional: Download the file `output-data.zip` from [Zenodo](https://doi.org/10.5281/zenodo.10629451) and extract its contents into the folder `output_data`. This way, you can access all experiment results without running them yourself.\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"google/gemma-7b-it\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Provide output in valid JSON. The data schema should be like this:\"+json.dumps(example_json)},\n",
    "        { \"role\": \"system\", \"content\": prompt}\n",
    "    ],\n",
    "    stream=True,\n",
    "    max_tokens=500,\n",
    "    response_format={\"type\": \"json_object\"},\n",
    ")\n",
    "# messages = []\n",
    "# for message in chat_completion:\n",
    "#     print(message.choices[0].delta.content, end=\"\")\n",
    "#     messages.append(message.choices[0].delta.content)\n",
    "\n",
    "with open('output.json', 'w') as f:\n",
    "    for chunk in response:\n",
    "        f.write(chunk.choices[0].delta.content)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## JSON object for installing research software from the text:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"total_steps\": 6,\n",
      "  \"steps\": [\n",
      "    \"Make sure you have Python, Pip and R installed.\",\n",
      "    \"Checkout this repository\",\n",
      "    \"Install the python dependencies with `pip3 install -r requirements.txt`\",\n",
      "    \"Install the `pygirgs` package at https://github.com/PFischbeck/pygirgs\",\n",
      "    \"Install the R dependencies (used for plots) with `R -e 'install.packages(c(ggplot2, reshape2, plyr, dplyr, scales), repos=https://cloud.r-project.org/)'\",\n",
      "    \"Download the file `konect-data.zip` from [Zenodo](https://doi.org/10.5281/zenodo.10629451) and extract its contents into the folder `input_data/konect`. This way, you can access all experiment results without running them yourself. The file `output-data.zip` can be optionally downloaded and extracted into the folder `output_data`.\"\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "**Total number of steps:** 6<eos>"
     ]
    }
   ],
   "source": [
    "# Given a INSTALL_TEXT, JSON object response\n",
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api-inference.huggingface.co/v1\",\n",
    "    api_key= api_key\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    model=\"google/gemma-7b-it\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Behave as an expert labeler. \\\n",
    "          Given a INSTALL_TEXT, generate an example json object describing the Steps of installation of the Research Software. Please indicate the total number of the Steps. \\\n",
    "         INSTALL_TEXT = # Installation - Make sure you have Python, Pip and R installed. - Checkout this repository - Install the python dependencies with \\\n",
    "         ```pip3 install -r requirements.txt``` \\\n",
    "         - Install the `pygirgs` package at https://github.com/PFischbeck/pygirgs \\\n",
    "         - Install the R dependencies (used for plots) with \\\n",
    "         ```R -e 'install.packages(c(ggplot2, reshape2, plyr, dplyr, scales), repos=https://cloud.r-project.org/)'``` \\\n",
    "         - Download the file `konect-data.zip` from [Zenodo](https://doi.org/10.5281/zenodo.10629451) and extract its contents into the folder `input_data/konect` \\\n",
    "         - Optional: Download the file `output-data.zip` from [Zenodo](https://doi.org/10.5281/zenodo.10629451) and extract its contents into the folder `output_data`. This way, you can access all experiment results without running them yourself.\"},        \n",
    "    ],\n",
    "    stream=True,\n",
    "    max_tokens=500,\n",
    "    response_format={\"type\": \"json_object\"},\n",
    ")\n",
    "\n",
    "for message in chat_completion:\n",
    "    print(message.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mistral orca-2 Model**\n",
    "\n",
    "ref: [mistral-7b-openorca.gguf2.Q4_0.gguf](mistral-7b-openorca.gguf2.Q4_0.gguf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gpt4all._pyllmodel:LLModel.prompt_model -- prompt:\n",
      "You are an expert label.\n",
      "Be terse.\n",
      "\n",
      "### Instruction:\n",
      " Given a TEXT, I want you generate task steps and plan type of installation.    The format must in a strict JSON format : {[{\"plan_type\": \"detect the method of installation\", \"task_steps\": [a ordered list of steps to install], \"commands\": [ a concise list of commands for the tool.      If you do not find software installation steps in the TEXT, return \"none\".    Annotate the following TEXT: # Installation ### Dependencies Initialize git submodules with `` git submodule init git submodule update```     Install the specific versions of every package from `requirements.txt` in a new conda environment:    conda create --name gsft python=3.9     conda activate gsft    pip install -r requirements.txt    To ensure that Python paths are properly defined, update the `~/.bashrc` by adding the following lines    export GSFT_PATH=/path_to_gsfc    export PYTHONPATH=$PYTHONPATH:/$GSFT_PATH\n",
      "### Response:\n",
      "\n",
      "===/LLModel.prompt_model -- prompt/===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:   Given a TEXT, I want you generate task steps and plan type of installation.    The format must in a strict JSON format : {[{\"plan_type\": \"detect the method of installation\", \"task_steps\": [a ordered list of steps to install], \"commands\": [ a concise list of commands for the tool.      If you do not find software installation steps in the TEXT, return \"none\".    Annotate the following TEXT: # Installation ### Dependencies Initialize git submodules with `` git submodule init git submodule update```     Install the specific versions of every package from `requirements.txt` in a new conda environment:    conda create --name gsft python=3.9     conda activate gsft    pip install -r requirements.txt    To ensure that Python paths are properly defined, update the `~/.bashrc` by adding the following lines    export GSFT_PATH=/path_to_gsfc    export PYTHONPATH=$PYTHONPATH:/$GSFT_PATH\n",
      "Response  {[{\"plan_type\": \"software installation\", \"task_steps\": [\n",
      "\"Determine the specific version of Python to install from `requirements.txt`\",\n",
      "\"Download and install the specified Python package using pip\",\n",
      "\"Install any necessary packages such as NumPy, Pandas or Matplotlib\",\n",
      "\"Create a conda environment named 'gsft' with Python 3.9\",\n",
      "\"Activate the conda environment\",\n",
      "\"Update the `~/.bashrc` to define Python paths properly by adding lines such as `export GSFT_PATH=/path_to_gsfc` and `export PYTHONPATH=$PYTHONPATH:/$GSFT_PATH`\"]]} \n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from gpt4all import GPT4All\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "model = GPT4All(\"orca-mini-3b-gguf2-q4_0.gguf\")\n",
    "with model.chat_session('You are an expert label.\\nBe terse.',\n",
    "                        '### Instruction:\\n{0}\\n### Response:\\n'):\n",
    "    prompt = ' Given a TEXT, I want you generate task steps and plan type of installation.\\\n",
    "    The format must in a strict JSON format : {[{\"plan_type\": \"detect the method of installation\", \"task_steps\": [a ordered list of steps to install], \"commands\": [ a concise list of commands for the tool.  \\\n",
    "    If you do not find software installation steps in the TEXT, return \"none\".\\\n",
    "    Annotate the following TEXT: # Installation ### Dependencies Initialize git submodules with `` git submodule init git submodule update``` \\\n",
    "    Install the specific versions of every package from `requirements.txt` in a new conda environment:\\\n",
    "    conda create --name gsft python=3.9 \\\n",
    "    conda activate gsft\\\n",
    "    pip install -r requirements.txt\\\n",
    "    To ensure that Python paths are properly defined, update the `~/.bashrc` by adding the following lines\\\n",
    "    export GSFT_PATH=/path_to_gsfc\\\n",
    "    export PYTHONPATH=$PYTHONPATH:/$GSFT_PATH'\n",
    "    print(\"PROMPT: \", prompt)\n",
    "    response = model.generate(prompt=prompt, temp=0.6)\n",
    "    print(\"Response\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gpt4all._pyllmodel:LLModel.prompt_model -- prompt:\n",
      "### System:\n",
      "You are an AI assistant that follows instruction extremely well. Help as much as you can.\n",
      "\n",
      "### User:\n",
      "Behave as an expert labeler. \\ Given a URL, I want you generate task steps and plan type of installation.    The format must in a strict JSON format : {\"task_steps\": [ step description of one or more steps ], \"plan_nodes\": [{\"plan_type\": \"detect the method of installation\", \"commands\": [ a concise list of commands for the tool.     URL: https://raw.githubusercontent.com/PFischbeck/parameter-fitting-experiments/main/Readme.md\n",
      "### Response:\n",
      "\n",
      "===/LLModel.prompt_model -- prompt/===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response  {\n",
      "\"task_steps\": [\n",
      " {\"step\": \"Detect the method of installation\",\n",
      " \"commands\": [\"sudo apt-get update && sudo apt-get install pf-script\"]\n",
      "}\n",
      "],\n",
      "\"plan_nodes\": [\n",
      " {\"plan_type\": \"Installation Plan\",\n",
      " \"instructions\": [\n",
      " \"1. Download the latest version of the package manager for your operating system from the official website.\",\n",
      " \"2. Open the terminal and run the command `sudo apt-get update` to check if there are any updates available.\",\n",
      " \"3. Run the command `sudo apt-get install pf-script` to download and install the package manager.\"\n",
      " ]}\n",
      "]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from gpt4all import GPT4All\n",
    "model = GPT4All(\"orca-mini-3b-gguf2-q4_0.gguf\")\n",
    "with model.chat_session():\n",
    "    prompt = 'Behave as an expert labeler. \\ Given a URL, I want you generate task steps and plan type of installation.\\\n",
    "    The format must in a strict JSON format : {\"task_steps\": [ step description of one or more steps ], \"plan_nodes\": [{\"plan_type\": \"detect the method of installation\", \"commands\": [ a concise list of commands for the tool. \\\n",
    "    URL: https://raw.githubusercontent.com/PFischbeck/parameter-fitting-experiments/main/Readme.md'\n",
    "    # print(\"PROMPT: \", prompt)\n",
    "    response = model.generate(prompt=prompt, temp=0)\n",
    "    print(\"Response\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. - TYPE OF PLANS PROMPTS\n",
    "\n",
    "#### Baseline-  Zero Shot\n",
    "```py\n",
    "prompt = \"\"\" Classify the text into LABELS [source, binary, container, and package manager]\"\"\"\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
