{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gpt4all openai -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example for validation ground_true**\n",
    "\n",
    "```json\n",
    "           {\n",
    "                \"id\": \"6\",\n",
    "                \"name\": \"PFischbeck/parameter-fitting-experiments\",\n",
    "                \"url\": \"https://raw.githubusercontent.com/PFischbeck/parameter-fitting-experiments/main/Readme.md\",\n",
    "                \"n_plans\": 2,\n",
    "                \"plan_nodes\": [\n",
    "                    {\n",
    "                        \"type\": \"Source\",\n",
    "                        \"plan_step\": [\n",
    "                            \"Step 1: Make sure you have Python, Pip and R installed.\",\n",
    "                            \"Step 2: Install the `pygirgs` package at https://github.com/PFischbeck/pygirgs.\",\n",
    "                            \"Step 3: Install the R dependencies (used for plots).\",\n",
    "                            \"Step 4: Checkout the repository.\",\n",
    "                            \"Step 5: Install the python dependencies.\",\n",
    "                            \"Step 6: Download the file at https://doi.org/10.5281/zenodo.10629451.\",\n",
    "                            \"Step 7: Extract the content\",\n",
    "                            \"Step 8: Place it into the folder `input_data/konect`\"\n",
    "                        ],\n",
    "                        \"commands\": [\n",
    "                            \"\",\n",
    "                            \"\",\n",
    "                            \"R -e 'install.packages(c(ggplot2\", \"reshape2\", \"plyr\", \"dplyr\", \"scales), repos=https://cloud.r-project.org/\",\n",
    "                            \"\",\n",
    "                            \"pip3 install -r requirements.txt\",\n",
    "                            \"\",\n",
    "                            \"\",\n",
    "                            \"\"\n",
    "                        ]\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"Binary\",\n",
    "                        \"plan_step\": [\n",
    "                            \"Step 1(Optional):Download the file `output-data.zip` from (https://doi.org/10.5281/zenodo.10629451)\",\n",
    "                            \"Step 2: extract its contents into the folder `output_data`.\"\n",
    "                        ]\n",
    "                    }\n",
    "                ],\n",
    "                \"readme_instructions\": \"\",\n",
    "                \"type\": \"easy\"\n",
    "            },\n",
    "``````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. - TYPE OF PLANS PROMPT\n",
    "\n",
    "#### Baseline Zero Shot\n",
    "```py\n",
    "prompt_TEXT = \"\"\" Given a INSTALL_TEXT, detect the class of plan for the installation of a software.\"\"\"\n",
    "prompt_URL =  \"\"\" Given a INSTALL_TEXT, detect the class of plan for the installation of a software.\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**GEMMA: Gemini-based open model**\n",
    "\n",
    "ref:\n",
    "[https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf](https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Plan class detection in URL text\n",
      "\n",
      "**URL:**  raw.githubusercontent.com/PFischbeck/parameter-fitting-experiments/main/Readme.md\n",
      "\n",
      "**Text from the URL:**\n",
      "\n",
      "The `parameter-fitting-experiments` repository contains code and data for various parameter fitting experiments. The main focus of this repository is on experiments involving deep learning models for natural language processing tasks. \n",
      "\n",
      "**Plan class:**\n",
      "\n",
      "The text does not explicitly state the plan class for the installation of software, therefore I cannot detect the class of plan for the installation of software in the text of the URL.<eos>"
     ]
    }
   ],
   "source": [
    "# Given a URL (no labels), Natural language response\n",
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api-inference.huggingface.co/v1\",\n",
    "    api_key= \"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\"\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    model=\"google/gemma-7b-it\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Behave as an expert labeler. \\\n",
    "          Given a URL, detect the class of plan for the installation of a software in the text of the URL. URL = https://raw.githubusercontent.com/PFischbeck/parameter-fitting-experiments/main/Readme.md\"},        \n",
    "    ],\n",
    "    stream=True,\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "for message in chat_completion:\n",
    "    print(message.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api-inference.huggingface.co/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Label:** Container\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "The URL provided is for a GitHub repository called \"parameter-fitting-experiments.\" The text of the URL does not contain any information about the plan class for the installation of the software, therefore I have labeled the plan class as \"Container\" based on the convention that GitHub repositories related to containerized software are typically labeled as such.<eos>"
     ]
    }
   ],
   "source": [
    "# Given a URL (with labels), natural language\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api-inference.huggingface.co/v1\",\n",
    "    api_key= \"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\"\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    model=\"google/gemma-7b-it\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Behave as an expert labeler. \\\n",
    "          Given a URL, detect the class of plan for the installation of a software in the text of the URL. \\\n",
    "         The label can only be 1 of the 4 class of plans which are binary, package manager, container and source.\\\n",
    "         URL = https://raw.githubusercontent.com/PFischbeck/parameter-fitting-experiments/main/Readme.md\"},        \n",
    "    ],\n",
    "    stream=True,\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "for message in chat_completion:\n",
    "    print(message.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Plan Class:** Software Installation\n",
      "\n",
      "**Reasoning:**\n",
      "\n",
      "The text describes the installation process for a software project. It includes steps such as installing Python, Pip, R, and required packages, downloading data files, and setting up the environment. The text clearly states that the purpose of these steps is to install software and its dependencies.\n",
      "\n",
      "**Key Indicators:**\n",
      "\n",
      "- **Installation verbs:** \"Install,\" \"Install the python dependencies,\" \"Install the `pygirgs` package,\" \"Install the R dependencies,\" \"Download and extract\"\n",
      "- **Software-related terms:** \"Software,\" \"Dependencies,\" \"Packages,\" \"Python,\" \"R\"\n",
      "- **Installation tools:** \"Pip,\" \"R -e\"\n",
      "- **Data files:** \"konect-data.zip,\" \"output-data.zip\"\n",
      "- **Purpose:** \"Access all experiment results without running them themselves\"<eos>"
     ]
    }
   ],
   "source": [
    "# Given a INSTALL_TEXT (no labels), natural language\n",
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api-inference.huggingface.co/v1\",\n",
    "    api_key= \"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\"\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    model=\"google/gemma-7b-it\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Behave as an expert labeler. \\\n",
    "          Given a INSTALL_TEXT, detect the class of plan for the installation of a software. \\\n",
    "         INSTALL_TEXT = # Installation - Make sure you have Python, Pip and R installed. - Checkout this repository - Install the python dependencies with \\\n",
    "         ```pip3 install -r requirements.txt``` \\\n",
    "         - Install the `pygirgs` package at https://github.com/PFischbeck/pygirgs \\\n",
    "         - Install the R dependencies (used for plots) with \\\n",
    "         ```R -e 'install.packages(c(ggplot2, reshape2, plyr, dplyr, scales), repos=https://cloud.r-project.org/)'``` \\\n",
    "         - Download the file `konect-data.zip` from [Zenodo](https://doi.org/10.5281/zenodo.10629451) and extract its contents into the folder `input_data/konect` \\\n",
    "         - Optional: Download the file `output-data.zip` from [Zenodo](https://doi.org/10.5281/zenodo.10629451) and extract its contents into the folder `output_data`. This way, you can access all experiment results without running them yourself.\"},        \n",
    "    ],\n",
    "    stream=True,\n",
    "    max_tokens=500,\n",
    ")\n",
    "\n",
    "for message in chat_completion:\n",
    "    print(message.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**INSTALL_TEXT:**\n",
      "\n",
      "# Installation - Make sure you have Python, Pip and R installed. - Checkout this repository - Install the python dependencies with `pip3 install -r requirements.txt` - Install the `pygirgs` package at https://github.com/PFischbeck/pygirgs - Install the R dependencies (used for plots) with `R -e 'install.packages(c(ggplot2, reshape2, plyr, dplyr, scales), repos=https://cloud.r-project.org/)'` - Download the file `konect-data.zip` from [Zenodo](https://doi.org/10.5281/zenodo.10629451) and extract its contents into the folder `input_data/konect` - Optional: Download the file `output-data.zip` from [Zenodo](https://doi.org/10.5281/zenodo.10629451) and extract its contents into the folder `output_data`. This way, you can access all experiment results without running them yourself.\n",
      "\n",
      "**Class of plan:**\n",
      "\n",
      "This text describes a plan that involves installing software dependencies using `pip` and `R` package manager. Therefore, the class of plan is **package manager**.<eos>"
     ]
    }
   ],
   "source": [
    "# Given a Install_text (with labels), natural language response\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api-inference.huggingface.co/v1\",\n",
    "    api_key= \"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\"\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    model=\"google/gemma-7b-it\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Behave as an expert labeler. \\\n",
    "          Given a INSTALL_TEXT, print the INSTALL_TEXT and detect the class of plan for the installation of a software. \\\n",
    "         The label can only be 1 of the 4 class of plans which are binary, package manager, container and source. \\\n",
    "         INSTALL_TEXT = # Installation - Make sure you have Python, Pip and R installed. - Checkout this repository - Install the python dependencies with \\\n",
    "         ```pip3 install -r requirements.txt``` \\\n",
    "         - Install the `pygirgs` package at https://github.com/PFischbeck/pygirgs \\\n",
    "         - Install the R dependencies (used for plots) with \\\n",
    "         ```R -e 'install.packages(c(ggplot2, reshape2, plyr, dplyr, scales), repos=https://cloud.r-project.org/)'``` \\\n",
    "         - Download the file `konect-data.zip` from [Zenodo](https://doi.org/10.5281/zenodo.10629451) and extract its contents into the folder `input_data/konect` \\\n",
    "         - Optional: Download the file `output-data.zip` from [Zenodo](https://doi.org/10.5281/zenodo.10629451) and extract its contents into the folder `output_data`. This way, you can access all experiment results without running them yourself.\"},        \n",
    "    ],\n",
    "    stream=True,\n",
    "    max_tokens=500,\n",
    ")\n",
    "\n",
    "for message in chat_completion:\n",
    "    print(message.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Mistral orca-2 Model**\n",
    "\n",
    "ref: [mistral-7b-openorca.gguf2.Q4_0.gguf](mistral-7b-openorca.gguf2.Q4_0.gguf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gpt4all._pyllmodel:LLModel.prompt_model -- prompt:\n",
      "### System:\n",
      "You are an AI assistant that follows instruction extremely well. Help as much as you can.\n",
      "\n",
      "### User:\n",
      "Behave as an expert labeler. \\ Given the indicated URL, detect the class of plan for the installation of a software in the text of the URL.    The output can only be 1 of the 4 class of plans which are binary, package manager, container and source.     URL: https://raw.githubusercontent.com/PFischbeck/parameter-fitting-experiments/main/Readme.md\n",
      "### Response:\n",
      "\n",
      "===/LLModel.prompt_model -- prompt/===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESPONSE  The URL indicates that the installation plan is for a software that uses a package manager. Therefore, the output is \"package manager\".\n"
     ]
    }
   ],
   "source": [
    "from gpt4all import GPT4All\n",
    "model = GPT4All(\"orca-mini-3b-gguf2-q4_0.gguf\")\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "with model.chat_session():\n",
    "    prompt = 'Behave as an expert labeler. \\ Given the indicated URL, detect the class of plan for the installation of a software in the text of the URL.\\\n",
    "    The output can only be 1 of the 4 class of plans which are binary, package manager, container and source. \\\n",
    "    URL: https://raw.githubusercontent.com/PFischbeck/parameter-fitting-experiments/main/Readme.md'\n",
    "    # print(\"PROMPT: \", prompt)\n",
    "    response = model.generate(prompt=prompt, temp=0)\n",
    "    print(\"RESPONSE\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gpt4all._pyllmodel:LLModel.prompt_model -- prompt:\n",
      "### System:\n",
      "You are an AI assistant that follows instruction extremely well. Help as much as you can.\n",
      "\n",
      "### User:\n",
      "Behave as an expert labeler. \\ Given the indicated INSTALL_TEXT, please identify the class of plan of installation.    The output can only be 1 of the 4 class of plans which are binary, package manager, container and source.     INSTALL_TEXT = # Installation - Make sure you have Python, Pip and R installed. - Checkout this repository - Install the python dependencies with          ```pip3 install -r requirements.txt```          - Install the `pygirgs` package at https://github.com/PFischbeck/pygirgs          - Install the R dependencies (used for plots) with  ```R -e install.packages(c(ggplot2, reshape2, plyr, dplyr, scales), repos=https://cloud.r-project.org/)           Download the file `konect-data.zip` from [Zenodo](https://doi.org/10.5281/zenodo.10629451) and extract its contents into the folder `input_data/konect`             Optional: Download the file `output-data.zip` from [Zenodo](https://doi.org/10.5281/zenodo.10629451) and extract its contents into the folder `output_data`. This way, you can access all experiment results without running them yourself.\n",
      "### Response:\n",
      "\n",
      "===/LLModel.prompt_model -- prompt/===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:  Behave as an expert labeler. \\ Given the indicated INSTALL_TEXT, please identify the class of plan of installation.    The output can only be 1 of the 4 class of plans which are binary, package manager, container and source.     INSTALL_TEXT = # Installation - Make sure you have Python, Pip and R installed. - Checkout this repository - Install the python dependencies with          ```pip3 install -r requirements.txt```          - Install the `pygirgs` package at https://github.com/PFischbeck/pygirgs          - Install the R dependencies (used for plots) with  ```R -e install.packages(c(ggplot2, reshape2, plyr, dplyr, scales), repos=https://cloud.r-project.org/)           Download the file `konect-data.zip` from [Zenodo](https://doi.org/10.5281/zenodo.10629451) and extract its contents into the folder `input_data/konect`             Optional: Download the file `output-data.zip` from [Zenodo](https://doi.org/10.5281/zenodo.10629451) and extract its contents into the folder `output_data`. This way, you can access all experiment results without running them yourself.\n",
      "Response  The class of plan of installation is package manager.\n"
     ]
    }
   ],
   "source": [
    "from gpt4all import GPT4All\n",
    "model = GPT4All(\"orca-mini-3b-gguf2-q4_0.gguf\")\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "with model.chat_session():\n",
    "    prompt = 'Behave as an expert labeler. \\ Given the indicated INSTALL_TEXT, please identify the class of plan of installation.\\\n",
    "    The output can only be 1 of the 4 class of plans which are binary, package manager, container and source. \\\n",
    "    INSTALL_TEXT = # Installation - Make sure you have Python, Pip and R installed. - Checkout this repository - Install the python dependencies with \\\n",
    "         ```pip3 install -r requirements.txt``` \\\n",
    "         - Install the `pygirgs` package at https://github.com/PFischbeck/pygirgs \\\n",
    "         - Install the R dependencies (used for plots) with  ```R -e install.packages(c(ggplot2, reshape2, plyr, dplyr, scales), repos=https://cloud.r-project.org/) \\\n",
    "          Download the file `konect-data.zip` from [Zenodo](https://doi.org/10.5281/zenodo.10629451) and extract its contents into the folder `input_data/konect` \\\n",
    "            Optional: Download the file `output-data.zip` from [Zenodo](https://doi.org/10.5281/zenodo.10629451) and extract its contents into the folder `output_data`. This way, you can access all experiment results without running them yourself.'\n",
    "         \n",
    "    print(\"PROMPT: \", prompt)\n",
    "    response = model.generate(prompt=prompt, temp=0)\n",
    "    print(\"Response\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One Shot Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chain of Thought Prompting**\n",
    "\n",
    "\n",
    "In addition to the four experiments above, we look at a fifth experiment using a state tracking chain of thought prompting technique in a natural language one shot setting. Within this configuration, we provide an annotated example where each action is annotated with the state prior to the action, the reason for why the action is applicable in the prior state, and the resulting state after applying the action. After the example, a meta-explanation about plan correctness is provided. The LLM is then asked to return a response making the same state tracking and justification annotations that were included in the example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. - LIST OF STEPS PROMPT\n",
    "\n",
    "#### Baseline Zero Shot\n",
    "```py\n",
    "prompt = \"\"\" Provide a detailed list of the Steps in the given Research Software which refers to \"[software-name]\" plan, including any optional or commands for each Step? \"\"\"\n",
    "```\n",
    "\n",
    "#### One Shot Learning:\n",
    "\n",
    "The prompt additionally contains an example instance of a research software installation plan (consisting of a description of the initial step and the end) and the corresponding plan (which ends with a tag, referred to as the plan-end tag, that denotes the end of the plan). The prompt is formatted in natural language (or **JSON/RDF format**?)\n",
    "\n",
    "**JSON**\n",
    "\n",
    "```py\n",
    "prompt = \"\"\"\\nGiven a TEXT, I want you generate task steps and plan type of installation.\\\n",
    "    The format must in a strict JSON format : {[{\"plan_type\": \"detect the method of installation\", \"task_steps\": [a ordered list of steps to install], \"commands\": [ a concise list of commands for the tool.  \\\n",
    "    If you do not find software installation steps in the TEXT, return \"none\".\\\n",
    "    Annotate the following TEXT  \"\"\"\n",
    "        prompt += \"\"\"\\n\\n# REQUIREMENTS #: \\n1. the generated task steps and plan types allign with # RESEARCH SOFTWARE TOOL # perfectly. Task name must be detected from the README file; \\n2. the task steps should strictly aligned with the plan type, and the number of task steps should be same with the readmes;  \\n4.  RESEARCH SOFTWARE TOOL command should be align with the input-type field of # TASK LIST #;\"\"\"\n",
    "```\n",
    "\n",
    "**ONTOLOGY**\n",
    "\n",
    "```py\n",
    "\n",
    "prompt = \"\"\" Here is the complete definition of the procedural RDF ontology in TTL format which defines Plans and Steps and relationships between each other:\n",
    "\n",
    "@prefix p-plan: <http://purl.org/net/p-plan#> .\n",
    "@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n",
    "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
    "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
    "@prefix prov: <http://www.w3.org/ns/prov#> .\n",
    "@prefix bpmn: <http://www.w3.org/ns/bpmn#> .\n",
    "\n",
    "[.....]\n",
    "\n",
    "\n",
    "Using the provided ontology, please create specific instances and data about plans representing the steps and  for the [-] software installation procedure. Include relationships such as 'isStepOfPlan' to model the hierarchy of main steps and their substeps. Additionally, create the RDF graph that represents the defined individuals and their relationships.  Refer to the RDF graph as Context, please provide an Answer to the Question below.\"\"\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**GEMMA: Gemini-based open model**\n",
    "\n",
    "ref:\n",
    "[https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf](https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**- Baseline Zero Shot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Steps for Installing Software Dependencies for Parameter Fitting Experiments\n",
      "\n",
      "**Number of Steps:** 4\n",
      "\n",
      "**URL:** raw.githubusercontent.com/PFischbeck/parameter-fitting-experiments/main/Readme.md\n",
      "\n",
      "**1. Create a virtual environment:**\n",
      "\n",
      "- Open a terminal window.\n",
      "- Run the command `virtualenv env` to create a virtual environment named `env`.\n",
      "\n",
      "**2. Activate the virtual environment:**\n",
      "\n",
      "- Run the command `source env/bin/activate` to activate the virtual environment.\n",
      "\n",
      "**3. Install dependencies:**\n",
      "\n",
      "- Run the command `pip install -r requirements.txt` to install the dependencies listed in the `requirements.txt` file.\n",
      "\n",
      "**4. Verify installation:**\n",
      "\n",
      "- Run the command `python` to start the Python interpreter.\n",
      "- Import the `parameter_fitting` module and check if it is imported successfully.<eos>"
     ]
    }
   ],
   "source": [
    "# Given a URL, Natural language response\n",
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api-inference.huggingface.co/v1\",\n",
    "    api_key= \"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\"\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    model=\"google/gemma-7b-it\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Behave as an expert labeler. \\\n",
    "          Given a URL, I want you generate a list of steps for the plan of installation. Please indicate the number of the steps. URL = https://raw.githubusercontent.com/PFischbeck/parameter-fitting-experiments/main/Readme.md\"},        \n",
    "    ],\n",
    "    stream=True,\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "for message in chat_completion:\n",
    "    print(message.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api-inference.huggingface.co/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Steps of installation for parameter-fitting-experiments project:\n",
      "\n",
      "**Number of steps:** 3\n",
      "\n",
      "```json\n",
      "\"steps\": {\n",
      "  \"1. Clone repository\": {\n",
      "    \"description\": \"Clone the parameter-fitting-experiments repository using the following command:\\n```\\nsgit clone git@github.com:PFischbeck/parameter-fitting-experiments.git```\",\n",
      "    \"os\": \"Linux, MacOS, Windows\"\n",
      "  },\n",
      "  \"2. Install dependencies\": {\n",
      "    \"description\": \"Install the required dependencies using pip:\\n```\\npip install -r requirements.txt```\",\n",
      "    \"os\": \"Linux, MacOS, Windows\"\n",
      "  },\n",
      "  \"3. Start the server\": {\n",
      "    \"description\": \"Start the server using the following command:\\n```\\nserver -m parameter-fitting-experiments```\",\n",
      "    \"os\": \"Linux, MacOS, Windows\"\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "**Note:** This JSON object describes the steps for installing the project on various operating systems. The steps may slightly vary depending on the specific version of the project and OS. Please check the official documentation for the most up-to-date information.<eos>"
     ]
    }
   ],
   "source": [
    "# given a URL, JSON object response\n",
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api-inference.huggingface.co/v1\",\n",
    "    api_key= \"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\"\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    model=\"google/gemma-7b-it\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Behave as an expert labeler. \\\n",
    "          Given a URL, generate an example json object describing the Steps of installation. Please indicate the number of the steps. URL = https://raw.githubusercontent.com/PFischbeck/parameter-fitting-experiments/main/Readme.md\"},        \n",
    "    ],\n",
    "    stream=True,\n",
    "    max_tokens=500,\n",
    "    response_format={\"type\": \"json_object\"},\n",
    ")\n",
    "\n",
    "for message in chat_completion:\n",
    "    print(message.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## JSON object describing the steps of the Research Software installation:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"total_steps\": 6,\n",
      "  \"steps\": [\n",
      "    \"Make sure you have Python, Pip and R installed.\",\n",
      "    \"Checkout this repository\",\n",
      "    \"Install the python dependencies with `pip3 install -r requirements.txt`\",\n",
      "    \"Install the `pygirgs` package at https://github.com/PFischbeck/pygirgs\",\n",
      "    \"Install the R dependencies (used for plots) with `R -e 'install.packages(c(ggplot2, reshape2, plyr, dplyr, scales), repos=https://cloud.r-project.org/)'`,\n",
      "    \"Download the file `konect-data.zip` from [Zenodo](https://doi.org/10.5281/zenodo.10629451) and extract its contents into the folder `input_data/konect`\"\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "Total number of steps: **6**<eos>"
     ]
    }
   ],
   "source": [
    "# Given a INSTALL_TEXT, JSON object response\n",
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api-inference.huggingface.co/v1\",\n",
    "    api_key= \"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\"\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    model=\"google/gemma-7b-it\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Behave as an expert labeler. \\\n",
    "          Given a INSTALL_TEXT, generate an example json object describing the Steps of installation of the Research Software. Please indicate the total number of the Steps. \\\n",
    "         INSTALL_TEXT = # Installation - Make sure you have Python, Pip and R installed. - Checkout this repository - Install the python dependencies with \\\n",
    "         ```pip3 install -r requirements.txt``` \\\n",
    "         - Install the `pygirgs` package at https://github.com/PFischbeck/pygirgs \\\n",
    "         - Install the R dependencies (used for plots) with \\\n",
    "         ```R -e 'install.packages(c(ggplot2, reshape2, plyr, dplyr, scales), repos=https://cloud.r-project.org/)'``` \\\n",
    "         - Download the file `konect-data.zip` from [Zenodo](https://doi.org/10.5281/zenodo.10629451) and extract its contents into the folder `input_data/konect` \\\n",
    "         - Optional: Download the file `output-data.zip` from [Zenodo](https://doi.org/10.5281/zenodo.10629451) and extract its contents into the folder `output_data`. This way, you can access all experiment results without running them yourself.\"},        \n",
    "    ],\n",
    "    stream=True,\n",
    "    max_tokens=500,\n",
    "    response_format={\"type\": \"json_object\"},\n",
    ")\n",
    "\n",
    "for message in chat_completion:\n",
    "    print(message.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mistral orca-2 Model**\n",
    "\n",
    "ref: [mistral-7b-openorca.gguf2.Q4_0.gguf](mistral-7b-openorca.gguf2.Q4_0.gguf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gpt4all._pyllmodel:LLModel.prompt_model -- prompt:\n",
      "You are an expert label.\n",
      "Be terse.\n",
      "\n",
      "### Instruction:\n",
      " Given a TEXT, I want you generate task steps and plan type of installation.    The format must in a strict JSON format : {[{\"plan_type\": \"detect the method of installation\", \"task_steps\": [a ordered list of steps to install], \"commands\": [ a concise list of commands for the tool.      If you do not find software installation steps in the TEXT, return \"none\".    Annotate the following TEXT: # Installation ### Dependencies Initialize git submodules with `` git submodule init git submodule update```     Install the specific versions of every package from `requirements.txt` in a new conda environment:    conda create --name gsft python=3.9     conda activate gsft    pip install -r requirements.txt    To ensure that Python paths are properly defined, update the `~/.bashrc` by adding the following lines    export GSFT_PATH=/path_to_gsfc    export PYTHONPATH=$PYTHONPATH:/$GSFT_PATH\n",
      "### Response:\n",
      "\n",
      "===/LLModel.prompt_model -- prompt/===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:   Given a TEXT, I want you generate task steps and plan type of installation.    The format must in a strict JSON format : {[{\"plan_type\": \"detect the method of installation\", \"task_steps\": [a ordered list of steps to install], \"commands\": [ a concise list of commands for the tool.      If you do not find software installation steps in the TEXT, return \"none\".    Annotate the following TEXT: # Installation ### Dependencies Initialize git submodules with `` git submodule init git submodule update```     Install the specific versions of every package from `requirements.txt` in a new conda environment:    conda create --name gsft python=3.9     conda activate gsft    pip install -r requirements.txt    To ensure that Python paths are properly defined, update the `~/.bashrc` by adding the following lines    export GSFT_PATH=/path_to_gsfc    export PYTHONPATH=$PYTHONPATH:/$GSFT_PATH\n",
      "Response  {[{\"plan_type\": \"software installation\", \"task_steps\": [\n",
      "\"Determine the specific version of Python to install from `requirements.txt`\",\n",
      "\"Download and install the specified Python package using pip\",\n",
      "\"Install any necessary packages such as NumPy, Pandas or Matplotlib\",\n",
      "\"Create a conda environment named 'gsft' with Python 3.9\",\n",
      "\"Activate the conda environment\",\n",
      "\"Update the `~/.bashrc` to define Python paths properly by adding lines such as `export GSFT_PATH=/path_to_gsfc` and `export PYTHONPATH=$PYTHONPATH:/$GSFT_PATH`\"]]} \n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from gpt4all import GPT4All\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "model = GPT4All(\"orca-mini-3b-gguf2-q4_0.gguf\")\n",
    "with model.chat_session('You are an expert label.\\nBe terse.',\n",
    "                        '### Instruction:\\n{0}\\n### Response:\\n'):\n",
    "    prompt = ' Given a TEXT, I want you generate task steps and plan type of installation.\\\n",
    "    The format must in a strict JSON format : {[{\"plan_type\": \"detect the method of installation\", \"task_steps\": [a ordered list of steps to install], \"commands\": [ a concise list of commands for the tool.  \\\n",
    "    If you do not find software installation steps in the TEXT, return \"none\".\\\n",
    "    Annotate the following TEXT: # Installation ### Dependencies Initialize git submodules with `` git submodule init git submodule update``` \\\n",
    "    Install the specific versions of every package from `requirements.txt` in a new conda environment:\\\n",
    "    conda create --name gsft python=3.9 \\\n",
    "    conda activate gsft\\\n",
    "    pip install -r requirements.txt\\\n",
    "    To ensure that Python paths are properly defined, update the `~/.bashrc` by adding the following lines\\\n",
    "    export GSFT_PATH=/path_to_gsfc\\\n",
    "    export PYTHONPATH=$PYTHONPATH:/$GSFT_PATH'\n",
    "    print(\"PROMPT: \", prompt)\n",
    "    response = model.generate(prompt=prompt, temp=0.6)\n",
    "    print(\"Response\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:  Behave as an expert labeler. \\ Given a URL, I want you generate task steps and plan type of installation.    The format must in a strict JSON format : {\"task_steps\": [ step description of one or more steps ], \"plan_nodes\": [{\"plan_type\": \"detect the method of installation\", \"commands\": [ a concise list of commands for the tool.     URL: https://raw.githubusercontent.com/PFischbeck/parameter-fitting-experiments/main/Readme.md\n",
      "Response  {\n",
      "\"task_steps\": [\n",
      " {\"step\": \"Detect the method of installation\",\n",
      " \"commands\": [\"sudo apt-get update && sudo apt-get install pf-script\"]\n",
      "}\n",
      "],\n",
      "\"plan_nodes\": [\n",
      " {\"plan_type\": \"Installation Plan\",\n",
      " \"instructions\": [\n",
      " \"1. Download the latest version of the package manager for your operating system from the official website.\",\n",
      " \"2. Open the terminal and run the command `sudo apt-get update` to check if there are any updates available.\",\n",
      " \"3. Run the command `sudo apt-get install pf-script` to download and install the package manager.\"\n",
      " ]}\n",
      "]\n",
      "}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gpt4all._pyllmodel:LLModel.prompt_model -- prompt:\n",
      "### System:\n",
      "You are an AI assistant that follows instruction extremely well. Help as much as you can.\n",
      "\n",
      "### User:\n",
      "Behave as an expert labeler. \\ Given a URL, I want you generate task steps and plan type of installation.    The format must in a strict JSON format : {\"task_steps\": [ step description of one or more steps ], \"plan_nodes\": [{\"plan_type\": \"detect the method of installation\", \"commands\": [ a concise list of commands for the tool.     URL: https://raw.githubusercontent.com/PFischbeck/parameter-fitting-experiments/main/Readme.md\n",
      "### Response:\n",
      "\n",
      "===/LLModel.prompt_model -- prompt/===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response  {\n",
      "\"task_steps\": [\n",
      " {\"step\": \"Detect the method of installation\",\n",
      " \"commands\": [\"sudo apt-get update && sudo apt-get install pf-script\"]\n",
      "}\n",
      "],\n",
      "\"plan_nodes\": [\n",
      " {\"plan_type\": \"Installation Plan\",\n",
      " \"instructions\": [\n",
      " \"1. Download the latest version of the package manager for your operating system from the official website.\",\n",
      " \"2. Open the terminal and run the command `sudo apt-get update` to check if there are any updates available.\",\n",
      " \"3. Run the command `sudo apt-get install pf-script` to download and install the package manager.\"\n",
      " ]}\n",
      "]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from gpt4all import GPT4All\n",
    "model = GPT4All(\"orca-mini-3b-gguf2-q4_0.gguf\")\n",
    "with model.chat_session():\n",
    "    prompt = 'Behave as an expert labeler. \\ Given a URL, I want you generate task steps and plan type of installation.\\\n",
    "    The format must in a strict JSON format : {\"task_steps\": [ step description of one or more steps ], \"plan_nodes\": [{\"plan_type\": \"detect the method of installation\", \"commands\": [ a concise list of commands for the tool. \\\n",
    "    URL: https://raw.githubusercontent.com/PFischbeck/parameter-fitting-experiments/main/Readme.md'\n",
    "    # print(\"PROMPT: \", prompt)\n",
    "    response = model.generate(prompt=prompt, temp=0)\n",
    "    print(\"Response\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. - TYPE OF PLANS PROMPTS\n",
    "\n",
    "#### Baseline-  Zero Shot\n",
    "```py\n",
    "prompt = \"\"\" Classify the text into LABELS [source, binary, container, and package manager]\"\"\"\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
